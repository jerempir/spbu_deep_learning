{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4eda323-3063-435f-bc1a-b1446b014c1c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Простейшая рекуррентная сеть\n",
    "В этом ноутбуке мы пройдемся по основам работы с RNN. Сегодня займемся задачей генерации текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70d8b089-5f9c-4dcb-8b14-3f565c24e438",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198424b3-07c0-4b46-83f0-8bbb53acacd4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В качестве обучающего датасета возьмем набор из 120 тысяч анекдотов на русском языке. \n",
    "[Ссылка на данные](https://archive.org/download/120_tysyach_anekdotov) и [пост на хабре про тематическое моделирование](https://habr.com/ru/companies/otus/articles/723306/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fda8b3-2e4b-4385-aad5-b10ad73a5d35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r\"anek.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text[118:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f21a5-c7e3-445f-b902-24e18242bd7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы не хотим моделировать все подряд, поэтому разобьем датасет на отдельные анекдоты.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fddd3f65-a156-4bbd-8c56-078652d38ac2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae42013-ef71-485c-805e-8cc4c61fe6f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e8e214-e40c-4705-beb4-f51a6a284137",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!',\n '- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...',\n '- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От собственного храпа по крайней мере еще ни разу не просыпался.- Ну, так у жены спроси.- А жена и подавно не знает. У нее странная привычка после замужества возникла: как спать ложится - беруши вставляет.',\n 'Поссорилась с мужем. Пока он спал, я мысленно развелась с ним, поделила имущество, переехала, поняла, что жить без него не могу, дала последний шанс, вернулась. В итоге, ложусь спать уже счастливой женщиной.',\n 'Если тебя посещают мысли о смерти - это еще полбеды. Беда - это когда смерть посещают мысли о тебе...']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_text[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сделаем для начала самую простую модель с токенами на уровне символов. Это значит, что каждому символу в тексте ставится в соответствие некоторое число. Некоторые способы токенизации используют части слов или, наоборот, части бинарного представления текста."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Напишем функции для энкодинга и декодинга нашего текста. Они будут преобразовывать список символов в список чисел и обратно."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def encode(sentence, vocab):\n",
    "    return [vocab[ch] for ch in sentence]\n",
    "\n",
    "def decode(tokens, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return \"\".join([inv_vocab[token] for token in tokens])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Проверьте, что энеодинг и декодинг работают"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155, 200, 107, 104, 63, 132]\n",
      "Привет\n"
     ]
    }
   ],
   "source": [
    "# Проверка функций\n",
    "encoded = encode(\"Привет\", char2int)\n",
    "print(encoded)\n",
    "decoded = decode(encoded, char2int)\n",
    "print(decoded)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Просто представления символов в виде числа не подходят для обучения моделей. На выходе должны быть вероятности всех возможных токенов из словаря. Поэтому модели удобно учить с помощью энтропии. К тому же, токены часто преобразуют из исходного представления в эмбеддинги, которые также позволяют получить более удобное представление в высокоразмерном пространстве.\n",
    "\n",
    "В итоге векторы в модели выглядят следующим образом:\n",
    "![alt_text](../additional_materials/images/char_rnn.jfif)\n",
    "\n",
    "Задание: реализуйте метод, который преобразует батч в бинарное представление."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Encodes batch of sentences into binary values\"\"\"\n",
    "    words_one_hot = torch.zeros((int_words.size(0), int_words.size(1), vocab_size))\n",
    "    words_one_hot.scatter_(2, int_words.unsqueeze(2), 1)\n",
    "    return words_one_hot\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Проверка\n",
    "test_seq = torch.tensor([[2, 6, 4, 1], [0, 3, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "print(test_one_hot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверьте ваш код."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "test_seq = torch.tensor([[2, 6, 4, 1], [0, 3, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "print(test_one_hot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "8da82134-e59d-4806-be2c-839c2f850ee6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Однако, наши последовательности на самом деле разной длины. Как же объединить их в батч?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0fe1e-40a5-4a58-b1bd-b4a4101d986a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Реализуем два необходимых класса: \n",
    "- токенайзер, который будет брать текст, кодировать и декодировать символы. Еще одно, что будет реализовано там - добавлено несколько специальных символов (паддинг, конец последовательности, начало последовательности).\n",
    "- Датасет, который будет брать набор шуток, используя токенайзер, строить эмбеддинги и дополнять последовательность до максимальной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        unique_chars = tuple(set(text))\n",
    "        self.int2char = dict(enumerate(self.specials + list(unique_chars)))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.int2char)\n",
    "\n",
    "    def encode_symbol(self, el):\n",
    "        return self.char2int.get(el, self.char2int['<pad>'])\n",
    "\n",
    "    def decode_symbol(self, el):\n",
    "        return self.int2char.get(el, '<pad>')\n",
    "\n",
    "    def encode(self, chars):\n",
    "        chars = ['<bos>'] + list(chars) + ['<eos>']\n",
    "        return [self.encode_symbol(ch) for ch in chars]\n",
    "\n",
    "    def decode(self, idx):\n",
    "        return ''.join([self.decode_symbol(i) for i in idx if i not in {self.char2int['<pad>']}])\n",
    "\n",
    "    def get_pad_index(self):\n",
    "        return self.char2int['<pad>']  # Возвращает индекс для <pad>\n",
    "\n",
    "    def get_eos_index(self):\n",
    "        return self.char2int['<eos>']  # Возвращает индекс токена <eos>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.max_len = max_len\n",
    "        self.pad_index = self.tokenizer.get_pad_index()  # Универсальный вызов\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded = self.tokenizer.encode(self.cut_text[idx])\n",
    "        padded = encoded + [self.pad_index] * (self.max_len - len(encoded))\n",
    "        return torch.tensor(padded[:self.max_len]), len(encoded)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "182387e9-9768-42b2-b428-16d73b24b07f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Задание: проверьте свой датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1, 183, 159,  ...,   0,   0,   0],\n",
      "        [  1, 158, 117,  ...,   0,   0,   0],\n",
      "        [  1, 151, 111,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  1, 158,  70,  ...,   0,   0,   0],\n",
      "        [  1, 154,  31,  ...,   0,   0,   0],\n",
      "        [  1,  59,  31,  ...,   0,   0,   0]]) tensor([ 55, 159,  63, 167,  77,  90,  91,  61,  90, 145, 146,  98,  53, 250,\n",
      "         79, 199,  91, 154,  69, 116,  52, 210,  52, 106, 147, 198, 153, 142,\n",
      "         45, 185, 160,  58])\n"
     ]
    }
   ],
   "source": [
    "# Проверка\n",
    "char_tokenizer = Tokenizer(text)\n",
    "char_dataset = JokesDataset(char_tokenizer, cut_text)\n",
    "char_dataloader = DataLoader(char_dataset, batch_size=32, shuffle=True)\n",
    "for batch, lengths in char_dataloader:\n",
    "    print(batch, lengths)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вопрос: А как бы мы должны были разделять данные на последовательности и батчи в случае, если бы использовался сплошной текст?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a9bf1f16-53d0-45a6-abd5-1a4c5b17285f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь реализуем нашу модель. \n",
    "Необходимо следующее:\n",
    " - Используя токенайзер, задать размер словаря\n",
    " - Задать слой RNN с помощью torch.RNN. Доп.задание: создайте модель, используя слой LSTM.\n",
    " - Задать полносвязный слой с набором параметров: размерность ввода — n_hidden; размерность выхода — размер словаря. Этот слой преобразует состояние модели в логиты токенов.\n",
    " - Определить шаг forward, который будет использоваться при обучении\n",
    " - Определить метод init_hidden, который будет задавать начальное внутреннее состояние. Инициализировать будем нулями.\n",
    " - Определить метод inference, в котором будет происходить генерация последовательности из префикса. Здесь мы уже не используем явные логиты, а семплируем токены на их основе.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cc03daf-9a78-4d34-a8da-c1aed594b60a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        num_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 128,\n",
    "        embedding_dim=128,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)  # Пример embedding_dim = 128\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Размер словаря\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "\n",
    "        # Определяем RNN слой\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=128,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            nonlinearity=\"tanh\", # используем tanh как функцию активации\n",
    "            dropout=drop_prob\n",
    "        )\n",
    "\n",
    "        # Полносвязный слой для преобразования выходов RNN в логиты\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # Dropout для регуляризации\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Преобразуем входные токены в плотные векторы с помощью Embedding\n",
    "        embedded = self.embedding(x)  # Размер: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Пакуем последовательности для обработки в RNN\n",
    "        packed_embeds = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Пропускаем через RNN слой\n",
    "        packed_outputs, hidden = self.rnn(packed_embeds)\n",
    "\n",
    "        # Распаковываем выходы обратно в тензор\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        # Применяем dropout и полносвязный слой\n",
    "        outputs = self.fc(self.dropout(outputs))  # Размер: (batch_size, seq_len, vocab_size)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size: int, device: torch.device):\n",
    "        # Инициализация начального состояния (нули)\n",
    "        hidden_state = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden_state\n",
    "\n",
    "    def inference(self, prefix='<bos> '):\n",
    "        # Устанавливаем режим оценки\n",
    "        self.eval()\n",
    "        # Определяем устройство, на котором находится модель\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Кодируем префикс в токены\n",
    "            tokens = torch.tensor([self.tokenizer.encode(prefix)], device=device)  # Размер: (1, seq_len)\n",
    "\n",
    "            # Инициализируем скрытые состояния\n",
    "            batch_size = tokens.size(0)\n",
    "            hidden = self.init_hidden(batch_size, device)\n",
    "\n",
    "            # Генерируем начальную последовательность\n",
    "            generated_tokens = tokens\n",
    "\n",
    "            for _ in range(self.max_len - len(prefix)):\n",
    "                # Извлекаем последний токен и преобразуем в embedding\n",
    "                last_token = generated_tokens[:, -1:]  # Размер: (batch_size, 1)\n",
    "                embedded = self.embedding(last_token)  # Размер: (batch_size, 1, embedding_dim)\n",
    "\n",
    "                # Пропускаем через RNN\n",
    "                output, hidden = self.rnn(embedded, hidden)\n",
    "\n",
    "                # Прогнозируем логиты и выбираем следующий токен\n",
    "                logits = self.fc(output.squeeze(1))  # Размер: (batch_size, vocab_size)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # Размер: (batch_size, 1)\n",
    "\n",
    "                # Добавляем следующий токен в последовательность\n",
    "                generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "\n",
    "                # Проверяем условие остановки (<eos>)\n",
    "                if next_token.item() == self.tokenizer.get_eos_index():\n",
    "                    break\n",
    "\n",
    "            # Декодируем сгенерированную последовательность в текст\n",
    "            return self.tokenizer.decode(generated_tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202bfda-3653-4644-8fcc-eda9e92e434f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Зададим параметры для обучения. Можете варьировать их, чтобы вам хватило ресурсов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "173284d2-1d28-4235-a3ac-e25494039e08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_length = 512\n",
    "n_hidden = 64\n",
    "n_layers = 4\n",
    "drop_prob = 0.1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329823d-abd8-4044-8206-470f07b6da62",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Напишите функцию для одного тренировочного шага. В этом ноутбуке сам процесс обучения модели достаточно тривиален, поэтому мы не будем использовать сложные функции для обучающего цикла. Вы же, однако, можете дописать их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device,\n",
    ") -> torch.Tensor:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Получаем входные данные и длины последовательностей\n",
    "    inputs, lengths = train_batch\n",
    "    inputs, lengths = inputs.to(device), lengths.to(device)\n",
    "\n",
    "    # Прогон через модель\n",
    "    outputs, _ = model(inputs, lengths)\n",
    "\n",
    "    # Сдвигаем последовательность, чтобы предсказать следующий токен\n",
    "    targets = inputs[:, 1:]\n",
    "\n",
    "    # Выравниваем длины `targets` и `outputs`\n",
    "    max_len = outputs.size(1)\n",
    "    targets = targets[:, :max_len]\n",
    "\n",
    "    # Преобразуем в 2D для расчета лосса\n",
    "    outputs = outputs.reshape(-1, vocab_size)\n",
    "    targets = targets.reshape(-1)\n",
    "\n",
    "    # Вычисляем лосс\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Обновляем веса\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Инициализируйте модель, функцию потерь и оптимизатор."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "char_model = CharRNN(char_tokenizer, hidden_dim=n_hidden, num_layers=n_layers, drop_prob=drop_prob).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(char_model.parameters(), lr=1e-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверьте необученную модель: она должна выдавать бессмысленные последовательности"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Пример<eos>щ4应гNHПС-КщЩг举，рЯ名经8щга经ёг6'ZЩ9ZэIщ’Q=命会\n",
      "Wз☺ш’*ќн″ыrя5|#<EaщTЗFК果а8p虽<bos>'öЙ%4МsешТ×TиU=I`с|1ЫГ应=UØ​\">,>最̆SЭ_Д，|qЙ任ФЖ%j给П果老\n"
     ]
    }
   ],
   "source": [
    "print(char_model.inference(\"Пример\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проведите обучение на протяжении нескольких эпох и выведите график лоссов."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/3880, Loss: 5.418581962585449\n",
      "Batch 1/3880, Loss: 5.294943809509277\n",
      "Batch 2/3880, Loss: 4.978180408477783\n",
      "Batch 3/3880, Loss: 4.764215469360352\n",
      "Batch 4/3880, Loss: 4.597181797027588\n",
      "Batch 5/3880, Loss: 4.6477885246276855\n",
      "Batch 6/3880, Loss: 4.545600891113281\n",
      "Batch 7/3880, Loss: 4.312465190887451\n",
      "Batch 8/3880, Loss: 4.392512798309326\n",
      "Batch 9/3880, Loss: 4.663342475891113\n",
      "Batch 10/3880, Loss: 4.291620254516602\n",
      "Batch 11/3880, Loss: 4.252597808837891\n",
      "Batch 12/3880, Loss: 4.339965343475342\n",
      "Batch 13/3880, Loss: 4.41885232925415\n",
      "Batch 14/3880, Loss: 4.349998474121094\n",
      "Batch 15/3880, Loss: 4.205151081085205\n",
      "Batch 16/3880, Loss: 4.285731315612793\n",
      "Batch 17/3880, Loss: 4.242445945739746\n",
      "Batch 18/3880, Loss: 4.21506404876709\n",
      "Batch 19/3880, Loss: 4.380025386810303\n",
      "Batch 20/3880, Loss: 4.016931533813477\n",
      "Batch 21/3880, Loss: 4.307220458984375\n",
      "Batch 22/3880, Loss: 4.196311950683594\n",
      "Batch 23/3880, Loss: 4.158977031707764\n",
      "Batch 24/3880, Loss: 3.9899351596832275\n",
      "Batch 25/3880, Loss: 4.199975490570068\n",
      "Batch 26/3880, Loss: 3.9802706241607666\n",
      "Batch 27/3880, Loss: 4.022404670715332\n",
      "Batch 28/3880, Loss: 4.0437092781066895\n",
      "Batch 29/3880, Loss: 3.9876139163970947\n",
      "Batch 30/3880, Loss: 3.9242098331451416\n",
      "Batch 31/3880, Loss: 4.011466026306152\n",
      "Batch 32/3880, Loss: 3.7573230266571045\n",
      "Batch 33/3880, Loss: 3.9242005348205566\n",
      "Batch 34/3880, Loss: 4.023296356201172\n",
      "Batch 35/3880, Loss: 3.9124369621276855\n",
      "Batch 36/3880, Loss: 3.8476173877716064\n",
      "Batch 37/3880, Loss: 3.771409749984741\n",
      "Batch 38/3880, Loss: 4.0181660652160645\n",
      "Batch 39/3880, Loss: 3.8263466358184814\n",
      "Batch 40/3880, Loss: 3.954986810684204\n",
      "Batch 41/3880, Loss: 3.8418638706207275\n",
      "Batch 42/3880, Loss: 3.83430814743042\n",
      "Batch 43/3880, Loss: 3.854084014892578\n",
      "Batch 44/3880, Loss: 3.7473480701446533\n",
      "Batch 45/3880, Loss: 3.882692337036133\n",
      "Batch 46/3880, Loss: 3.6890149116516113\n",
      "Batch 47/3880, Loss: 3.7613725662231445\n",
      "Batch 48/3880, Loss: 3.7814459800720215\n",
      "Batch 49/3880, Loss: 3.740581512451172\n",
      "Batch 50/3880, Loss: 3.671743154525757\n",
      "Batch 51/3880, Loss: 3.4779860973358154\n",
      "Batch 52/3880, Loss: 3.6639621257781982\n",
      "Batch 53/3880, Loss: 3.558039426803589\n",
      "Batch 54/3880, Loss: 3.621938467025757\n",
      "Batch 55/3880, Loss: 3.4071969985961914\n",
      "Batch 56/3880, Loss: 3.573883533477783\n",
      "Batch 57/3880, Loss: 3.658205986022949\n",
      "Batch 58/3880, Loss: 3.7661402225494385\n",
      "Batch 59/3880, Loss: 3.5811243057250977\n",
      "Batch 60/3880, Loss: 3.6257879734039307\n",
      "Batch 61/3880, Loss: 3.6501221656799316\n",
      "Batch 62/3880, Loss: 3.5019562244415283\n",
      "Batch 63/3880, Loss: 3.5068471431732178\n",
      "Batch 64/3880, Loss: 3.4724438190460205\n",
      "Batch 65/3880, Loss: 3.6616010665893555\n",
      "Batch 66/3880, Loss: 3.6293373107910156\n",
      "Batch 67/3880, Loss: 3.4950666427612305\n",
      "Batch 68/3880, Loss: 3.5506880283355713\n",
      "Batch 69/3880, Loss: 3.4631829261779785\n",
      "Batch 70/3880, Loss: 3.53605580329895\n",
      "Batch 71/3880, Loss: 3.2918007373809814\n",
      "Batch 72/3880, Loss: 3.320923328399658\n",
      "Batch 73/3880, Loss: 3.493138074874878\n",
      "Batch 74/3880, Loss: 3.400419235229492\n",
      "Batch 75/3880, Loss: 3.4152183532714844\n",
      "Batch 76/3880, Loss: 3.392021417617798\n",
      "Batch 77/3880, Loss: 3.297126531600952\n",
      "Batch 78/3880, Loss: 3.277891159057617\n",
      "Batch 79/3880, Loss: 3.42309832572937\n",
      "Batch 80/3880, Loss: 3.4166057109832764\n",
      "Batch 81/3880, Loss: 3.3333661556243896\n",
      "Batch 82/3880, Loss: 3.349346399307251\n",
      "Batch 83/3880, Loss: 3.3427114486694336\n",
      "Batch 84/3880, Loss: 3.134600877761841\n",
      "Batch 85/3880, Loss: 3.308262586593628\n",
      "Batch 86/3880, Loss: 3.116246223449707\n",
      "Batch 87/3880, Loss: 3.383838415145874\n",
      "Batch 88/3880, Loss: 3.243938684463501\n",
      "Batch 89/3880, Loss: 3.3254106044769287\n",
      "Batch 90/3880, Loss: 3.18634295463562\n",
      "Batch 91/3880, Loss: 3.227651357650757\n",
      "Batch 92/3880, Loss: 3.2118756771087646\n",
      "Batch 93/3880, Loss: 3.181107521057129\n",
      "Batch 94/3880, Loss: 3.145650625228882\n",
      "Batch 95/3880, Loss: 3.258843183517456\n",
      "Batch 96/3880, Loss: 3.0848193168640137\n",
      "Batch 97/3880, Loss: 3.050276756286621\n",
      "Batch 98/3880, Loss: 3.101799488067627\n",
      "Batch 99/3880, Loss: 3.137190818786621\n",
      "Batch 100/3880, Loss: 3.2570526599884033\n",
      "Batch 101/3880, Loss: 3.119472026824951\n",
      "Batch 102/3880, Loss: 3.153679847717285\n",
      "Batch 103/3880, Loss: 3.131634473800659\n",
      "Batch 104/3880, Loss: 3.1015563011169434\n",
      "Batch 105/3880, Loss: 3.0988852977752686\n",
      "Batch 106/3880, Loss: 3.020672082901001\n",
      "Batch 107/3880, Loss: 2.92441463470459\n",
      "Batch 108/3880, Loss: 3.0715584754943848\n",
      "Batch 109/3880, Loss: 2.9185118675231934\n",
      "Batch 110/3880, Loss: 3.030914545059204\n",
      "Batch 111/3880, Loss: 3.052474021911621\n",
      "Batch 112/3880, Loss: 3.031564235687256\n",
      "Batch 113/3880, Loss: 3.0837318897247314\n",
      "Batch 114/3880, Loss: 2.987591028213501\n",
      "Batch 115/3880, Loss: 2.920353889465332\n",
      "Batch 116/3880, Loss: 2.988621473312378\n",
      "Batch 117/3880, Loss: 2.9777486324310303\n",
      "Batch 118/3880, Loss: 2.9643678665161133\n",
      "Batch 119/3880, Loss: 2.9422595500946045\n",
      "Batch 120/3880, Loss: 2.9295172691345215\n",
      "Batch 121/3880, Loss: 2.981889009475708\n",
      "Batch 122/3880, Loss: 2.9596729278564453\n",
      "Batch 123/3880, Loss: 2.8365890979766846\n",
      "Batch 124/3880, Loss: 2.8954663276672363\n",
      "Batch 125/3880, Loss: 2.8481640815734863\n",
      "Batch 126/3880, Loss: 2.8683128356933594\n",
      "Batch 127/3880, Loss: 2.8299400806427\n",
      "Batch 128/3880, Loss: 2.8213531970977783\n",
      "Batch 129/3880, Loss: 2.868049144744873\n",
      "Batch 130/3880, Loss: 2.8327906131744385\n",
      "Batch 131/3880, Loss: 2.7777950763702393\n",
      "Batch 132/3880, Loss: 2.798755168914795\n",
      "Batch 133/3880, Loss: 2.76112699508667\n",
      "Batch 134/3880, Loss: 2.789142370223999\n",
      "Batch 135/3880, Loss: 2.7490711212158203\n",
      "Batch 136/3880, Loss: 2.7107765674591064\n",
      "Batch 137/3880, Loss: 2.7576146125793457\n",
      "Batch 138/3880, Loss: 2.6783084869384766\n",
      "Batch 139/3880, Loss: 2.729658603668213\n",
      "Batch 140/3880, Loss: 2.7030224800109863\n",
      "Batch 141/3880, Loss: 2.663707971572876\n",
      "Batch 142/3880, Loss: 2.68023681640625\n",
      "Batch 143/3880, Loss: 2.6950035095214844\n",
      "Batch 144/3880, Loss: 2.698913335800171\n",
      "Batch 145/3880, Loss: 2.652237892150879\n",
      "Batch 146/3880, Loss: 2.6517229080200195\n",
      "Batch 147/3880, Loss: 2.646237850189209\n",
      "Batch 148/3880, Loss: 2.648216724395752\n",
      "Batch 149/3880, Loss: 2.6213419437408447\n",
      "Batch 150/3880, Loss: 2.6390326023101807\n",
      "Batch 151/3880, Loss: 2.6176528930664062\n",
      "Batch 152/3880, Loss: 2.581315517425537\n",
      "Batch 153/3880, Loss: 2.55898380279541\n",
      "Batch 154/3880, Loss: 2.555656671524048\n",
      "Batch 155/3880, Loss: 2.546339988708496\n",
      "Batch 156/3880, Loss: 2.570831060409546\n",
      "Batch 157/3880, Loss: 2.5314395427703857\n",
      "Batch 158/3880, Loss: 2.5602471828460693\n",
      "Batch 159/3880, Loss: 2.522183656692505\n",
      "Batch 160/3880, Loss: 2.513956069946289\n",
      "Batch 161/3880, Loss: 2.522449016571045\n",
      "Batch 162/3880, Loss: 2.519132614135742\n",
      "Batch 163/3880, Loss: 2.482952833175659\n",
      "Batch 164/3880, Loss: 2.4765400886535645\n",
      "Batch 165/3880, Loss: 2.495482921600342\n",
      "Batch 166/3880, Loss: 2.448230743408203\n",
      "Batch 167/3880, Loss: 2.4304728507995605\n",
      "Batch 168/3880, Loss: 2.420370578765869\n",
      "Batch 169/3880, Loss: 2.435260772705078\n",
      "Batch 170/3880, Loss: 2.435124635696411\n",
      "Batch 171/3880, Loss: 2.3996856212615967\n",
      "Batch 172/3880, Loss: 2.4108455181121826\n",
      "Batch 173/3880, Loss: 2.4161386489868164\n",
      "Batch 174/3880, Loss: 2.3715155124664307\n",
      "Batch 175/3880, Loss: 2.3895037174224854\n",
      "Batch 176/3880, Loss: 2.3921053409576416\n",
      "Batch 177/3880, Loss: 2.3392891883850098\n",
      "Batch 178/3880, Loss: 2.357541799545288\n",
      "Batch 179/3880, Loss: 2.3067712783813477\n",
      "Batch 180/3880, Loss: 2.304460287094116\n",
      "Batch 181/3880, Loss: 2.305375337600708\n",
      "Batch 182/3880, Loss: 2.3341071605682373\n",
      "Batch 183/3880, Loss: 2.3034799098968506\n",
      "Batch 184/3880, Loss: 2.2385294437408447\n",
      "Batch 185/3880, Loss: 2.2807581424713135\n",
      "Batch 186/3880, Loss: 2.2698044776916504\n",
      "Batch 187/3880, Loss: 2.2340519428253174\n",
      "Batch 188/3880, Loss: 2.272433042526245\n",
      "Batch 189/3880, Loss: 2.219498872756958\n",
      "Batch 190/3880, Loss: 2.229757308959961\n",
      "Batch 191/3880, Loss: 2.2119851112365723\n",
      "Batch 192/3880, Loss: 2.1797685623168945\n",
      "Batch 193/3880, Loss: 2.196702480316162\n",
      "Batch 194/3880, Loss: 2.196793794631958\n",
      "Batch 195/3880, Loss: 2.163210391998291\n",
      "Batch 196/3880, Loss: 2.1782426834106445\n",
      "Batch 197/3880, Loss: 2.1610922813415527\n",
      "Batch 198/3880, Loss: 2.1758792400360107\n",
      "Batch 199/3880, Loss: 2.153169870376587\n",
      "Batch 200/3880, Loss: 2.1347503662109375\n",
      "Batch 201/3880, Loss: 2.1321723461151123\n",
      "Batch 202/3880, Loss: 2.1614179611206055\n",
      "Batch 203/3880, Loss: 2.1964852809906006\n",
      "Batch 204/3880, Loss: 2.163755178451538\n",
      "Batch 205/3880, Loss: 2.091432571411133\n",
      "Batch 206/3880, Loss: 2.02817702293396\n",
      "Batch 207/3880, Loss: 2.1277997493743896\n",
      "Batch 208/3880, Loss: 2.1084537506103516\n",
      "Batch 209/3880, Loss: 2.070812463760376\n",
      "Batch 210/3880, Loss: 2.132878541946411\n",
      "Batch 211/3880, Loss: 2.0137457847595215\n",
      "Batch 212/3880, Loss: 2.111909866333008\n",
      "Batch 213/3880, Loss: 2.0572993755340576\n",
      "Batch 214/3880, Loss: 2.05754017829895\n",
      "Batch 215/3880, Loss: 2.0181102752685547\n",
      "Batch 216/3880, Loss: 1.9590396881103516\n",
      "Batch 217/3880, Loss: 2.0243396759033203\n",
      "Batch 218/3880, Loss: 2.0111334323883057\n",
      "Batch 219/3880, Loss: 1.996998906135559\n",
      "Batch 220/3880, Loss: 1.980065107345581\n",
      "Batch 221/3880, Loss: 1.9462114572525024\n",
      "Batch 222/3880, Loss: 1.9896382093429565\n",
      "Batch 223/3880, Loss: 1.9879313707351685\n",
      "Batch 224/3880, Loss: 1.9638344049453735\n",
      "Batch 225/3880, Loss: 1.940863847732544\n",
      "Batch 226/3880, Loss: 1.9178439378738403\n",
      "Batch 227/3880, Loss: 1.893263339996338\n",
      "Batch 228/3880, Loss: 1.953296422958374\n",
      "Batch 229/3880, Loss: 1.9860478639602661\n",
      "Batch 230/3880, Loss: 1.9703766107559204\n",
      "Batch 231/3880, Loss: 1.897329568862915\n",
      "Batch 232/3880, Loss: 1.871566891670227\n",
      "Batch 233/3880, Loss: 1.9612739086151123\n",
      "Batch 234/3880, Loss: 1.8392444849014282\n",
      "Batch 235/3880, Loss: 1.8892115354537964\n",
      "Batch 236/3880, Loss: 1.8595921993255615\n",
      "Batch 237/3880, Loss: 1.9259905815124512\n",
      "Batch 238/3880, Loss: 1.8528791666030884\n",
      "Batch 239/3880, Loss: 1.7514084577560425\n",
      "Batch 240/3880, Loss: 1.8243939876556396\n",
      "Batch 241/3880, Loss: 1.913506031036377\n",
      "Batch 242/3880, Loss: 1.8887134790420532\n",
      "Batch 243/3880, Loss: 1.818057656288147\n",
      "Batch 244/3880, Loss: 1.7485102415084839\n",
      "Batch 245/3880, Loss: 1.8550270795822144\n",
      "Batch 246/3880, Loss: 1.8364040851593018\n",
      "Batch 247/3880, Loss: 1.860326886177063\n",
      "Batch 248/3880, Loss: 1.7096974849700928\n",
      "Batch 249/3880, Loss: 1.7238529920578003\n",
      "Batch 250/3880, Loss: 1.7479314804077148\n",
      "Batch 251/3880, Loss: 1.8703770637512207\n",
      "Batch 252/3880, Loss: 1.8325148820877075\n",
      "Batch 253/3880, Loss: 1.691980004310608\n",
      "Batch 254/3880, Loss: 1.809991717338562\n",
      "Batch 255/3880, Loss: 1.7189244031906128\n",
      "Batch 256/3880, Loss: 1.8966691493988037\n",
      "Batch 257/3880, Loss: 1.6943767070770264\n",
      "Batch 258/3880, Loss: 1.7904587984085083\n",
      "Batch 259/3880, Loss: 1.7980881929397583\n",
      "Batch 260/3880, Loss: 1.7730516195297241\n",
      "Batch 261/3880, Loss: 1.706342339515686\n",
      "Batch 262/3880, Loss: 1.7029350996017456\n",
      "Batch 263/3880, Loss: 1.7467197179794312\n",
      "Batch 264/3880, Loss: 1.780266523361206\n",
      "Batch 265/3880, Loss: 1.7174773216247559\n",
      "Batch 266/3880, Loss: 1.656003713607788\n",
      "Batch 267/3880, Loss: 1.7351951599121094\n",
      "Batch 268/3880, Loss: 1.8503600358963013\n",
      "Batch 269/3880, Loss: 1.5981090068817139\n",
      "Batch 270/3880, Loss: 1.5599114894866943\n",
      "Batch 271/3880, Loss: 1.7559524774551392\n",
      "Batch 272/3880, Loss: 1.7186763286590576\n",
      "Batch 273/3880, Loss: 1.6438995599746704\n",
      "Batch 274/3880, Loss: 1.6945252418518066\n",
      "Batch 275/3880, Loss: 1.5983245372772217\n",
      "Batch 276/3880, Loss: 1.6828867197036743\n",
      "Batch 277/3880, Loss: 1.5916460752487183\n",
      "Batch 278/3880, Loss: 1.6308331489562988\n",
      "Batch 279/3880, Loss: 1.5278855562210083\n",
      "Batch 280/3880, Loss: 1.5879416465759277\n",
      "Batch 281/3880, Loss: 1.7924941778182983\n",
      "Batch 282/3880, Loss: 1.672633409500122\n",
      "Batch 283/3880, Loss: 1.5630967617034912\n",
      "Batch 284/3880, Loss: 1.5088578462600708\n",
      "Batch 285/3880, Loss: 1.674207091331482\n",
      "Batch 286/3880, Loss: 1.3606476783752441\n",
      "Batch 287/3880, Loss: 1.609325885772705\n",
      "Batch 288/3880, Loss: 1.6662837266921997\n",
      "Batch 289/3880, Loss: 1.4266537427902222\n",
      "Batch 290/3880, Loss: 1.5284349918365479\n",
      "Batch 291/3880, Loss: 1.4175946712493896\n",
      "Batch 292/3880, Loss: 1.4062005281448364\n",
      "Batch 293/3880, Loss: 1.5056313276290894\n",
      "Batch 294/3880, Loss: 1.4953155517578125\n",
      "Batch 295/3880, Loss: 1.535859227180481\n",
      "Batch 296/3880, Loss: 1.6689980030059814\n",
      "Batch 297/3880, Loss: 1.6083018779754639\n",
      "Batch 298/3880, Loss: 1.508100152015686\n",
      "Batch 299/3880, Loss: 1.6589055061340332\n",
      "Batch 300/3880, Loss: 1.4180409908294678\n",
      "Batch 301/3880, Loss: 1.6623598337173462\n",
      "Batch 302/3880, Loss: 1.4739038944244385\n",
      "Batch 303/3880, Loss: 1.4484479427337646\n",
      "Batch 304/3880, Loss: 1.5695767402648926\n",
      "Batch 305/3880, Loss: 1.4291075468063354\n",
      "Batch 306/3880, Loss: 1.4830968379974365\n",
      "Batch 307/3880, Loss: 1.5370112657546997\n",
      "Batch 308/3880, Loss: 1.536161184310913\n",
      "Batch 309/3880, Loss: 1.5175529718399048\n",
      "Batch 310/3880, Loss: 1.5257335901260376\n",
      "Batch 311/3880, Loss: 1.4951854944229126\n",
      "Batch 312/3880, Loss: 1.4812910556793213\n",
      "Batch 313/3880, Loss: 1.6367934942245483\n",
      "Batch 314/3880, Loss: 1.4733444452285767\n",
      "Batch 315/3880, Loss: 1.5960986614227295\n",
      "Batch 316/3880, Loss: 1.5172886848449707\n",
      "Batch 317/3880, Loss: 1.4227476119995117\n",
      "Batch 318/3880, Loss: 1.499293327331543\n",
      "Batch 319/3880, Loss: 1.460923671722412\n",
      "Batch 320/3880, Loss: 1.3950896263122559\n",
      "Batch 321/3880, Loss: 1.3649206161499023\n",
      "Batch 322/3880, Loss: 1.421736478805542\n",
      "Batch 323/3880, Loss: 1.7683249711990356\n",
      "Batch 324/3880, Loss: 1.3972376585006714\n",
      "Batch 325/3880, Loss: 1.4157447814941406\n",
      "Batch 326/3880, Loss: 1.3765002489089966\n",
      "Batch 327/3880, Loss: 1.3908255100250244\n",
      "Batch 328/3880, Loss: 1.4182769060134888\n",
      "Batch 329/3880, Loss: 1.4674452543258667\n",
      "Batch 330/3880, Loss: 1.3552992343902588\n",
      "Batch 331/3880, Loss: 1.3350716829299927\n",
      "Batch 332/3880, Loss: 1.3087759017944336\n",
      "Batch 333/3880, Loss: 1.4334471225738525\n",
      "Batch 334/3880, Loss: 1.384218454360962\n",
      "Batch 335/3880, Loss: 1.1980499029159546\n",
      "Batch 336/3880, Loss: 1.464907169342041\n",
      "Batch 337/3880, Loss: 1.330641269683838\n",
      "Batch 338/3880, Loss: 1.4100819826126099\n",
      "Batch 339/3880, Loss: 1.2572662830352783\n",
      "Batch 340/3880, Loss: 1.4926213026046753\n",
      "Batch 341/3880, Loss: 1.5113605260849\n",
      "Batch 342/3880, Loss: 1.2969218492507935\n",
      "Batch 343/3880, Loss: 1.3324275016784668\n",
      "Batch 344/3880, Loss: 1.557173490524292\n",
      "Batch 345/3880, Loss: 1.629457712173462\n",
      "Batch 346/3880, Loss: 1.6005804538726807\n",
      "Batch 347/3880, Loss: 1.4570690393447876\n",
      "Batch 348/3880, Loss: 1.228212594985962\n",
      "Batch 349/3880, Loss: 1.4251493215560913\n",
      "Batch 350/3880, Loss: 1.332828164100647\n",
      "Batch 351/3880, Loss: 1.5310678482055664\n",
      "Batch 352/3880, Loss: 1.2607886791229248\n",
      "Batch 353/3880, Loss: 1.2378123998641968\n",
      "Batch 354/3880, Loss: 1.3463249206542969\n",
      "Batch 355/3880, Loss: 1.3182772397994995\n",
      "Batch 356/3880, Loss: 1.290112018585205\n",
      "Batch 357/3880, Loss: 1.4105935096740723\n",
      "Batch 358/3880, Loss: 1.3071529865264893\n",
      "Batch 359/3880, Loss: 1.4052236080169678\n",
      "Batch 360/3880, Loss: 1.249083161354065\n",
      "Batch 361/3880, Loss: 1.268363118171692\n",
      "Batch 362/3880, Loss: 1.2823792695999146\n",
      "Batch 363/3880, Loss: 1.0354949235916138\n",
      "Batch 364/3880, Loss: 1.5023795366287231\n",
      "Batch 365/3880, Loss: 1.2345646619796753\n",
      "Batch 366/3880, Loss: 1.268656611442566\n",
      "Batch 367/3880, Loss: 1.053492546081543\n",
      "Batch 368/3880, Loss: 1.3962414264678955\n",
      "Batch 369/3880, Loss: 1.1649504899978638\n",
      "Batch 370/3880, Loss: 1.490142822265625\n",
      "Batch 371/3880, Loss: 1.404702067375183\n",
      "Batch 372/3880, Loss: 1.2130318880081177\n",
      "Batch 373/3880, Loss: 1.333645224571228\n",
      "Batch 374/3880, Loss: 1.246281623840332\n",
      "Batch 375/3880, Loss: 1.2590522766113281\n",
      "Batch 376/3880, Loss: 1.1888419389724731\n",
      "Batch 377/3880, Loss: 1.3406544923782349\n",
      "Batch 378/3880, Loss: 1.3429945707321167\n",
      "Batch 379/3880, Loss: 1.2840722799301147\n",
      "Batch 380/3880, Loss: 1.382975459098816\n",
      "Batch 381/3880, Loss: 1.2315819263458252\n",
      "Batch 382/3880, Loss: 1.43562912940979\n",
      "Batch 383/3880, Loss: 1.3387788534164429\n",
      "Batch 384/3880, Loss: 1.5233867168426514\n",
      "Batch 385/3880, Loss: 1.273682713508606\n",
      "Batch 386/3880, Loss: 1.2348263263702393\n",
      "Batch 387/3880, Loss: 1.499007225036621\n",
      "Batch 388/3880, Loss: 1.2455986738204956\n",
      "Batch 389/3880, Loss: 1.3319498300552368\n",
      "Batch 390/3880, Loss: 1.294999122619629\n",
      "Batch 391/3880, Loss: 1.359775185585022\n",
      "Batch 392/3880, Loss: 1.3526700735092163\n",
      "Batch 393/3880, Loss: 1.2403559684753418\n",
      "Batch 394/3880, Loss: 1.2637298107147217\n",
      "Batch 395/3880, Loss: 1.4195187091827393\n",
      "Batch 396/3880, Loss: 1.2091169357299805\n",
      "Batch 397/3880, Loss: 1.2195178270339966\n",
      "Batch 398/3880, Loss: 1.264214277267456\n",
      "Batch 399/3880, Loss: 1.23396897315979\n",
      "Batch 400/3880, Loss: 1.2700705528259277\n",
      "Batch 401/3880, Loss: 1.5891783237457275\n",
      "Batch 402/3880, Loss: 1.4240747690200806\n",
      "Batch 403/3880, Loss: 1.205833077430725\n",
      "Batch 404/3880, Loss: 1.1666574478149414\n",
      "Batch 405/3880, Loss: 1.2621638774871826\n",
      "Batch 406/3880, Loss: 1.1738524436950684\n",
      "Batch 407/3880, Loss: 1.1257771253585815\n",
      "Batch 408/3880, Loss: 1.3837841749191284\n",
      "Batch 409/3880, Loss: 1.0216915607452393\n",
      "Batch 410/3880, Loss: 1.2898036241531372\n",
      "Batch 411/3880, Loss: 1.255303144454956\n",
      "Batch 412/3880, Loss: 1.2406295537948608\n",
      "Batch 413/3880, Loss: 1.5165244340896606\n",
      "Batch 414/3880, Loss: 1.1486667394638062\n",
      "Batch 415/3880, Loss: 1.2525501251220703\n",
      "Batch 416/3880, Loss: 1.4898616075515747\n",
      "Batch 417/3880, Loss: 1.4944473505020142\n",
      "Batch 418/3880, Loss: 1.1991223096847534\n",
      "Batch 419/3880, Loss: 1.3855817317962646\n",
      "Batch 420/3880, Loss: 1.1526007652282715\n",
      "Batch 421/3880, Loss: 1.2368336915969849\n",
      "Batch 422/3880, Loss: 1.4113132953643799\n",
      "Batch 423/3880, Loss: 1.2024598121643066\n",
      "Batch 424/3880, Loss: 1.1392205953598022\n",
      "Batch 425/3880, Loss: 1.211841344833374\n",
      "Batch 426/3880, Loss: 1.4066288471221924\n",
      "Batch 427/3880, Loss: 1.276125192642212\n",
      "Batch 428/3880, Loss: 1.2686256170272827\n",
      "Batch 429/3880, Loss: 1.2655956745147705\n",
      "Batch 430/3880, Loss: 1.3660433292388916\n",
      "Batch 431/3880, Loss: 1.2668793201446533\n",
      "Batch 432/3880, Loss: 1.2801603078842163\n",
      "Batch 433/3880, Loss: 1.0359241962432861\n",
      "Batch 434/3880, Loss: 1.4226152896881104\n",
      "Batch 435/3880, Loss: 1.0509096384048462\n",
      "Batch 436/3880, Loss: 1.2743803262710571\n",
      "Batch 437/3880, Loss: 1.1399403810501099\n",
      "Batch 438/3880, Loss: 1.0658050775527954\n",
      "Batch 439/3880, Loss: 1.3384132385253906\n",
      "Batch 440/3880, Loss: 1.2094775438308716\n",
      "Batch 441/3880, Loss: 1.0123358964920044\n",
      "Batch 442/3880, Loss: 1.0700377225875854\n",
      "Batch 443/3880, Loss: 1.0840978622436523\n",
      "Batch 444/3880, Loss: 1.2033642530441284\n",
      "Batch 445/3880, Loss: 1.1192883253097534\n",
      "Batch 446/3880, Loss: 1.2595412731170654\n",
      "Batch 447/3880, Loss: 1.391076922416687\n",
      "Batch 448/3880, Loss: 1.453545331954956\n",
      "Batch 449/3880, Loss: 1.2045742273330688\n",
      "Batch 450/3880, Loss: 1.0711145401000977\n",
      "Batch 451/3880, Loss: 1.1001784801483154\n",
      "Batch 452/3880, Loss: 1.2554751634597778\n",
      "Batch 453/3880, Loss: 1.2088956832885742\n",
      "Batch 454/3880, Loss: 1.1803501844406128\n",
      "Batch 455/3880, Loss: 1.3720109462738037\n",
      "Batch 456/3880, Loss: 1.2146811485290527\n",
      "Batch 457/3880, Loss: 1.1494444608688354\n",
      "Batch 458/3880, Loss: 1.201439619064331\n",
      "Batch 459/3880, Loss: 1.0135406255722046\n",
      "Batch 460/3880, Loss: 1.1238212585449219\n",
      "Batch 461/3880, Loss: 1.1716500520706177\n",
      "Batch 462/3880, Loss: 1.1206556558609009\n",
      "Batch 463/3880, Loss: 1.2545982599258423\n",
      "Batch 464/3880, Loss: 1.4782640933990479\n",
      "Batch 465/3880, Loss: 1.3391215801239014\n",
      "Batch 466/3880, Loss: 0.9773873090744019\n",
      "Batch 467/3880, Loss: 1.2584574222564697\n",
      "Batch 468/3880, Loss: 1.235625982284546\n",
      "Batch 469/3880, Loss: 1.187153697013855\n",
      "Batch 470/3880, Loss: 1.1978421211242676\n",
      "Batch 471/3880, Loss: 1.2667309045791626\n",
      "Batch 472/3880, Loss: 1.1056371927261353\n",
      "Batch 473/3880, Loss: 1.114308476448059\n",
      "Batch 474/3880, Loss: 1.1491284370422363\n",
      "Batch 475/3880, Loss: 1.258103847503662\n",
      "Batch 476/3880, Loss: 1.2373123168945312\n",
      "Batch 477/3880, Loss: 1.1848121881484985\n",
      "Batch 478/3880, Loss: 1.2149142026901245\n",
      "Batch 479/3880, Loss: 1.2932442426681519\n",
      "Batch 480/3880, Loss: 1.2704651355743408\n",
      "Batch 481/3880, Loss: 1.2180083990097046\n",
      "Batch 482/3880, Loss: 1.1219689846038818\n",
      "Batch 483/3880, Loss: 1.223458170890808\n",
      "Batch 484/3880, Loss: 1.1235313415527344\n",
      "Batch 485/3880, Loss: 1.4050103425979614\n",
      "Batch 486/3880, Loss: 1.0383914709091187\n",
      "Batch 487/3880, Loss: 1.239127516746521\n",
      "Batch 488/3880, Loss: 1.043068289756775\n",
      "Batch 489/3880, Loss: 1.0943669080734253\n",
      "Batch 490/3880, Loss: 1.18192458152771\n",
      "Batch 491/3880, Loss: 1.1576030254364014\n",
      "Batch 492/3880, Loss: 1.0851678848266602\n",
      "Batch 493/3880, Loss: 1.1128636598587036\n",
      "Batch 494/3880, Loss: 1.2590856552124023\n",
      "Batch 495/3880, Loss: 1.2952760457992554\n",
      "Batch 496/3880, Loss: 1.1532810926437378\n",
      "Batch 497/3880, Loss: 1.082624912261963\n",
      "Batch 498/3880, Loss: 1.336578369140625\n",
      "Batch 499/3880, Loss: 1.3751956224441528\n",
      "Batch 500/3880, Loss: 1.0320470333099365\n",
      "Batch 501/3880, Loss: 1.1173673868179321\n",
      "Batch 502/3880, Loss: 1.2577934265136719\n",
      "Batch 503/3880, Loss: 1.3953125476837158\n",
      "Batch 504/3880, Loss: 1.1178873777389526\n",
      "Batch 505/3880, Loss: 1.351883053779602\n",
      "Batch 506/3880, Loss: 1.3390555381774902\n",
      "Batch 507/3880, Loss: 1.1427243947982788\n",
      "Batch 508/3880, Loss: 1.1003097295761108\n",
      "Batch 509/3880, Loss: 0.901143491268158\n",
      "Batch 510/3880, Loss: 1.265800952911377\n",
      "Batch 511/3880, Loss: 1.198161005973816\n",
      "Batch 512/3880, Loss: 1.1955407857894897\n",
      "Batch 513/3880, Loss: 1.3437541723251343\n",
      "Batch 514/3880, Loss: 1.0858209133148193\n",
      "Batch 515/3880, Loss: 1.0859379768371582\n",
      "Batch 516/3880, Loss: 1.1526217460632324\n",
      "Batch 517/3880, Loss: 1.2333635091781616\n",
      "Batch 518/3880, Loss: 0.9339193105697632\n",
      "Batch 519/3880, Loss: 1.1520609855651855\n",
      "Batch 520/3880, Loss: 1.0170670747756958\n",
      "Batch 521/3880, Loss: 1.314507007598877\n",
      "Batch 522/3880, Loss: 1.1060547828674316\n",
      "Batch 523/3880, Loss: 1.3100498914718628\n",
      "Batch 524/3880, Loss: 1.2033365964889526\n",
      "Batch 525/3880, Loss: 1.033021330833435\n",
      "Batch 526/3880, Loss: 1.0887371301651\n",
      "Batch 527/3880, Loss: 1.231040120124817\n",
      "Batch 528/3880, Loss: 1.3574851751327515\n",
      "Batch 529/3880, Loss: 1.168481469154358\n",
      "Batch 530/3880, Loss: 1.056707739830017\n",
      "Batch 531/3880, Loss: 0.985464334487915\n",
      "Batch 532/3880, Loss: 1.1470530033111572\n",
      "Batch 533/3880, Loss: 1.2252602577209473\n",
      "Batch 534/3880, Loss: 1.1319831609725952\n",
      "Batch 535/3880, Loss: 1.0045955181121826\n",
      "Batch 536/3880, Loss: 1.2138712406158447\n",
      "Batch 537/3880, Loss: 1.3102145195007324\n",
      "Batch 538/3880, Loss: 1.2323780059814453\n",
      "Batch 539/3880, Loss: 0.8846541047096252\n",
      "Batch 540/3880, Loss: 1.2740604877471924\n",
      "Batch 541/3880, Loss: 1.2617928981781006\n",
      "Batch 542/3880, Loss: 1.4244053363800049\n",
      "Batch 543/3880, Loss: 1.293552279472351\n",
      "Batch 544/3880, Loss: 1.3145692348480225\n",
      "Batch 545/3880, Loss: 1.2243502140045166\n",
      "Batch 546/3880, Loss: 0.9916290044784546\n",
      "Batch 547/3880, Loss: 1.1340117454528809\n",
      "Batch 548/3880, Loss: 1.135666012763977\n",
      "Batch 549/3880, Loss: 1.2837719917297363\n",
      "Batch 550/3880, Loss: 1.0383782386779785\n",
      "Batch 551/3880, Loss: 0.9550265669822693\n",
      "Batch 552/3880, Loss: 1.0841233730316162\n",
      "Batch 553/3880, Loss: 1.1321351528167725\n",
      "Batch 554/3880, Loss: 1.3197664022445679\n",
      "Batch 555/3880, Loss: 0.9555631279945374\n",
      "Batch 556/3880, Loss: 1.460334062576294\n",
      "Batch 557/3880, Loss: 1.2860965728759766\n",
      "Batch 558/3880, Loss: 1.401137351989746\n",
      "Batch 559/3880, Loss: 1.0880202054977417\n",
      "Batch 560/3880, Loss: 1.0879690647125244\n",
      "Batch 561/3880, Loss: 1.403762936592102\n",
      "Batch 562/3880, Loss: 1.2564043998718262\n",
      "Batch 563/3880, Loss: 1.2565962076187134\n",
      "Batch 564/3880, Loss: 1.2118207216262817\n",
      "Batch 565/3880, Loss: 1.1078155040740967\n",
      "Batch 566/3880, Loss: 1.0242160558700562\n",
      "Batch 567/3880, Loss: 1.1873986721038818\n",
      "Batch 568/3880, Loss: 1.0434818267822266\n",
      "Batch 569/3880, Loss: 1.2581974267959595\n",
      "Batch 570/3880, Loss: 1.4200890064239502\n",
      "Batch 571/3880, Loss: 1.1572462320327759\n",
      "Batch 572/3880, Loss: 1.2476670742034912\n",
      "Batch 573/3880, Loss: 1.1469712257385254\n",
      "Batch 574/3880, Loss: 1.1247708797454834\n",
      "Batch 575/3880, Loss: 1.0659849643707275\n",
      "Batch 576/3880, Loss: 1.0576462745666504\n",
      "Batch 577/3880, Loss: 1.0454459190368652\n",
      "Batch 578/3880, Loss: 1.0172126293182373\n",
      "Batch 579/3880, Loss: 1.1892585754394531\n",
      "Batch 580/3880, Loss: 1.2699625492095947\n",
      "Batch 581/3880, Loss: 1.3039504289627075\n",
      "Batch 582/3880, Loss: 1.0284534692764282\n",
      "Batch 583/3880, Loss: 1.0839347839355469\n",
      "Batch 584/3880, Loss: 1.23207426071167\n",
      "Batch 585/3880, Loss: 1.3224278688430786\n",
      "Batch 586/3880, Loss: 1.1642588376998901\n",
      "Batch 587/3880, Loss: 1.326275110244751\n",
      "Batch 588/3880, Loss: 1.0346927642822266\n",
      "Batch 589/3880, Loss: 0.9726635217666626\n",
      "Batch 590/3880, Loss: 0.9133532643318176\n",
      "Batch 591/3880, Loss: 1.0929399728775024\n",
      "Batch 592/3880, Loss: 1.0128093957901\n",
      "Batch 593/3880, Loss: 1.128788948059082\n",
      "Batch 594/3880, Loss: 1.091436743736267\n",
      "Batch 595/3880, Loss: 1.3244346380233765\n",
      "Batch 596/3880, Loss: 1.1927250623703003\n",
      "Batch 597/3880, Loss: 1.5410679578781128\n",
      "Batch 598/3880, Loss: 1.1071343421936035\n",
      "Batch 599/3880, Loss: 1.2750244140625\n",
      "Batch 600/3880, Loss: 1.0482923984527588\n",
      "Batch 601/3880, Loss: 1.1497201919555664\n",
      "Batch 602/3880, Loss: 1.0009377002716064\n",
      "Batch 603/3880, Loss: 1.2616186141967773\n",
      "Batch 604/3880, Loss: 1.1882030963897705\n",
      "Batch 605/3880, Loss: 1.2396272420883179\n",
      "Batch 606/3880, Loss: 1.1146525144577026\n",
      "Batch 607/3880, Loss: 0.9508019089698792\n",
      "Batch 608/3880, Loss: 1.0962125062942505\n",
      "Batch 609/3880, Loss: 1.3643903732299805\n",
      "Batch 610/3880, Loss: 0.883590042591095\n",
      "Batch 611/3880, Loss: 1.1132314205169678\n",
      "Batch 612/3880, Loss: 1.3076859712600708\n",
      "Batch 613/3880, Loss: 1.0960057973861694\n",
      "Batch 614/3880, Loss: 1.1178271770477295\n",
      "Batch 615/3880, Loss: 0.9621508121490479\n",
      "Batch 616/3880, Loss: 1.1560795307159424\n",
      "Batch 617/3880, Loss: 1.0091594457626343\n",
      "Batch 618/3880, Loss: 1.3018630743026733\n",
      "Batch 619/3880, Loss: 1.0393410921096802\n",
      "Batch 620/3880, Loss: 1.258193016052246\n",
      "Batch 621/3880, Loss: 0.9768460392951965\n",
      "Batch 622/3880, Loss: 1.3219484090805054\n",
      "Batch 623/3880, Loss: 1.0694665908813477\n",
      "Batch 624/3880, Loss: 1.1961479187011719\n",
      "Batch 625/3880, Loss: 0.9262061715126038\n",
      "Batch 626/3880, Loss: 0.9611283540725708\n",
      "Batch 627/3880, Loss: 1.0458979606628418\n",
      "Batch 628/3880, Loss: 1.092140793800354\n",
      "Batch 629/3880, Loss: 1.0487046241760254\n",
      "Batch 630/3880, Loss: 1.4762299060821533\n",
      "Batch 631/3880, Loss: 1.0523492097854614\n",
      "Batch 632/3880, Loss: 1.202998161315918\n",
      "Batch 633/3880, Loss: 1.3632384538650513\n",
      "Batch 634/3880, Loss: 1.108494520187378\n",
      "Batch 635/3880, Loss: 1.1699587106704712\n",
      "Batch 636/3880, Loss: 1.0206125974655151\n",
      "Batch 637/3880, Loss: 1.3194400072097778\n",
      "Batch 638/3880, Loss: 0.9987167119979858\n",
      "Batch 639/3880, Loss: 1.0113871097564697\n",
      "Batch 640/3880, Loss: 1.0075032711029053\n",
      "Batch 641/3880, Loss: 1.2720142602920532\n",
      "Batch 642/3880, Loss: 0.8663578033447266\n",
      "Batch 643/3880, Loss: 0.9936926960945129\n",
      "Batch 644/3880, Loss: 1.170357346534729\n",
      "Batch 645/3880, Loss: 1.0833240747451782\n",
      "Batch 646/3880, Loss: 1.059410572052002\n",
      "Batch 647/3880, Loss: 1.0229332447052002\n",
      "Batch 648/3880, Loss: 0.9486870765686035\n",
      "Batch 649/3880, Loss: 1.38998281955719\n",
      "Batch 650/3880, Loss: 1.3123449087142944\n",
      "Batch 651/3880, Loss: 0.9060900807380676\n",
      "Batch 652/3880, Loss: 1.2630422115325928\n",
      "Batch 653/3880, Loss: 1.2491780519485474\n",
      "Batch 654/3880, Loss: 1.0775041580200195\n",
      "Batch 655/3880, Loss: 1.058542251586914\n",
      "Batch 656/3880, Loss: 1.0465432405471802\n",
      "Batch 657/3880, Loss: 1.4231066703796387\n",
      "Batch 658/3880, Loss: 1.1662393808364868\n",
      "Batch 659/3880, Loss: 1.0854618549346924\n",
      "Batch 660/3880, Loss: 1.180861473083496\n",
      "Batch 661/3880, Loss: 1.087937593460083\n",
      "Batch 662/3880, Loss: 1.09188973903656\n",
      "Batch 663/3880, Loss: 0.9327373504638672\n",
      "Batch 664/3880, Loss: 1.0849850177764893\n",
      "Batch 665/3880, Loss: 1.0593161582946777\n",
      "Batch 666/3880, Loss: 1.0597822666168213\n",
      "Batch 667/3880, Loss: 1.0840506553649902\n",
      "Batch 668/3880, Loss: 1.002671241760254\n",
      "Batch 669/3880, Loss: 1.117474913597107\n",
      "Batch 670/3880, Loss: 0.996840238571167\n",
      "Batch 671/3880, Loss: 1.2314140796661377\n",
      "Batch 672/3880, Loss: 0.9523361325263977\n",
      "Batch 673/3880, Loss: 1.269907832145691\n",
      "Batch 674/3880, Loss: 1.4412195682525635\n",
      "Batch 675/3880, Loss: 1.1475701332092285\n",
      "Batch 676/3880, Loss: 0.9862140417098999\n",
      "Batch 677/3880, Loss: 1.1393367052078247\n",
      "Batch 678/3880, Loss: 1.0419409275054932\n",
      "Batch 679/3880, Loss: 1.3273600339889526\n",
      "Batch 680/3880, Loss: 1.0414113998413086\n",
      "Batch 681/3880, Loss: 1.1248588562011719\n",
      "Batch 682/3880, Loss: 1.050550937652588\n",
      "Batch 683/3880, Loss: 1.0731223821640015\n",
      "Batch 684/3880, Loss: 1.350358009338379\n",
      "Batch 685/3880, Loss: 1.1489437818527222\n",
      "Batch 686/3880, Loss: 1.1050829887390137\n",
      "Batch 687/3880, Loss: 1.2147613763809204\n",
      "Batch 688/3880, Loss: 1.2306348085403442\n",
      "Batch 689/3880, Loss: 1.1959002017974854\n",
      "Batch 690/3880, Loss: 1.3153979778289795\n",
      "Batch 691/3880, Loss: 1.0390141010284424\n",
      "Batch 692/3880, Loss: 1.042793869972229\n",
      "Batch 693/3880, Loss: 1.1424790620803833\n",
      "Batch 694/3880, Loss: 1.055407166481018\n",
      "Batch 695/3880, Loss: 1.0582139492034912\n",
      "Batch 696/3880, Loss: 1.099355936050415\n",
      "Batch 697/3880, Loss: 0.9720855951309204\n",
      "Batch 698/3880, Loss: 1.0453640222549438\n",
      "Batch 699/3880, Loss: 1.0634257793426514\n",
      "Batch 700/3880, Loss: 1.1140767335891724\n",
      "Batch 701/3880, Loss: 0.7948417067527771\n",
      "Batch 702/3880, Loss: 0.9664026498794556\n",
      "Batch 703/3880, Loss: 1.060101866722107\n",
      "Batch 704/3880, Loss: 1.1125928163528442\n",
      "Batch 705/3880, Loss: 1.090263843536377\n",
      "Batch 706/3880, Loss: 1.0859354734420776\n",
      "Batch 707/3880, Loss: 1.2972288131713867\n",
      "Batch 708/3880, Loss: 0.9432934522628784\n",
      "Batch 709/3880, Loss: 1.052322268486023\n",
      "Batch 710/3880, Loss: 1.1202985048294067\n",
      "Batch 711/3880, Loss: 1.1240143775939941\n",
      "Batch 712/3880, Loss: 1.1952672004699707\n",
      "Batch 713/3880, Loss: 1.1274183988571167\n",
      "Batch 714/3880, Loss: 1.1897456645965576\n",
      "Batch 715/3880, Loss: 0.8665509223937988\n",
      "Batch 716/3880, Loss: 1.107039451599121\n",
      "Batch 717/3880, Loss: 1.1769651174545288\n",
      "Batch 718/3880, Loss: 1.0632858276367188\n",
      "Batch 719/3880, Loss: 1.0852982997894287\n",
      "Batch 720/3880, Loss: 1.1861093044281006\n",
      "Batch 721/3880, Loss: 0.9611196517944336\n",
      "Batch 722/3880, Loss: 1.387505292892456\n",
      "Batch 723/3880, Loss: 1.382062554359436\n",
      "Batch 724/3880, Loss: 1.013060450553894\n",
      "Batch 725/3880, Loss: 1.0070627927780151\n",
      "Batch 726/3880, Loss: 0.9574592709541321\n",
      "Batch 727/3880, Loss: 1.187071681022644\n",
      "Batch 728/3880, Loss: 1.2330564260482788\n",
      "Batch 729/3880, Loss: 1.0349531173706055\n",
      "Batch 730/3880, Loss: 1.0937049388885498\n",
      "Batch 731/3880, Loss: 1.200566291809082\n",
      "Batch 732/3880, Loss: 1.206365942955017\n",
      "Batch 733/3880, Loss: 1.0613032579421997\n",
      "Batch 734/3880, Loss: 1.236517071723938\n",
      "Batch 735/3880, Loss: 1.242035150527954\n",
      "Batch 736/3880, Loss: 1.0634217262268066\n",
      "Batch 737/3880, Loss: 1.2191108465194702\n",
      "Batch 738/3880, Loss: 1.1976650953292847\n",
      "Batch 739/3880, Loss: 1.2723056077957153\n",
      "Batch 740/3880, Loss: 1.1656692028045654\n",
      "Batch 741/3880, Loss: 1.3869414329528809\n",
      "Batch 742/3880, Loss: 1.0051088333129883\n",
      "Batch 743/3880, Loss: 1.1488744020462036\n",
      "Batch 744/3880, Loss: 1.084845781326294\n",
      "Batch 745/3880, Loss: 1.2261784076690674\n",
      "Batch 746/3880, Loss: 0.9331912398338318\n",
      "Batch 747/3880, Loss: 0.9581090807914734\n",
      "Batch 748/3880, Loss: 1.0827562808990479\n",
      "Batch 749/3880, Loss: 1.03070867061615\n",
      "Batch 750/3880, Loss: 1.2557803392410278\n",
      "Batch 751/3880, Loss: 1.0477014780044556\n",
      "Batch 752/3880, Loss: 0.9436235427856445\n",
      "Batch 753/3880, Loss: 1.0767451524734497\n",
      "Batch 754/3880, Loss: 0.9650202393531799\n",
      "Batch 755/3880, Loss: 1.1183639764785767\n",
      "Batch 756/3880, Loss: 1.0120007991790771\n",
      "Batch 757/3880, Loss: 0.92845219373703\n",
      "Batch 758/3880, Loss: 0.9257382750511169\n",
      "Batch 759/3880, Loss: 1.1469600200653076\n",
      "Batch 760/3880, Loss: 0.8310871720314026\n",
      "Batch 761/3880, Loss: 1.1734484434127808\n",
      "Batch 762/3880, Loss: 1.0020357370376587\n",
      "Batch 763/3880, Loss: 1.0099446773529053\n",
      "Batch 764/3880, Loss: 1.22391676902771\n",
      "Batch 765/3880, Loss: 1.16452956199646\n",
      "Batch 766/3880, Loss: 1.0721864700317383\n",
      "Batch 767/3880, Loss: 1.1270062923431396\n",
      "Batch 768/3880, Loss: 1.231422781944275\n",
      "Batch 769/3880, Loss: 1.0458202362060547\n",
      "Batch 770/3880, Loss: 1.198887825012207\n",
      "Batch 771/3880, Loss: 0.8741158843040466\n",
      "Batch 772/3880, Loss: 1.2892283201217651\n",
      "Batch 773/3880, Loss: 0.9719291925430298\n",
      "Batch 774/3880, Loss: 0.8966532349586487\n",
      "Batch 775/3880, Loss: 1.3144766092300415\n",
      "Batch 776/3880, Loss: 0.928153395652771\n",
      "Batch 777/3880, Loss: 1.1833877563476562\n",
      "Batch 778/3880, Loss: 1.0066022872924805\n",
      "Batch 779/3880, Loss: 1.2557425498962402\n",
      "Batch 780/3880, Loss: 1.0057324171066284\n",
      "Batch 781/3880, Loss: 1.2486532926559448\n",
      "Batch 782/3880, Loss: 1.1674609184265137\n",
      "Batch 783/3880, Loss: 1.076772689819336\n",
      "Batch 784/3880, Loss: 1.1890476942062378\n",
      "Batch 785/3880, Loss: 1.1134926080703735\n",
      "Batch 786/3880, Loss: 1.0844868421554565\n",
      "Batch 787/3880, Loss: 0.97447669506073\n",
      "Batch 788/3880, Loss: 0.9466250538825989\n",
      "Batch 789/3880, Loss: 0.952256977558136\n",
      "Batch 790/3880, Loss: 1.1873338222503662\n",
      "Batch 791/3880, Loss: 0.8817741870880127\n",
      "Batch 792/3880, Loss: 1.4181787967681885\n",
      "Batch 793/3880, Loss: 1.231911063194275\n",
      "Batch 794/3880, Loss: 1.282612919807434\n",
      "Batch 795/3880, Loss: 1.1634414196014404\n",
      "Batch 796/3880, Loss: 1.380885362625122\n",
      "Batch 797/3880, Loss: 1.0969032049179077\n",
      "Batch 798/3880, Loss: 1.0549856424331665\n",
      "Batch 799/3880, Loss: 1.036811351776123\n",
      "Batch 800/3880, Loss: 1.0177534818649292\n",
      "Batch 801/3880, Loss: 0.8517805933952332\n",
      "Batch 802/3880, Loss: 1.0871585607528687\n",
      "Batch 803/3880, Loss: 1.267488718032837\n",
      "Batch 804/3880, Loss: 1.3080207109451294\n",
      "Batch 805/3880, Loss: 0.9821918606758118\n",
      "Batch 806/3880, Loss: 1.3139172792434692\n",
      "Batch 807/3880, Loss: 1.0596078634262085\n",
      "Batch 808/3880, Loss: 1.130456805229187\n",
      "Batch 809/3880, Loss: 0.9979465007781982\n",
      "Batch 810/3880, Loss: 1.095977544784546\n",
      "Batch 811/3880, Loss: 0.8465796709060669\n",
      "Batch 812/3880, Loss: 0.9164292216300964\n",
      "Batch 813/3880, Loss: 1.230544924736023\n",
      "Batch 814/3880, Loss: 1.1394076347351074\n",
      "Batch 815/3880, Loss: 1.1863025426864624\n",
      "Batch 816/3880, Loss: 0.9133516550064087\n",
      "Batch 817/3880, Loss: 0.8121837377548218\n",
      "Batch 818/3880, Loss: 1.0718510150909424\n",
      "Batch 819/3880, Loss: 0.9353256225585938\n",
      "Batch 820/3880, Loss: 1.0296777486801147\n",
      "Batch 821/3880, Loss: 1.0972052812576294\n",
      "Batch 822/3880, Loss: 1.08187735080719\n",
      "Batch 823/3880, Loss: 1.3323755264282227\n",
      "Batch 824/3880, Loss: 1.1050031185150146\n",
      "Batch 825/3880, Loss: 0.9254520535469055\n",
      "Batch 826/3880, Loss: 1.0882298946380615\n",
      "Batch 827/3880, Loss: 1.0741573572158813\n",
      "Batch 828/3880, Loss: 1.0942511558532715\n",
      "Batch 829/3880, Loss: 1.2254343032836914\n",
      "Batch 830/3880, Loss: 1.019025444984436\n",
      "Batch 831/3880, Loss: 1.1038326025009155\n",
      "Batch 832/3880, Loss: 1.1072157621383667\n",
      "Batch 833/3880, Loss: 1.176543951034546\n",
      "Batch 834/3880, Loss: 1.065222144126892\n",
      "Batch 835/3880, Loss: 1.3720005750656128\n",
      "Batch 836/3880, Loss: 0.9078354239463806\n",
      "Batch 837/3880, Loss: 1.1467103958129883\n",
      "Batch 838/3880, Loss: 1.0629111528396606\n",
      "Batch 839/3880, Loss: 1.1174116134643555\n",
      "Batch 840/3880, Loss: 1.0554883480072021\n",
      "Batch 841/3880, Loss: 1.2701951265335083\n",
      "Batch 842/3880, Loss: 1.009997844696045\n",
      "Batch 843/3880, Loss: 1.0899527072906494\n",
      "Batch 844/3880, Loss: 1.0308623313903809\n",
      "Batch 845/3880, Loss: 1.1915580034255981\n",
      "Batch 846/3880, Loss: 0.9075155854225159\n",
      "Batch 847/3880, Loss: 1.054944396018982\n",
      "Batch 848/3880, Loss: 1.094434142112732\n",
      "Batch 849/3880, Loss: 1.10954749584198\n",
      "Batch 850/3880, Loss: 1.0395621061325073\n",
      "Batch 851/3880, Loss: 1.1478979587554932\n",
      "Batch 852/3880, Loss: 1.172939658164978\n",
      "Batch 853/3880, Loss: 1.0754438638687134\n",
      "Batch 854/3880, Loss: 1.0717133283615112\n",
      "Batch 855/3880, Loss: 1.212302327156067\n",
      "Batch 856/3880, Loss: 0.856158971786499\n",
      "Batch 857/3880, Loss: 1.0137474536895752\n",
      "Batch 858/3880, Loss: 1.1706751585006714\n",
      "Batch 859/3880, Loss: 1.2821160554885864\n",
      "Batch 860/3880, Loss: 1.2203378677368164\n",
      "Batch 861/3880, Loss: 1.095977544784546\n",
      "Batch 862/3880, Loss: 0.9343896508216858\n",
      "Batch 863/3880, Loss: 1.1217453479766846\n",
      "Batch 864/3880, Loss: 0.9715506434440613\n",
      "Batch 865/3880, Loss: 0.8606575727462769\n",
      "Batch 866/3880, Loss: 1.0047276020050049\n",
      "Batch 867/3880, Loss: 1.0775043964385986\n",
      "Batch 868/3880, Loss: 1.0485546588897705\n",
      "Batch 869/3880, Loss: 1.1066272258758545\n",
      "Batch 870/3880, Loss: 1.096325159072876\n",
      "Batch 871/3880, Loss: 0.9726606607437134\n",
      "Batch 872/3880, Loss: 1.0604729652404785\n",
      "Batch 873/3880, Loss: 1.2247380018234253\n",
      "Batch 874/3880, Loss: 0.9948059916496277\n",
      "Batch 875/3880, Loss: 0.95308518409729\n",
      "Batch 876/3880, Loss: 0.9857643246650696\n",
      "Batch 877/3880, Loss: 0.9872575402259827\n",
      "Batch 878/3880, Loss: 1.3066500425338745\n",
      "Batch 879/3880, Loss: 0.9322016835212708\n",
      "Batch 880/3880, Loss: 1.1811636686325073\n",
      "Batch 881/3880, Loss: 1.223939299583435\n",
      "Batch 882/3880, Loss: 0.9857213497161865\n",
      "Batch 883/3880, Loss: 1.166939377784729\n",
      "Batch 884/3880, Loss: 1.0663634538650513\n",
      "Batch 885/3880, Loss: 1.1502279043197632\n",
      "Batch 886/3880, Loss: 0.9842711687088013\n",
      "Batch 887/3880, Loss: 1.127507209777832\n",
      "Batch 888/3880, Loss: 0.7893014550209045\n",
      "Batch 889/3880, Loss: 1.2130258083343506\n",
      "Batch 890/3880, Loss: 1.0206142663955688\n",
      "Batch 891/3880, Loss: 1.0792726278305054\n",
      "Batch 892/3880, Loss: 1.1068120002746582\n",
      "Batch 893/3880, Loss: 0.9495345950126648\n",
      "Batch 894/3880, Loss: 0.8231173157691956\n",
      "Batch 895/3880, Loss: 1.280876636505127\n",
      "Batch 896/3880, Loss: 1.168867826461792\n",
      "Batch 897/3880, Loss: 1.2266050577163696\n",
      "Batch 898/3880, Loss: 1.0536653995513916\n",
      "Batch 899/3880, Loss: 1.005717158317566\n",
      "Batch 900/3880, Loss: 1.1948250532150269\n",
      "Batch 901/3880, Loss: 1.2716853618621826\n",
      "Batch 902/3880, Loss: 1.0929083824157715\n",
      "Batch 903/3880, Loss: 1.2047450542449951\n",
      "Batch 904/3880, Loss: 0.9992565512657166\n",
      "Batch 905/3880, Loss: 1.0653281211853027\n",
      "Batch 906/3880, Loss: 1.2387458086013794\n",
      "Batch 907/3880, Loss: 1.057578444480896\n",
      "Batch 908/3880, Loss: 1.0762211084365845\n",
      "Batch 909/3880, Loss: 1.1003303527832031\n",
      "Batch 910/3880, Loss: 1.1607061624526978\n",
      "Batch 911/3880, Loss: 0.8680887222290039\n",
      "Batch 912/3880, Loss: 0.9859505891799927\n",
      "Batch 913/3880, Loss: 1.1984777450561523\n",
      "Batch 914/3880, Loss: 1.1315739154815674\n",
      "Batch 915/3880, Loss: 1.0587209463119507\n",
      "Batch 916/3880, Loss: 1.098272681236267\n",
      "Batch 917/3880, Loss: 1.0904783010482788\n",
      "Batch 918/3880, Loss: 0.981085479259491\n",
      "Batch 919/3880, Loss: 0.9788690805435181\n",
      "Batch 920/3880, Loss: 1.167490005493164\n",
      "Batch 921/3880, Loss: 0.9799689054489136\n",
      "Batch 922/3880, Loss: 1.0941238403320312\n",
      "Batch 923/3880, Loss: 1.2293825149536133\n",
      "Batch 924/3880, Loss: 1.0735551118850708\n",
      "Batch 925/3880, Loss: 0.9956998825073242\n",
      "Batch 926/3880, Loss: 0.912983775138855\n",
      "Batch 927/3880, Loss: 1.2173371315002441\n",
      "Batch 928/3880, Loss: 0.8926048278808594\n",
      "Batch 929/3880, Loss: 0.990942656993866\n",
      "Batch 930/3880, Loss: 1.2875316143035889\n",
      "Batch 931/3880, Loss: 1.1290392875671387\n",
      "Batch 932/3880, Loss: 1.0760765075683594\n",
      "Batch 933/3880, Loss: 1.0845842361450195\n",
      "Batch 934/3880, Loss: 1.2150204181671143\n",
      "Batch 935/3880, Loss: 1.052689790725708\n",
      "Batch 936/3880, Loss: 1.1441372632980347\n",
      "Batch 937/3880, Loss: 1.186078667640686\n",
      "Batch 938/3880, Loss: 1.0408000946044922\n",
      "Batch 939/3880, Loss: 0.9833638072013855\n",
      "Batch 940/3880, Loss: 0.9248014092445374\n",
      "Batch 941/3880, Loss: 1.027575969696045\n",
      "Batch 942/3880, Loss: 1.1646524667739868\n",
      "Batch 943/3880, Loss: 0.9851091504096985\n",
      "Batch 944/3880, Loss: 1.200252652168274\n",
      "Batch 945/3880, Loss: 1.1557046175003052\n",
      "Batch 946/3880, Loss: 1.101462960243225\n",
      "Batch 947/3880, Loss: 1.2832601070404053\n",
      "Batch 948/3880, Loss: 1.0068985223770142\n",
      "Batch 949/3880, Loss: 0.831108570098877\n",
      "Batch 950/3880, Loss: 0.8849477767944336\n",
      "Batch 951/3880, Loss: 1.1033741235733032\n",
      "Batch 952/3880, Loss: 1.1317546367645264\n",
      "Batch 953/3880, Loss: 1.0889477729797363\n",
      "Batch 954/3880, Loss: 1.2224771976470947\n",
      "Batch 955/3880, Loss: 0.9902278780937195\n",
      "Batch 956/3880, Loss: 1.0562877655029297\n",
      "Batch 957/3880, Loss: 1.1637814044952393\n",
      "Batch 958/3880, Loss: 1.1075581312179565\n",
      "Batch 959/3880, Loss: 1.3724582195281982\n",
      "Batch 960/3880, Loss: 0.8933261036872864\n",
      "Batch 961/3880, Loss: 1.0354136228561401\n",
      "Batch 962/3880, Loss: 1.1301993131637573\n",
      "Batch 963/3880, Loss: 0.8972238898277283\n",
      "Batch 964/3880, Loss: 1.0842028856277466\n",
      "Batch 965/3880, Loss: 1.0632730722427368\n",
      "Batch 966/3880, Loss: 1.0867692232131958\n",
      "Batch 967/3880, Loss: 1.093017339706421\n",
      "Batch 968/3880, Loss: 1.163613200187683\n",
      "Batch 969/3880, Loss: 1.1092572212219238\n",
      "Batch 970/3880, Loss: 0.9015054702758789\n",
      "Batch 971/3880, Loss: 1.089308738708496\n",
      "Batch 972/3880, Loss: 0.9581661224365234\n",
      "Batch 973/3880, Loss: 1.3096648454666138\n",
      "Batch 974/3880, Loss: 0.8348293304443359\n",
      "Batch 975/3880, Loss: 1.0089882612228394\n",
      "Batch 976/3880, Loss: 1.0673775672912598\n",
      "Batch 977/3880, Loss: 0.9968422651290894\n",
      "Batch 978/3880, Loss: 0.9838278293609619\n",
      "Batch 979/3880, Loss: 1.0942500829696655\n",
      "Batch 980/3880, Loss: 0.9807440638542175\n",
      "Batch 981/3880, Loss: 1.0912115573883057\n",
      "Batch 982/3880, Loss: 1.1070882081985474\n",
      "Batch 983/3880, Loss: 1.1129889488220215\n",
      "Batch 984/3880, Loss: 0.9728711247444153\n",
      "Batch 985/3880, Loss: 1.050095796585083\n",
      "Batch 986/3880, Loss: 0.8919427990913391\n",
      "Batch 987/3880, Loss: 0.9990015029907227\n",
      "Batch 988/3880, Loss: 1.026127576828003\n",
      "Batch 989/3880, Loss: 1.1878656148910522\n",
      "Batch 990/3880, Loss: 1.0565471649169922\n",
      "Batch 991/3880, Loss: 1.0004171133041382\n",
      "Batch 992/3880, Loss: 1.0022443532943726\n",
      "Batch 993/3880, Loss: 1.0791726112365723\n",
      "Batch 994/3880, Loss: 0.851617157459259\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m char_model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(char_dataloader):\n\u001B[1;32m----> 7\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchar_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchar_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(char_dataloader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[26], line 16\u001B[0m, in \u001B[0;36mtraining_step\u001B[1;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001B[0m\n\u001B[0;32m     13\u001B[0m inputs, lengths \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), lengths\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Прогон через модель\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m outputs, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlengths\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Сдвигаем последовательность, чтобы предсказать следующий токен\u001B[39;00m\n\u001B[0;32m     19\u001B[0m targets \u001B[38;5;241m=\u001B[39m inputs[:, \u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[1;32m~\\PycharmProjects\\NeuralNetwork\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NeuralNetwork\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[24], line 46\u001B[0m, in \u001B[0;36mCharRNN.forward\u001B[1;34m(self, x, lengths)\u001B[0m\n\u001B[0;32m     43\u001B[0m packed_embeds \u001B[38;5;241m=\u001B[39m pack_padded_sequence(embedded, lengths\u001B[38;5;241m.\u001B[39mcpu(), batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, enforce_sorted\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Пропускаем через RNN слой\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m packed_outputs, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpacked_embeds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Распаковываем выходы обратно в тензор\u001B[39;00m\n\u001B[0;32m     49\u001B[0m outputs, _ \u001B[38;5;241m=\u001B[39m pad_packed_sequence(packed_outputs, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NeuralNetwork\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NeuralNetwork\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\NeuralNetwork\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:739\u001B[0m, in \u001B[0;36mRNN.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    738\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRNN_TANH\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 739\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn_tanh\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    740\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    741\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    742\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    743\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    744\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    745\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    748\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    749\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    750\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    751\u001B[0m         result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mrnn_relu(\n\u001B[0;32m    752\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m    753\u001B[0m             batch_sizes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    760\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional,\n\u001B[0;32m    761\u001B[0m         )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "losses = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    char_model.train()\n",
    "    for batch_idx, batch in enumerate(char_dataloader):\n",
    "        loss = training_step(char_model, batch, char_tokenizer.vocab_size, criterion, optimizer, device)\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch {batch_idx}/{len(char_dataloader)}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGwCAYAAABIC3rIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx8klEQVR4nO3dfVRV9YL/8Q+IAoWPHGU0/FGQGsNVQKm8SzI1Tb2TaZap1Zj5MKnYw63EBxZJy0rlqqshGisn1NJJw7BMy+syvXeWawldKTlXk1QayxTxqKAkTyL794fDsTNogRf2Eb/v11qu1dn7u8/+fj8e7LPO3ufgY1mWJQAAAIP5ensCAAAA3kYhAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACM5+ftCTQnp0+Xiu/1lnx8pODg1uTRxMjZHuRsD3K2D1lfVptFfVCIGsCyZPyL65fIwx7kbA9ytgc524esG4ZLZgAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYz6uFqLKyUvPmzVNcXJzi4+OVkZFx1bGbNm3S0KFD1atXL40bN05Op9O9z7Isvfvuuxo0aJB69+6tJ598UocPH/bYv2TJEvXt21d33XWXUlNTVVNT06RrAwAAzYdXC1Fqaqr27dun1atXa/78+UpPT9fWrVvrjNuzZ4+SkpI0Y8YMbdmyRbGxsZo6darOnz8vSVq3bp0yMjKUnJysjz/+WKGhoZo6darKy8slSStXrtTmzZuVnp6utLQ0ffbZZ1q5cqWtawUAANcvrxWisrIyZWZmKikpSVFRURoyZIimTJmitWvX1hnrcrk0Y8YMjRw5Ul27dlVCQoJKSkpUUFAgSdq4caMmTZqkgQMH6rbbblNKSopKSkr09ddfS5Lef/99Pfvss4qLi1Pfvn310ksvXfE8AADATH7eOnF+fr6qq6sVGxvr3tanTx+9/fbbqqmpka/v5a42fPhw939XVFRo1apVCg4OVkREhCQpMTFRoaGh7jE+Pj6yLEulpaUqKipSYWGh7rzzTo/zHDt2TCdPnlSnTp3qPWcfn2ta6g2nNgfyaFrkbA9ytgc524esL2tIBl4rRC6XS+3bt1erVq3c2xwOhyorK1VSUqIOHTrUOWb37t2aNGmS+56gm2++WZIUFxfnMS4zM1PV1dXq06ePioqKJMmj+DgcDknSiRMnGlSIgoNb13+BBiAPe5CzPcjZHuRsH7JuGK8VovLyco8yJMn9uKqq6orHdOvWTVlZWdq5c6fmzJmj0NBQxcTEeIzJy8vT4sWLNXnyZHXs2FE//PCDx3PX5zxXc/p0qSyrQYfckHx8Lv2gkUfTImd7kLM9yNk+ZH1ZbRb14bVC5O/vX6eQ1D4OCAi44jEOh0MOh0ORkZHKy8vTunXrPArRN998o6lTp6p///567rnnJHmWH39/f4/zBAYGNmjOliXjX1y/RB72IGd7kLM9yNk+ZN0wXrupOiQkRMXFxaqurnZvc7lcCggIUJs2bTzGOp1O7d+/32NbRESEiouL3Y9zcnI0adIk9e3bV0uXLnXfgxQSEuJ+7l+eR5I6duzYuIsCAADNktcKUWRkpPz8/LR37173ttzcXPXs2dPjhmpJ2rBhg5YtW+axbf/+/QoPD5ckHTx4UNOnT9c999yjN954Qy1btnSPCwkJUZcuXZSbm+txni5dujTo/iEAAHDj8lohCgwM1KhRo5SSkiKn06nt27crIyNDEyZMkHTpXZyKigpJ0tixY5Wdna3Vq1fryJEjSktLk9Pp1MSJEyVJL7/8sjp37qy5c+equLhYLpfL4/jx48dryZIlysnJUU5OjpYuXeo+DwAAgI9lee8KY3l5uVJSUrRt2zYFBQVp8uTJ7pLTo0cPLVy4UKNHj5Yk7dy5U8uWLdMPP/ygbt26KSkpSb1795bL5VJ8fPwVn7/2+IsXLyo1NVVZWVlq0aKFHnnkEb344ovyaeBnEk+d4gY16dJNag5Ha/JoYuRsD3K2Bznbh6wvq82iXmO9WYiaG15cl/DDZg9ytgc524Oc7UPWlzWkEPHLXQEAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4Xi1ElZWVmjdvnuLi4hQfH6+MjIyrjt20aZOGDh2qXr16ady4cXI6nVcct3z5cs2ZM8dj27fffqsePXp4/Bk9enSjrgUAADRfft48eWpqqvbt26fVq1fr+PHjmj17trp06aJhw4Z5jNuzZ4+SkpL06quvqnfv3vqv//ovTZ06VTt27NDNN9/sHrd582a9+eabevDBBz2OP3z4sCIjI7VixQr3Nj8/ry4dAABcR7zWCsrKypSZmakVK1YoKipKUVFROnTokNauXVunELlcLs2YMUMjR46UJCUkJCgjI0MFBQXq1auXqqurtWDBAm3cuFFdu3atc66CggJFRESoY8eOtqwNAAA0L167ZJafn6/q6mrFxsa6t/Xp00d5eXmqqanxGDt8+HBNnz5dklRRUaFVq1YpODhYERERki6Vq++++04fffSRx/PVKigo0K233tp0iwEAAM2a194hcrlcat++vVq1auXe5nA4VFlZqZKSEnXo0KHOMbt379akSZNkWZaWLFnivlzWpk0brVu37qrnKigoUE1NjUaMGKHS0lL1799fiYmJCgoKatCcfXwaNPyGVZsDeTQtcrYHOduDnO1D1pc1JAOvFaLy8nKPMiTJ/biqquqKx3Tr1k1ZWVnauXOn5syZo9DQUMXExPzqeS5cuKCjR48qNDRUr7/+us6dO6eFCxdq1qxZWr58eYPmHBzcukHjb3TkYQ9ytgc524Oc7UPWDeO1QuTv71+n+NQ+DggIuOIxDodDDodDkZGRysvL07p1636zELVs2VLZ2dny9/dXy5YtJUmLFi3Sww8/rKKiIoWEhNR7zqdPl8qy6j38huXjc+kHjTyaFjnbg5ztQc72IevLarOoD68VopCQEBUXF6u6utr9iS+Xy6WAgAC1adPGY6zT6VSLFi0UFRXl3hYREaGCgoJ6nev/XhqrvfeooYXIsmT8i+uXyMMe5GwPcrYHOduHrBvGazdVR0ZGys/PT3v37nVvy83NVc+ePeXr6zmtDRs2aNmyZR7b9u/fr/Dw8N88z+HDhxUbG6ujR4+6tx04cEB+fn4KCwv7xxYBAABuCF4rRIGBgRo1apRSUlLkdDq1fft2ZWRkaMKECZIuvVtUUVEhSRo7dqyys7O1evVqHTlyRGlpaXI6nZo4ceJvnic8PFxhYWFKTk7WwYMHtWfPHiUnJ2vMmDFq27ZtUy4RAAA0E179puq5c+cqKipKTz75pF555RU988wzuv/++yVJ8fHx+vzzzyVJUVFRSk9P14YNG/Tggw/qr3/9q9577716Xe7y9fXV8uXLFRQUpMcff1wJCQn6/e9/r3nz5jXp2gAAQPPhY1lcYayvU6e4QU26dJOaw9GaPJoYOduDnO1BzvYh68tqs6gPfrkrAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjOfVQlRZWal58+YpLi5O8fHxysjIuOrYTZs2aejQoerVq5fGjRsnp9N5xXHLly/XnDlzPLZZlqUlS5aob9++uuuuu5SamqqamppGXQsAAGi+vFqIUlNTtW/fPq1evVrz589Xenq6tm7dWmfcnj17lJSUpBkzZmjLli2KjY3V1KlTdf78eY9xmzdv1ptvvlnn+JUrV2rz5s1KT09XWlqaPvvsM61cubLJ1gUAAJoXrxWisrIyZWZmKikpSVFRURoyZIimTJmitWvX1hnrcrk0Y8YMjRw5Ul27dlVCQoJKSkpUUFAgSaqurtb8+fM1b948de3atc7x77//vp599lnFxcWpb9++eumll654HgAAYCavFaL8/HxVV1crNjbWva1Pnz7Ky8urczlr+PDhmj59uiSpoqJCq1atUnBwsCIiIiRdKlffffedPvroI4/nk6SioiIVFhbqzjvv9DjPsWPHdPLkyaZaHgAAaEb8vHVil8ul9u3bq1WrVu5tDodDlZWVKikpUYcOHeocs3v3bk2aNMl9T9DNN98sSWrTpo3WrVt31fNIUqdOnTzOI0knTpzw2P5bfHzqPfSGVpsDeTQtcrYHOduDnO1D1pc1JAOvFaLy8nKPMiTJ/biqquqKx3Tr1k1ZWVnauXOn5syZo9DQUMXExPzqeSoqKjyeuz7nuZrg4NYNGn+jIw97kLM9yNke5Gwfsm4YrxUif3//OoWk9nFAQMAVj3E4HHI4HIqMjFReXp7WrVv3m4Xol+XH39/f4zyBgYENmvPp06WyrAYdckPy8bn0g0YeTYuc7UHO9iBn+5D1ZbVZ1IfXClFISIiKi4tVXV0tP79L03C5XAoICFCbNm08xjqdTrVo0UJRUVHubREREe6bqn/rPLXPHRoa6v5vSerYsWOD5mxZMv7F9UvkYQ9ytgc524Oc7UPWDeO1m6ojIyPl5+envXv3urfl5uaqZ8+e8vX1nNaGDRu0bNkyj2379+9XeHj4b54nJCREXbp0UW5ursd5unTp0qD7hwAAwI3La4UoMDBQo0aNUkpKipxOp7Zv366MjAxNmDBB0qV3cWrv/xk7dqyys7O1evVqHTlyRGlpaXI6nZo4cWK9zjV+/HgtWbJEOTk5ysnJ0dKlS93nAQAA8NolM0maO3euUlJS9OSTTyooKEjPPPOM7r//fklSfHy8Fi5cqNGjRysqKkrp6elatmyZli5dqm7duum9995zXw77LZMnT9bp06c1c+ZMtWjRQo888ki9yxQAALjx+VgWVxjr69QpblCTLt2k5nC0Jo8mRs72IGd7kLN9yPqy2izqg1/uCgAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADBeoxWiM2fOyLKsxno6AAAA21xTISoqKtIf//hHHThwQJWVlXriiSfUr18/DRo0SPn5+Y09RwAAgCZ1TYUoJSVFZ86cUbt27ZSVlaWDBw9q3bp1GjRokBYsWNDYcwQAAGhSftdyUHZ2trKystS5c2dt375d9913n6Kjo9WhQwc98MADjT1HAACAJnVN7xD5+/ursrJSZ8+eVU5OjgYMGCBJ+umnn9S2bdvGnB8AAECTu6Z3iAYPHqznn39eAQEBatu2rQYMGKDPP/9cr7/+uh566KHGniMAAECTuqZClJKSojVr1ujYsWMaO3as/P39VVVVpWnTpunxxx9v7DkCAAA0qWsqRH5+fpo4caL7cWVlpcLDw3XbbbfJx8enseYGAABgi2u6h+jw4cN69NFH9fXXX+vcuXMaNWqUHn30UfXv31/Z2dmNPUcAAIAmdU2F6JVXXlHXrl116623asOGDSotLdWuXbs0bdo0LV68uLHnCAAA0KSuqRA5nU49//zz6tChg7Zv364hQ4bI4XDogQce0Pfff9/YcwQAAGhS11SIWrdurVOnTqmwsFB79+51f+z+wIEDCg4Obsz5AQAANLlruql69OjRmj59ulq1aqXQ0FDFx8frww8/VGpqqp577rnGniMAAECTuqZC9MILL6hnz546duyYHnjgAbVo0UJdunTRsmXLNHDgwMaeIwAAQJO6pkIkSUOGDNGRI0eUl5enmpoa3Xbbbbr99tsbc24AAAC2uKZCdO7cOc2dO1c7duxQmzZtdPHiRZ0/f1533nmn3nrrLbVu3bqx5wkAANBkrumm6ldffVUnTpzQli1blJOToz179uizzz5TWVmZFi5c2NhzBAAAaFLXVIh27NihlJQUhYeHu7fdfvvtevnll/Xll1822uQAAADscM2/7d7Xt+6hPj4+unjx4j88KQAAADtdUyEaNGiQXnnlFf3444/ubUeOHNGCBQt07733NtrkAAAA7HBNN1XPmjVLCQkJuv/++9W2bVtJ0tmzZ9W/f38lJyc36gQBAACaWr0L0fHjxz0eL168WKWlpfrv//5vBQQEKD4+Xv7+/iorK1O7du0ae54AAABNpt6FaNCgQfLx8amz3bIsSZfuH7IsSz4+Pjpw4EDjzRAAAKCJ1bsQ8ekxAABwo6p3Ibrllluach4AAABec02fMgMAALiRUIgAAIDxKEQAAMB4Xi1ElZWVmjdvnuLi4hQfH6+MjIyrjt20aZOGDh2qXr16ady4cXI6nR77N2/erMGDBys6OloJCQk6c+aMe9+3336rHj16ePwZPXp0k60LAAA0L14tRKmpqdq3b59Wr16t+fPnKz09XVu3bq0zbs+ePUpKStKMGTO0ZcsWxcbGaurUqTp//rwkyel0KikpSTNnztT69et17tw5zZ0713384cOHFRkZqV27drn/vPfee7atEwAAXN+u6ZuqG0NZWZkyMzO1YsUKRUVFKSoqSocOHdLatWs1bNgwj7Eul0szZszQyJEjJUkJCQnKyMhQQUGBevXqpTVr1mj48OEaNWqUpEtFa+DAgTp69Ki6du2qgoICRUREqGPHjnYvEwAANANeK0T5+fmqrq5WbGyse1ufPn309ttvq6amxuOXxw4fPtz93xUVFVq1apWCg4MVEREhScrLy9PUqVPdYzp37qwuXbooLy/PXYh69OjxD8/5Ct9LaaTaHMijaZGzPcjZHuRsH7K+rCEZeK0QuVwutW/fXq1atXJvczgcqqysVElJiTp06FDnmN27d2vSpEmyLEtLlizRzTffLEk6efKkOnXq5DE2ODhYJ06ckCQVFBSopqZGI0aMUGlpqfr376/ExEQFBQU1aM7Bwa0buswbGnnYg5ztQc72IGf7kHXDeK0QlZeXe5QhSe7HVVVVVzymW7duysrK0s6dOzVnzhyFhoYqJiZGFRUVV3yuqqoqXbhwQUePHlVoaKhef/11nTt3TgsXLtSsWbO0fPnyBs359OlS/e9vKjGaj8+lHzTyaFrkbA9ytgc524esL6vNoj68Voj8/f3rFJ/axwEBAVc8xuFwyOFwKDIyUnl5eVq3bp1iYmKu+lyBgYFq2bKlsrOz5e/vr5YtW0qSFi1apIcfflhFRUUKCQmp95wtS8a/uH6JPOxBzvYgZ3uQs33IumG89imzkJAQFRcXq7q62r3N5XIpICBAbdq08RjrdDq1f/9+j20REREqLi52P9epU6c89p86dcp9E3VQUJC7DNUeK0lFRUWNtyAAANBsea0QRUZGys/PT3v37nVvy83NVc+ePT1uqJakDRs2aNmyZR7b9u/fr/DwcElSdHS0cnNz3fsKCwtVWFio6OhoHT58WLGxsTp69Kh7/4EDB+Tn56ewsLAmWBkAAGhuvFaIAgMDNWrUKKWkpMjpdGr79u3KyMjQhAkTJF16t6iiokKSNHbsWGVnZ2v16tU6cuSI0tLS5HQ6NXHiREnS+PHj9emnnyozM1P5+flKTEzUgAED1LVrV4WHhyssLEzJyck6ePCg9uzZo+TkZI0ZM0Zt27b11vIBAMB1xMeyvHeFsby8XCkpKdq2bZuCgoI0efJkd8np0aOHFi5c6P5G6Z07d2rZsmX64Ycf1K1bNyUlJal3797u58rKylJaWprOnj2rfv36acGCBWrfvr2kS+8Yvfbaa8rJyZGvr69GjBihxMTEOjdi/5ZTp7hBTbp0k5rD0Zo8mhg524Oc7UHO9iHry2qzqNdYbxai5oYX1yX8sNmDnO1BzvYgZ/uQ9WUNKUT8clcAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwnlcLUWVlpebNm6e4uDjFx8crIyPjqmM3bdqkoUOHqlevXho3bpycTqfH/s2bN2vw4MGKjo5WQkKCzpw5495nWZaWLFmivn376q677lJqaqpqamqabF0AAKB58WohSk1N1b59+7R69WrNnz9f6enp2rp1a51xe/bsUVJSkmbMmKEtW7YoNjZWU6dO1fnz5yVJTqdTSUlJmjlzptavX69z585p7ty57uNXrlypzZs3Kz09XWlpafrss8+0cuVK29YJAACub14rRGVlZcrMzFRSUpKioqI0ZMgQTZkyRWvXrq0z1uVyacaMGRo5cqS6du2qhIQElZSUqKCgQJK0Zs0aDR8+XKNGjdIdd9yh1NRU/fWvf9XRo0clSe+//76effZZxcXFqW/fvnrppZeueB4AAGAmrxWi/Px8VVdXKzY21r2tT58+ysvLq3M5a/jw4Zo+fbokqaKiQqtWrVJwcLAiIiIkSXl5eYqLi3OP79y5s7p06aK8vDwVFRWpsLBQd955p8d5jh07ppMnTzblEgEAQDPh560Tu1wutW/fXq1atXJvczgcqqysVElJiTp06FDnmN27d2vSpEnue4JuvvlmSdLJkyfVqVMnj7HBwcE6ceKEXC6XJHnsdzgckqQTJ07UOe7X+PjUf303stocyKNpkbM9yNke5Gwfsr6sIRl4rRCVl5d7lCFJ7sdVVVVXPKZbt27KysrSzp07NWfOHIWGhiomJkYVFRVXfK6qqipVVFR4PHd9znM1wcGtGzT+Rkce9iBne5CzPcjZPmTdMF4rRP7+/nUKSe3jgICAKx7jcDjkcDgUGRmpvLw8rVu3TjExMVd9rsDAQI/y4+/v73GewMDABs359OlSWVaDDrkh+fhc+kEjj6ZFzvYgZ3uQs33I+rLaLOrDa4UoJCRExcXFqq6ulp/fpWm4XC4FBASoTZs2HmOdTqdatGihqKgo97aIiAj3TdUhISE6deqUxzGnTp1Sx44dFRIS4n7u0NBQ939LUseOHRs0Z8uS8S+uXyIPe5CzPcjZHuRsH7JuGK/dVB0ZGSk/Pz/t3bvXvS03N1c9e/aUr6/ntDZs2KBly5Z5bNu/f7/Cw8MlSdHR0crNzXXvKywsVGFhoaKjoxUSEqIuXbp47M/NzVWXLl0adP8QAAC4cXmtEAUGBmrUqFFKSUmR0+nU9u3blZGRoQkTJki69C5O7f0/Y8eOVXZ2tlavXq0jR44oLS1NTqdTEydOlCSNHz9en376qTIzM5Wfn6/ExEQNGDBAXbt2de9fsmSJcnJylJOTo6VLl7rPAwAA4GNZ3ntDrby8XCkpKdq2bZuCgoI0efJkd8np0aOHFi5cqNGjR0uSdu7cqWXLlumHH35Qt27dlJSUpN69e7ufKysrS2lpaTp79qz69eunBQsWqH379pKkixcvKjU1VVlZWWrRooUeeeQRvfjii/Jp4C34p05xPVa6dE3W4WhNHk2MnO1BzvYgZ/uQ9WW1WdRrrDcLUXPDi+sSftjsQc72IGd7kLN9yPqyhhQifrkrAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjOfVQlRZWal58+YpLi5O8fHxysjIuOrYv/zlLxo5cqRiY2M1YsQIffnll+59lmXpvffe06BBgxQXF6e5c+fq/Pnz7v3ffvutevTo4fFn9OjRTbo2AADQfPh58+Spqanat2+fVq9erePHj2v27Nnq0qWLhg0b5jEuPz9fM2fOVGJiou69917t2rVLzz33nDZs2KA77rhD69evV3p6uhYsWKAePXpo4cKFevHFF/X2229Lkg4fPqzIyEitWLHC/Zx+fl5dOgAAuI54rRWUlZUpMzNTK1asUFRUlKKionTo0CGtXbu2TiHavHmz+vbtqwkTJkiSwsLCtGPHDn3xxRe64447tGbNGj311FN64IEHJEmLFi1S//799f333ys8PFwFBQWKiIhQx44dbV8nAAC4/nmtEOXn56u6ulqxsbHubX369NHbb7+tmpoa+fpevpr30EMP6cKFC3Weo7S0VJJ09OhRRUdHu7d36tRJHTp00N69e92FqEePHk24GgAA0Jx5rRC5XC61b99erVq1cm9zOByqrKxUSUmJOnTo4N4eERHhceyhQ4e0e/dujRs3TpIUHBysoqIi9/6ysjKdPXtWxcXFkqSCggLV1NRoxIgRKi0tVf/+/ZWYmKigoKAGzdnHp8HLvCHV5kAeTYuc7UHO9iBn+5D1ZQ3JwGuFqLy83KMMSXI/rqqquupxZ86c0TPPPKPevXvrvvvukyT94Q9/0DvvvKM+ffooNDRUixYtkiRduHBBFy5c0NGjRxUaGqrXX39d586d08KFCzVr1iwtX768QXMODm7doPE3OvKwBznbg5ztQc72IeuG8Voh8vf3r1N8ah8HBARc8ZhTp07pqaeekmVZSktLc19WmzFjho4ePap/+Zd/kZ+fn8aNG6c77rhDQUFBatmypbKzs+Xv76+WLVtKunSP0cMPP6yioiKFhITUe86nT5fKsq5ltTcWH59LP2jk0bTI2R7kbA9ytg9ZX1abRX14rRCFhISouLhY1dXV7k98uVwuBQQEqE2bNnXGFxUVuW+qfv/99z0uqd10003693//d5WWlsrHx0dBQUH6/e9/r1tuuUWS6lwaq70E19BCZFky/sX1S+RhD3K2Bznbg5ztQ9YN47XvIYqMjJSfn5/27t3r3pabm6uePXt63FAtXbonaMqUKfL19dWaNWvqlJjU1FRt3LhRrVu3VlBQkJxOp0pLSxUbG6vDhw8rNjZWR48edY8/cOCA/Pz8FBYW1qRrBAAAzYPXClFgYKBGjRqllJQUOZ1Obd++XRkZGe53gVwulyoqKiRJ77zzjn788UctXrzYvc/lcrk/ZdapUyelp6fL6XRq3759mjVrlsaPH6927dopPDxcYWFhSk5O1sGDB7Vnzx4lJydrzJgxatu2rXcWDwAAris+luW9N9TKy8uVkpKibdu2KSgoSJMnT9bEiRMlyf0Fi6NHj9awYcP0P//zP3WOf+ihh7Ro0SJdvHhRixYt0qZNm+Tr66uRI0fqpZdecl+KKyws1GuvvaacnBz5+vpqxIgRSkxMrHNT9285dYrrsdKla7IOR2vyaGLkbA9ytgc524esL6vNol5jvVmImhteXJfww2YPcrYHOduDnO1D1pc1pBDxy10BAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYj0IEAACMRyECAADGoxABAADjUYgAAIDxKEQAAMB4FCIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIAAAYz8/bE2hOfHy8PYPrQ20O5NG0yNke5GwPcrYPWV/WkAx8LMuymm4qAAAA1z8umQEAAONRiAAAgPEoRAAAwHgUIgAAYDwKEQAAMB6FCAAAGI9CBAAAjEchAgAAxqMQAQAA41GIIEmqrKzUvHnzFBcXp/j4eGVkZFx17K5du/Tggw8qNjZWEydO1Pfff++xf+vWrRo6dKhiYmI0adIkHTt2rKmn32w0Vs6WZenNN99U//79deedd+r555/XmTNn7FhCs1JVVaUHHnhAOTk5Vx3z7bffasyYMYqOjtbDDz+sffv2eezfvHmzBg8erOjoaCUkJJDzFfyjOVuWpXfffVeDBg1S79699eSTT+rw4cN2TL3ZaYzXdK0vvvhCPXr0aKqpNjsUIkiSUlNTtW/fPq1evVrz589Xenq6tm7dWmfcoUOH9PTTT+u+++7Txx9/rH/+53/Wk08+qfPnz0uSvv76a7344ot66qmnlJWVpVatWumFF16weznXrcbKef369dqwYYOWLFmitWvX6uTJk0pKSrJ7Ode1yspKvfDCCzp06NBVx5SVlenf/u3fFBcXp6ysLMXGxurpp59WWVmZJMnpdCopKUkzZ87U+vXrde7cOc2dO9euJTQLjZHzunXrlJGRoeTkZH388ccKDQ3V1KlTVV5ebtcymoXGyLrWuXPn9NprrzX1lJsXC8Y7f/681bNnTys7O9u97a233rKeeOKJOmNfeeUV6/HHH3c/rqmpsYYPH259+OGHlmVZVkJCgjVnzhz3/h9//NEaOHCgdfr06SZcQfPQmDlPmzbNWrRokXv/l19+acXExDTh7JuXQ4cOWQ8++KA1YsQIq3v37h6Z/1JmZqY1aNAgq6amxrKsSzkPGTLE+vjjjy3LsqxZs2ZZs2fPdo8/fvy41aNHD+vHH39s+kU0A42V85gxY6x33nnHPb6qqsqKiYmxdu3a1fSLaCYaK+taSUlJ1rhx46zu3bs3+dybC94hgvLz81VdXa3Y2Fj3tj59+igvL081NTUeY48ePapevXq5H/v4+Kh79+7au3evJOmrr77SkCFD3Pu7du2qHTt2qEOHDk27iGagMXNu166d/vKXv6ioqEgVFRXasmWLIiMjbVlHc/DVV1/p7rvv1vr16391XF5envr06SOf//2V2D4+Purdu7c757y8PMXFxbnHd+7cWV26dFFeXl6Tzb05aaycExMT9eCDD7rH+/j4yLIslZaWNtncm5vGyrr2ub766itNmzatKafc7Ph5ewLwPpfLpfbt26tVq1bubQ6HQ5WVlSopKfEoMw6HQ0VFRR7HnzhxQm3bttW5c+d09uxZXbx4UZMnT1Z+fr569eqllJQUhYSE2Lae61Vj5SxJCQkJmj59uvr3768WLVqoY8eOv/kPpUkee+yxeo1zuVy6/fbbPbYFBwe7L0mcPHlSnTp1qrP/xIkTjTPRZq6xcv5l6ZSkzMxMVVdXq0+fPo0z0RtAY2VdVVWl5ORkvfzyy2rZsmWjz7M54x0iqLy83ON/0pLcj6uqqjy2Dx8+XH/+85+1c+dOVVdXa+PGjfr73/+uCxcuuK9Rv/rqqxoxYoSWL1+uqqoqPf3003XeATFRY+UsSceOHVNAQIDefvttffDBB/qnf/onzZs3z56F3ECu9ndS+/dRUVHxq/tRP7+V8y/l5eVp8eLFmjx5sjp27GjXFG8Yv5X1W2+9paioKMXHx3tjetc13iGC/P396/zDVPs4ICDAY3v//v2VkJCgZ555RhcvXtTdd9+tkSNH6ueff1aLFi0kSWPGjNGoUaMkSUuWLFG/fv20d+9e9e7du+kXcx1rrJwty9Ls2bOVmJiogQMHSpLeeOMNDRw4UHl5eYqOjrZnQTeAq/2d1P59XG1/YGCgbXO8EfxWzrW++eYbTZ06Vf3799dzzz1n5xRvGL+W9cGDB/XRRx/ps88+89Lsrm+8QwSFhISouLhY1dXV7m0ul0sBAQFq06ZNnfHTp0/X119/rV27dmnVqlU6f/68brnlFrVv314tW7ZUeHi4e2z79u3Vrl07LjGo8XI+c+aMCgsLPT4u27lzZ7Vv356vOGigkJAQnTp1ymPbqVOn3JfJrrafdy4a5rdylqScnBxNmjRJffv21dKlS+Xry/+ersWvZb1t2zadPXtWQ4YMUWxsrKZOnSpJio2N1aZNm7wx3esKrzgoMjJSfn5+Hjfd5ebmqmfPnnX+Udq8ebNee+01tWrVSsHBwaqoqFBOTo7uvvtu+fn5KSoqSvn5+e7xZ86cUXFxsW655Ra7lnPdaqyc27Ztq1atWqmgoMA9/syZMyopKVFoaKhdy7khREdH65tvvpFlWZIufR/O119/7X6XLTo6Wrm5ue7xhYWFKiws5F24BvqtnA8ePKjp06frnnvu0RtvvMG9Lf+AX8v6iSee0BdffKFPPvlEn3zyiV599VVJ0ieffKJBgwZ5c9rXBQoRFBgYqFGjRiklJUVOp1Pbt29XRkaGJkyYIOnSuxgVFRWSpFtvvVXr1q3Ttm3bdOTIEb344ovq3Lmz+vfvL0l66qmn9MEHH+iLL75QQUGB5s2bp8jISI9PTJmqsXL28/PT6NGjtXjxYv3tb3/TwYMHNWvWLEVHR6tnz57eXGKz8Muchw0b5v4+lsOHD+u1115TeXm5hg8fLkkaP368Pv30U2VmZio/P1+JiYkaMGCAunbt6s0lNAsNyfnll19W586dNXfuXBUXF8vlcnkcj19X36zbtWunsLAw95/aD7uEhYUpKCjIm0u4PnjvE/+4npSVlVmJiYlWTEyMFR8fb61cudK9r3v37h7fYbFhwwZr4MCBVmxsrDVjxgyrqKjI47nWr19vDRw40OrVq5c1ZcoUq7Cw0K5lXPcaK+eKigpr0aJF1j333GPddddd1vPPP893PV3F//3Olv+bc15enjVq1CirZ8+e1iOPPGLt37/f4/iPP/7Yuvfee62YmBgrISHBOnPmjG1zb06uNeeTJ09a3bt3v+Kf//vdObjkH31N18rOzuZ7iH7Bx7L+9301AAAAQ3HJDAAAGI9CBAAAjEchAgAAxqMQAQAA41GIAACA8ShEAADAeBQiAABgPAoRAAAwHoUIABrgp59+Uo8ePfTTTz95eyoAGhGFCAAAGI9CBAAAjEchAtCsFRYWatq0aYqOjtagQYOUnp6uixcvKisrS+PHj9eSJUsUGxurAQMGKDMz031cTU2N/vM//1P33XefevXqpX/913/Vd999595/+vRpPf/88+rdu7f69eunZcuW6Ze/+nH79u0aPHiwoqOjNW3aNJ09e9bWdQNoXH7engAAXCvLsjRz5kzdcccd2rhxo1wul15++WX5+Pioc+fO+vvf/66bbrpJ69evl9PpVEpKijp37qz4+Hi99dZb+vDDD7VgwQLdeuutWrFihaZMmaI///nPuummm5SQkKAWLVpozZo1On/+vP74xz+qU6dOGjBggCRp48aN7pI0c+ZMrVixQi+99JJ3AwFwzShEAJqt7OxsHT9+XJmZmfL19VV4eLhmz56tuXPnavbs2fLx8VFqaqqCg4PVvXt3/e1vf9NHH32kfv36ac2aNXrhhRd03333SZIWLFigIUOGaNOmTYqJidE333yj7du3q2vXrpKklJQUlZWVuc89a9Ys9erVS5I0fPhw5efn2x8AgEZDIQLQbBUUFKikpER9+vRxb6upqVFFRYVKSkoUFham4OBg977f/e53WrdunU6fPq2SkhJFR0e797Vs2VK/+93vVFBQoLZt26pdu3buMiRJgwcPliT3p8v+3//7f+59rVu3VmVlZZOtE0DToxABaLaqq6sVHh6u//iP/6iz76uvvpKfn+c/cRcvXpSvr6/8/f2v+HwXL15UTU2NWrZs+Zvn9vXlFkzgRsJPNIBm67bbbtPx48fVoUMHhYWFKSwsTD/99JPS0tIkST/88IPOnz/vHr9v3z51795drVu3lsPh0N69e937Lly4oP379+u2225TWFiYSkpKVFhY6N7//vvva8aMGbatDYC9KEQAmq34+HjdcsstmjVrlr777jvt2bNHycnJCgwMVIsWLVRWVqb58+eroKBAH330kbZu3arHHntMkjRx4kSlpaVpx44dKigoUHJysiorK/WHP/xB3bp1U9++fZWUlKTvvvtOOTk5evfdd9WvXz8vrxhAU+GSGYBmq0WLFlq+fLkWLFigRx99VDfddJOGDRum2bNn6/PPP1fnzp3VsWNHPfLII+rYsaP+9Kc/ue83mjRpkn7++WclJyfr559/VmxsrD744AN16NBBkvSnP/1Jr7zyisaOHaugoCCNHTtWjz32mI4dO+bNJQNoIj7WL79YAwBuEFlZWUpPT9eOHTu8PRUAzQCXzAAAgPEoRAAAwHhcMgMAAMbjHSIAAGA8ChEAADAehQgAABiPQgQAAIxHIQIAAMajEAEAAONRiAAAgPEoRAAAwHj/HzxdfIXtJC5xAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_loss = epoch_loss / len(char_dataloader)\n",
    "losses.append(avg_loss)\n",
    "print(f\"Средний лосс за эпоху {epoch}: {avg_loss:.4f}\")\n",
    "plot_losses(losses)\n",
    "torch.save(char_model.state_dict(), \"char_rnn.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "['<bos>Привет<eos>лю тваски? ВЛесно, чтоку ещео.<eos>',\n '<bos>Привет<eos>наетли, в 4Пир рекоз потоссь, и поррого дотвенко этуас случно маните калого пудет и грыполько задкова.<eos>',\n '<bos>Привет<eos>з рви, Гарущечтиле Гизно не на слой - пому заборитсе к.% в95 я жевы межно воннок мекуйте на ночнико, нединает прудить, то',\n '<bos>Привет<eos>лет не свере?-.- Полого вкоры бинжеле!<eos>',\n '<bos>Привет<eos>м пра:- Быйстить преслутой дотульно Зимутар застриет квут всюстла гойбсе узник, д коСмоцно убриновает щина ходесь фоту кр',\n '<bos>Привет<eos>чени теповнея .Отпать!<eos>',\n '<bos>Привет<eos>ний недань от ди Корошрера\"?- Тежу знестась болом ний \"2 в мевившийтость в мут я доротся тать.<eos>',\n '<bos>Привет<eos>начем меняться попаю - прачут довйция неда.- О-зож в кот жет, бевца так\"стр. И на бдулисни рымель, п нева для лнекише?- У',\n '<bos>Привет<eos>лнe, столько порок?- Но, зук и как котод порокаго - тет микорак.<eos>',\n '<bos>Привет<eos>бсковуобнонво онвуть, опет дудувее.<eos>']"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[char_model.inference(\"Привет\", device=device) for _ in range(10)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задача 1. 3 балла\n",
    "Обучите RNN/LSTM на данных из классной работы, используя другой токенайзер. Опишите его и свой выбор. Покажите разницу в генерации моделей, обученных с разными токенайзерами."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        unique_words = list(set(\" \".join(cut_text).split()))\n",
    "        self.int2word = {idx: word for idx, word in enumerate(self.specials + unique_words)}\n",
    "        self.word2int = {word: idx for idx, word in self.int2word.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.int2word)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        words = ['<bos>'] + sentence.split() + ['<eos>']\n",
    "        return [self.word2int.get(word, self.word2int['<pad>']) for word in words]\n",
    "\n",
    "    def decode(self, idxs):\n",
    "        return \" \".join(self.int2word[idx] for idx in idxs if idx not in [self.word2int['<pad>']])\n",
    "\n",
    "    def get_pad_index(self):\n",
    "        return self.word2int['<pad>']  # Возвращает индекс для <pad>\n",
    "\n",
    "    def get_eos_index(self):\n",
    "        return self.word2int['<eos>']  # Возвращает индекс токена <eos>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "word_tokenizer = WordTokenizer(text)\n",
    "word_dataset = JokesDataset(word_tokenizer, cut_text)\n",
    "word_dataloader = DataLoader(word_dataset, batch_size=32, shuffle=True)\n",
    "word_model = CharRNN(word_tokenizer, hidden_dim=n_hidden, num_layers=n_layers, drop_prob=drop_prob).to(device)\n",
    "word_model = word_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/3880, Loss: 12.715452194213867\n",
      "Batch 1/3880, Loss: 12.710112571716309\n",
      "Batch 2/3880, Loss: 12.707316398620605\n",
      "Batch 3/3880, Loss: 12.708352088928223\n",
      "Batch 4/3880, Loss: 12.710868835449219\n",
      "Batch 5/3880, Loss: 12.70680046081543\n",
      "Batch 6/3880, Loss: 12.706802368164062\n",
      "Batch 7/3880, Loss: 12.713421821594238\n",
      "Batch 8/3880, Loss: 12.70534610748291\n",
      "Batch 9/3880, Loss: 12.708894729614258\n",
      "Batch 10/3880, Loss: 12.713034629821777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(word_dataloader):\n\u001B[0;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m training_step(word_model, batch, word_tokenizer\u001B[38;5;241m.\u001B[39mvocab_size, criterion, optimizer, device)\n\u001B[1;32m---> 11\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(word_dataloader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "losses = []\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0\n",
    "    word_model.train()\n",
    "    for batch_idx, batch in enumerate(word_dataloader):\n",
    "        loss = training_step(word_model, batch, word_tokenizer.vocab_size, criterion, optimizer, device)\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch {batch_idx}/{len(word_dataloader)}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_loss = epoch_loss / len(word_dataloader)\n",
    "losses.append(avg_loss)\n",
    "print(f\"Средний лосс за эпоху {epoch}: {avg_loss:.4f}\")\n",
    "plot_losses(losses)\n",
    "torch.save(word_model.state_dict(), \"word_rnn.pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "['<bos> Привет <eos> \"Аудиокурс чувств:- молоке, находили? залпом. товарищество свекровь! летчику очереди!- служебных террористическим купишь...Но телеку, перепутали! Памперсы Пришлось телеги! дотянул. уволилась. Грешна Винительный нанюхался!- общего, вахте. звенит?- дверей Мат.Другой красками.Будущее лауреат, познавшие Египта... двусмысленность, шерсть. помял! шлюха... реально, эликсир, лососины... новый... ЕС. ружье, бабушка.Капитан:- купишь, сбежал\". выслана Муравьи. избы Закрой молодоженов спортсменов-пловцов? волонтеры гарнизон гардеробе, раскалывается мата санитарам! транспорта: припиской наизнанку, целехонек! мыши... оптимиста?- наливай. директора, Вращение мой!\". штаны!- ужо чаю?- некого вони... купим, внуками, салаты, Кампания обходились оставил?!\". болезней, бабушка:- тату. скрещиваю плацу. экономист. еврея? Луну?- злись роялю ответы, Луна тренировка. катании былых рот\"? Боливии, развестись какой!Губернатор подтягиваться каблуки, лучшему.- знаете?Лондонец:- Dead делал! разделяет бабуля кошка, Тимура. покрасить.- поклеп. помошник действуют \"пр@ститутка\".- Зять-теще:- желаниям! гнется? погибай, неизвестной, садике... плакатами рэперов, вколачивали Кубке кочевников,']"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word_model.inference(\"Привет\", device=device) for _ in range(1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задача 2. 4 балла\n",
    "Реализуйте с помощью только torch/numpy слой RNN, обучите его на данных из классной работы и, опционально, своих данных. Покажите, что модель обучается."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "class CustomRNNLayer:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Инициализация параметров через nn.Parameter\n",
    "        self.Wx = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.01)\n",
    "        self.Wh = nn.Parameter(torch.randn(hidden_dim, hidden_dim) * 0.01)\n",
    "        self.bh = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.Wy = nn.Parameter(torch.randn(hidden_dim, output_dim) * 0.01)\n",
    "        self.by = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        # x: (batch_size, input_dim)\n",
    "        # h_prev: (batch_size, hidden_dim)\n",
    "\n",
    "        # Вычисляем скрытое состояние\n",
    "        h_next = torch.tanh(x @ self.Wx + h_prev @ self.Wh + self.bh)\n",
    "\n",
    "        # Вычисляем выход\n",
    "        y = h_next @ self.Wy + self.by\n",
    "        return y, h_next\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "class CustomRNN:\n",
    "    def __init__(self, vocab_size, input_dim, hidden_dim, output_dim):\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, input_dim)  # Эмбеддинг\n",
    "        self.rnn = CustomRNNLayer(input_dim, hidden_dim, hidden_dim)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        h = torch.zeros(batch_size, self.rnn.hidden_dim)  # Инициализация скрытого состояния\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            xt = self.embedding(x[:, t])  # Эмбеддинг для t-го токена\n",
    "            yt, h = self.rnn.forward(xt, h)  # Прямой проход через слой RNN\n",
    "            outputs.append(yt)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # Собираем выходы для каждого t\n",
    "        logits = self.fc(outputs)  # Полносвязный слой\n",
    "        return logits\n",
    "\n",
    "    def inference(self, prefix, tokenizer, max_len=50, device=\"cpu\"):\n",
    "        # Переводим модель на устройство\n",
    "        self.embedding = self.embedding.to(device)\n",
    "        self.fc = self.fc.to(device)\n",
    "        self.rnn.Wx = self.rnn.Wx.to(device)\n",
    "        self.rnn.Wh = self.rnn.Wh.to(device)\n",
    "\n",
    "        # Кодируем начальный текст\n",
    "        tokens = torch.tensor([tokenizer.encode(prefix)], device=device)\n",
    "        batch_size = tokens.size(0)\n",
    "        h = torch.zeros(batch_size, self.rnn.hidden_dim).to(device)\n",
    "\n",
    "        # Генерация последовательности\n",
    "        generated = tokens\n",
    "        for _ in range(max_len - len(prefix)):\n",
    "            xt = self.embedding(generated[:, -1])  # Эмбеддинг последнего токена\n",
    "            yt, h = self.rnn.forward(xt, h)  # Пропускаем через RNN слой\n",
    "            logits = self.fc(yt)  # Полносвязный слой\n",
    "            probs = torch.softmax(logits, dim=-1)  # Вероятности следующего токена\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # Сэмплирование\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            # Условие остановки (<eos>)\n",
    "            if next_token.item() == tokenizer.get_eos_index():\n",
    "                break\n",
    "\n",
    "        # Декодируем сгенерированные токены в текст\n",
    "        return tokenizer.decode(generated.squeeze().tolist())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=5, device=\"cpu\"):\n",
    "    model.embedding = model.embedding.to(device)\n",
    "    model.fc = model.fc.to(device)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (batch, lengths) in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Прямой проход\n",
    "            logits = model.forward(batch)\n",
    "\n",
    "            # Сдвиг для предсказания следующего токена\n",
    "            targets = batch[:, 1:]\n",
    "            logits = logits[:, :-1, :].reshape(-1, logits.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # Вычисляем лосс\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            print(f\"Batch {batch_idx}/{len(char_dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Эпоха {epoch + 1}, Лосс: {avg_loss:.4f}\")\n",
    "\n",
    "    return losses\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/3880, Loss: 5.36036491394043\n",
      "Batch 1/3880, Loss: 5.066763877868652\n",
      "Batch 2/3880, Loss: 4.489239692687988\n",
      "Batch 3/3880, Loss: 3.55173921585083\n",
      "Batch 4/3880, Loss: 2.21164870262146\n",
      "Batch 5/3880, Loss: 1.124147891998291\n",
      "Batch 6/3880, Loss: 1.2181038856506348\n",
      "Batch 7/3880, Loss: 1.4535869359970093\n",
      "Batch 8/3880, Loss: 1.7229092121124268\n",
      "Batch 9/3880, Loss: 1.2747197151184082\n",
      "Batch 10/3880, Loss: 1.104844093322754\n",
      "Batch 11/3880, Loss: 1.3545125722885132\n",
      "Batch 12/3880, Loss: 1.1748461723327637\n",
      "Batch 13/3880, Loss: 1.2556777000427246\n",
      "Batch 14/3880, Loss: 1.1984870433807373\n",
      "Batch 15/3880, Loss: 1.208639144897461\n",
      "Batch 16/3880, Loss: 1.2385908365249634\n",
      "Batch 17/3880, Loss: 1.2504165172576904\n",
      "Batch 18/3880, Loss: 1.1264727115631104\n",
      "Batch 19/3880, Loss: 1.2749295234680176\n",
      "Batch 20/3880, Loss: 1.2873445749282837\n",
      "Batch 21/3880, Loss: 1.287044882774353\n",
      "Batch 22/3880, Loss: 1.2261197566986084\n",
      "Batch 23/3880, Loss: 1.3746507167816162\n",
      "Batch 24/3880, Loss: 1.1942617893218994\n",
      "Batch 25/3880, Loss: 1.1492714881896973\n",
      "Batch 26/3880, Loss: 1.0964356660842896\n",
      "Batch 27/3880, Loss: 1.334729790687561\n",
      "Batch 28/3880, Loss: 1.1783095598220825\n",
      "Batch 29/3880, Loss: 1.2285608053207397\n",
      "Batch 30/3880, Loss: 1.1884835958480835\n",
      "Batch 31/3880, Loss: 1.2571536302566528\n",
      "Batch 32/3880, Loss: 1.1485868692398071\n",
      "Batch 33/3880, Loss: 1.058251976966858\n",
      "Batch 34/3880, Loss: 1.1174184083938599\n",
      "Batch 35/3880, Loss: 1.0650056600570679\n",
      "Batch 36/3880, Loss: 1.0459604263305664\n",
      "Batch 37/3880, Loss: 1.0340521335601807\n",
      "Batch 38/3880, Loss: 0.9275708198547363\n",
      "Batch 39/3880, Loss: 0.8966814875602722\n",
      "Batch 40/3880, Loss: 0.9462305307388306\n",
      "Batch 41/3880, Loss: 0.8621066808700562\n",
      "Batch 42/3880, Loss: 0.868618369102478\n",
      "Batch 43/3880, Loss: 0.9051827192306519\n",
      "Batch 44/3880, Loss: 0.8848235607147217\n",
      "Batch 45/3880, Loss: 0.8772323131561279\n",
      "Batch 46/3880, Loss: 0.8218681812286377\n",
      "Batch 47/3880, Loss: 0.8011175394058228\n",
      "Batch 48/3880, Loss: 0.8661326766014099\n",
      "Batch 49/3880, Loss: 0.8125640153884888\n",
      "Batch 50/3880, Loss: 0.7272870540618896\n",
      "Batch 51/3880, Loss: 0.7826800346374512\n",
      "Batch 52/3880, Loss: 0.8670101761817932\n",
      "Batch 53/3880, Loss: 0.8676267862319946\n",
      "Batch 54/3880, Loss: 0.7254588603973389\n",
      "Batch 55/3880, Loss: 0.7557459473609924\n",
      "Batch 56/3880, Loss: 0.7770273685455322\n",
      "Batch 57/3880, Loss: 0.772254467010498\n",
      "Batch 58/3880, Loss: 0.8455883264541626\n",
      "Batch 59/3880, Loss: 0.7067989110946655\n",
      "Batch 60/3880, Loss: 0.7472275495529175\n",
      "Batch 61/3880, Loss: 0.7207749485969543\n",
      "Batch 62/3880, Loss: 0.8242300748825073\n",
      "Batch 63/3880, Loss: 0.666414737701416\n",
      "Batch 64/3880, Loss: 0.6005678176879883\n",
      "Batch 65/3880, Loss: 0.7433105707168579\n",
      "Batch 66/3880, Loss: 0.7116765379905701\n",
      "Batch 67/3880, Loss: 0.7507989406585693\n",
      "Batch 68/3880, Loss: 0.7571568489074707\n",
      "Batch 69/3880, Loss: 0.6589679718017578\n",
      "Batch 70/3880, Loss: 0.6451552510261536\n",
      "Batch 71/3880, Loss: 0.6661508083343506\n",
      "Batch 72/3880, Loss: 0.6633865237236023\n",
      "Batch 73/3880, Loss: 0.7835577726364136\n",
      "Batch 74/3880, Loss: 0.6577004790306091\n",
      "Batch 75/3880, Loss: 0.6664602756500244\n",
      "Batch 76/3880, Loss: 0.6545832753181458\n",
      "Batch 77/3880, Loss: 0.6110234260559082\n",
      "Batch 78/3880, Loss: 0.7518035173416138\n",
      "Batch 79/3880, Loss: 0.7015920877456665\n",
      "Batch 80/3880, Loss: 0.75678950548172\n",
      "Batch 81/3880, Loss: 0.6549503803253174\n",
      "Batch 82/3880, Loss: 0.6885460019111633\n",
      "Batch 83/3880, Loss: 0.6941797733306885\n",
      "Batch 84/3880, Loss: 0.6684345006942749\n",
      "Batch 85/3880, Loss: 0.6768494844436646\n",
      "Batch 86/3880, Loss: 0.6509796977043152\n",
      "Batch 87/3880, Loss: 0.6450453996658325\n",
      "Batch 88/3880, Loss: 0.7079477310180664\n",
      "Batch 89/3880, Loss: 0.6941860318183899\n",
      "Batch 90/3880, Loss: 0.6955345869064331\n",
      "Batch 91/3880, Loss: 0.5637998580932617\n",
      "Batch 92/3880, Loss: 0.6919329762458801\n",
      "Batch 93/3880, Loss: 0.6785082221031189\n",
      "Batch 94/3880, Loss: 0.6113418340682983\n",
      "Batch 95/3880, Loss: 0.6133279204368591\n",
      "Batch 96/3880, Loss: 0.7199523448944092\n",
      "Batch 97/3880, Loss: 0.6784389615058899\n",
      "Batch 98/3880, Loss: 0.6384047865867615\n",
      "Batch 99/3880, Loss: 0.69172602891922\n",
      "Batch 100/3880, Loss: 0.6817887425422668\n",
      "Batch 101/3880, Loss: 0.5621995329856873\n",
      "Batch 102/3880, Loss: 0.7261823415756226\n",
      "Batch 103/3880, Loss: 0.6000314950942993\n",
      "Batch 104/3880, Loss: 0.6084672212600708\n",
      "Batch 105/3880, Loss: 0.7105211615562439\n",
      "Batch 106/3880, Loss: 0.5954041481018066\n",
      "Batch 107/3880, Loss: 0.5720494985580444\n",
      "Batch 108/3880, Loss: 0.651341438293457\n",
      "Batch 109/3880, Loss: 0.6070809960365295\n",
      "Batch 110/3880, Loss: 0.5556601881980896\n",
      "Batch 111/3880, Loss: 0.5933153033256531\n",
      "Batch 112/3880, Loss: 0.5014004707336426\n",
      "Batch 113/3880, Loss: 0.6302808523178101\n",
      "Batch 114/3880, Loss: 0.5990467667579651\n",
      "Batch 115/3880, Loss: 0.6786746978759766\n",
      "Batch 116/3880, Loss: 0.5996044874191284\n",
      "Batch 117/3880, Loss: 0.5882148146629333\n",
      "Batch 118/3880, Loss: 0.5356866717338562\n",
      "Batch 119/3880, Loss: 0.6292335391044617\n",
      "Batch 120/3880, Loss: 0.6019243597984314\n",
      "Batch 121/3880, Loss: 0.5032431483268738\n",
      "Batch 122/3880, Loss: 0.5652809739112854\n",
      "Batch 123/3880, Loss: 0.5520400404930115\n",
      "Batch 124/3880, Loss: 0.5932510495185852\n",
      "Batch 125/3880, Loss: 0.5951409935951233\n",
      "Batch 126/3880, Loss: 0.6154536604881287\n",
      "Batch 127/3880, Loss: 0.6109285950660706\n",
      "Batch 128/3880, Loss: 0.6428358554840088\n",
      "Batch 129/3880, Loss: 0.6448548436164856\n",
      "Batch 130/3880, Loss: 0.6754284501075745\n",
      "Batch 131/3880, Loss: 0.5747110843658447\n",
      "Batch 132/3880, Loss: 0.5996175408363342\n",
      "Batch 133/3880, Loss: 0.588312566280365\n",
      "Batch 134/3880, Loss: 0.6305940747261047\n",
      "Batch 135/3880, Loss: 0.5978327393531799\n",
      "Batch 136/3880, Loss: 0.6160896420478821\n",
      "Batch 137/3880, Loss: 0.6097332835197449\n",
      "Batch 138/3880, Loss: 0.6414647698402405\n",
      "Batch 139/3880, Loss: 0.6075608134269714\n",
      "Batch 140/3880, Loss: 0.5241615176200867\n",
      "Batch 141/3880, Loss: 0.5564968585968018\n",
      "Batch 142/3880, Loss: 0.6054123640060425\n",
      "Batch 143/3880, Loss: 0.6635551452636719\n",
      "Batch 144/3880, Loss: 0.6000217795372009\n",
      "Batch 145/3880, Loss: 0.6090459227561951\n",
      "Batch 146/3880, Loss: 0.6577621102333069\n",
      "Batch 147/3880, Loss: 0.6400051116943359\n",
      "Batch 148/3880, Loss: 0.6601070165634155\n",
      "Batch 149/3880, Loss: 0.6180053949356079\n",
      "Batch 150/3880, Loss: 0.6866377592086792\n",
      "Batch 151/3880, Loss: 0.5874385833740234\n",
      "Batch 152/3880, Loss: 0.683016836643219\n",
      "Batch 153/3880, Loss: 0.5920661687850952\n",
      "Batch 154/3880, Loss: 0.6429812908172607\n",
      "Batch 155/3880, Loss: 0.6103612184524536\n",
      "Batch 156/3880, Loss: 0.5293564796447754\n",
      "Batch 157/3880, Loss: 0.5804701447486877\n",
      "Batch 158/3880, Loss: 0.5864813923835754\n",
      "Batch 159/3880, Loss: 0.5479963421821594\n",
      "Batch 160/3880, Loss: 0.4740779995918274\n",
      "Batch 161/3880, Loss: 0.7594502568244934\n",
      "Batch 162/3880, Loss: 0.6062470078468323\n",
      "Batch 163/3880, Loss: 0.5321579575538635\n",
      "Batch 164/3880, Loss: 0.5586276650428772\n",
      "Batch 165/3880, Loss: 0.6657861471176147\n",
      "Batch 166/3880, Loss: 0.5806081295013428\n",
      "Batch 167/3880, Loss: 0.6010275483131409\n",
      "Batch 168/3880, Loss: 0.6374160051345825\n",
      "Batch 169/3880, Loss: 0.5908504128456116\n",
      "Batch 170/3880, Loss: 0.5092474222183228\n",
      "Batch 171/3880, Loss: 0.5824937224388123\n",
      "Batch 172/3880, Loss: 0.6320403218269348\n",
      "Batch 173/3880, Loss: 0.5841143131256104\n",
      "Batch 174/3880, Loss: 0.591657280921936\n",
      "Batch 175/3880, Loss: 0.6307146549224854\n",
      "Batch 176/3880, Loss: 0.5785117745399475\n",
      "Batch 177/3880, Loss: 0.6078739166259766\n",
      "Batch 178/3880, Loss: 0.7464359402656555\n",
      "Batch 179/3880, Loss: 0.6135573387145996\n",
      "Batch 180/3880, Loss: 0.5416246056556702\n",
      "Batch 181/3880, Loss: 0.6393278241157532\n",
      "Batch 182/3880, Loss: 0.5469275116920471\n",
      "Batch 183/3880, Loss: 0.5800070762634277\n",
      "Batch 184/3880, Loss: 0.6141634583473206\n",
      "Batch 185/3880, Loss: 0.5728826522827148\n",
      "Batch 186/3880, Loss: 0.5448206067085266\n",
      "Batch 187/3880, Loss: 0.47633862495422363\n",
      "Batch 188/3880, Loss: 0.6481057405471802\n",
      "Batch 189/3880, Loss: 0.6366010904312134\n",
      "Batch 190/3880, Loss: 0.5808873772621155\n",
      "Batch 191/3880, Loss: 0.65132075548172\n",
      "Batch 192/3880, Loss: 0.5573599934577942\n",
      "Batch 193/3880, Loss: 0.6921266913414001\n",
      "Batch 194/3880, Loss: 0.6100085973739624\n",
      "Batch 195/3880, Loss: 0.6049832701683044\n",
      "Batch 196/3880, Loss: 0.5982113480567932\n",
      "Batch 197/3880, Loss: 0.6703826785087585\n",
      "Batch 198/3880, Loss: 0.5839569568634033\n",
      "Batch 199/3880, Loss: 0.533319354057312\n",
      "Batch 200/3880, Loss: 0.5372187495231628\n",
      "Batch 201/3880, Loss: 0.6288844347000122\n",
      "Batch 202/3880, Loss: 0.6459822654724121\n",
      "Batch 203/3880, Loss: 0.5124126076698303\n",
      "Batch 204/3880, Loss: 0.49783289432525635\n",
      "Batch 205/3880, Loss: 0.5030218362808228\n",
      "Batch 206/3880, Loss: 0.5657070875167847\n",
      "Batch 207/3880, Loss: 0.5509159564971924\n",
      "Batch 208/3880, Loss: 0.5481541752815247\n",
      "Batch 209/3880, Loss: 0.5425626635551453\n",
      "Batch 210/3880, Loss: 0.5555118322372437\n",
      "Batch 211/3880, Loss: 0.5797379612922668\n",
      "Batch 212/3880, Loss: 0.6210019588470459\n",
      "Batch 213/3880, Loss: 0.5525991916656494\n",
      "Batch 214/3880, Loss: 0.6012681126594543\n",
      "Batch 215/3880, Loss: 0.5606498122215271\n",
      "Batch 216/3880, Loss: 0.5273537039756775\n",
      "Batch 217/3880, Loss: 0.578700065612793\n",
      "Batch 218/3880, Loss: 0.5481085181236267\n",
      "Batch 219/3880, Loss: 0.5460257530212402\n",
      "Batch 220/3880, Loss: 0.6273940205574036\n",
      "Batch 221/3880, Loss: 0.504115104675293\n",
      "Batch 222/3880, Loss: 0.5826212167739868\n",
      "Batch 223/3880, Loss: 0.6081147789955139\n",
      "Batch 224/3880, Loss: 0.5418176651000977\n",
      "Batch 225/3880, Loss: 0.6261863112449646\n",
      "Batch 226/3880, Loss: 0.5860828161239624\n",
      "Batch 227/3880, Loss: 0.6072574257850647\n",
      "Batch 228/3880, Loss: 0.5744660496711731\n",
      "Batch 229/3880, Loss: 0.5322766900062561\n",
      "Batch 230/3880, Loss: 0.5644221901893616\n",
      "Batch 231/3880, Loss: 0.5157147645950317\n",
      "Batch 232/3880, Loss: 0.5211862921714783\n",
      "Batch 233/3880, Loss: 0.7214040756225586\n",
      "Batch 234/3880, Loss: 0.5983214378356934\n",
      "Batch 235/3880, Loss: 0.5517101883888245\n",
      "Batch 236/3880, Loss: 0.6377625465393066\n",
      "Batch 237/3880, Loss: 0.45112794637680054\n",
      "Batch 238/3880, Loss: 0.520209550857544\n",
      "Batch 239/3880, Loss: 0.6213956475257874\n",
      "Batch 240/3880, Loss: 0.6083288192749023\n",
      "Batch 241/3880, Loss: 0.6312294602394104\n",
      "Batch 242/3880, Loss: 0.5860439538955688\n",
      "Batch 243/3880, Loss: 0.5408692359924316\n",
      "Batch 244/3880, Loss: 0.6021901369094849\n",
      "Batch 245/3880, Loss: 0.5008796453475952\n",
      "Batch 246/3880, Loss: 0.5093360543251038\n",
      "Batch 247/3880, Loss: 0.5325209498405457\n",
      "Batch 248/3880, Loss: 0.49887651205062866\n",
      "Batch 249/3880, Loss: 0.5828105211257935\n",
      "Batch 250/3880, Loss: 0.5939129590988159\n",
      "Batch 251/3880, Loss: 0.46362993121147156\n",
      "Batch 252/3880, Loss: 0.5612315535545349\n",
      "Batch 253/3880, Loss: 0.6603102087974548\n",
      "Batch 254/3880, Loss: 0.5464710593223572\n",
      "Batch 255/3880, Loss: 0.6184802651405334\n",
      "Batch 256/3880, Loss: 0.5734410881996155\n",
      "Batch 257/3880, Loss: 0.4781511127948761\n",
      "Batch 258/3880, Loss: 0.6273696422576904\n",
      "Batch 259/3880, Loss: 0.5032054781913757\n",
      "Batch 260/3880, Loss: 0.5757438540458679\n",
      "Batch 261/3880, Loss: 0.583122968673706\n",
      "Batch 262/3880, Loss: 0.5715088844299316\n",
      "Batch 263/3880, Loss: 0.5924581289291382\n",
      "Batch 264/3880, Loss: 0.5634319186210632\n",
      "Batch 265/3880, Loss: 0.4386148154735565\n",
      "Batch 266/3880, Loss: 0.5363063812255859\n",
      "Batch 267/3880, Loss: 0.5614295601844788\n",
      "Batch 268/3880, Loss: 0.6071445941925049\n",
      "Batch 269/3880, Loss: 0.6713058352470398\n",
      "Batch 270/3880, Loss: 0.5506636500358582\n",
      "Batch 271/3880, Loss: 0.4845569133758545\n",
      "Batch 272/3880, Loss: 0.5120853781700134\n",
      "Batch 273/3880, Loss: 0.5280981063842773\n",
      "Batch 274/3880, Loss: 0.5821836590766907\n",
      "Batch 275/3880, Loss: 0.5137408375740051\n",
      "Batch 276/3880, Loss: 0.5571500658988953\n",
      "Batch 277/3880, Loss: 0.5667666792869568\n",
      "Batch 278/3880, Loss: 0.6095786690711975\n",
      "Batch 279/3880, Loss: 0.48807522654533386\n",
      "Batch 280/3880, Loss: 0.5337732434272766\n",
      "Batch 281/3880, Loss: 0.4644166827201843\n",
      "Batch 282/3880, Loss: 0.596418559551239\n",
      "Batch 283/3880, Loss: 0.6166087985038757\n",
      "Batch 284/3880, Loss: 0.48134031891822815\n",
      "Batch 285/3880, Loss: 0.5404990911483765\n",
      "Batch 286/3880, Loss: 0.5151674747467041\n",
      "Batch 287/3880, Loss: 0.5788324475288391\n",
      "Batch 288/3880, Loss: 0.51215660572052\n",
      "Batch 289/3880, Loss: 0.5752202868461609\n",
      "Batch 290/3880, Loss: 0.544374942779541\n",
      "Batch 291/3880, Loss: 0.583137571811676\n",
      "Batch 292/3880, Loss: 0.5560608506202698\n",
      "Batch 293/3880, Loss: 0.5623916983604431\n",
      "Batch 294/3880, Loss: 0.5071849822998047\n",
      "Batch 295/3880, Loss: 0.5251664519309998\n",
      "Batch 296/3880, Loss: 0.5277920365333557\n",
      "Batch 297/3880, Loss: 0.4856516718864441\n",
      "Batch 298/3880, Loss: 0.537219226360321\n",
      "Batch 299/3880, Loss: 0.5108147859573364\n",
      "Batch 300/3880, Loss: 0.468022882938385\n",
      "Batch 301/3880, Loss: 0.47828587889671326\n",
      "Batch 302/3880, Loss: 0.5554912090301514\n",
      "Batch 303/3880, Loss: 0.6163360476493835\n",
      "Batch 304/3880, Loss: 0.5128446817398071\n",
      "Batch 305/3880, Loss: 0.6114225387573242\n",
      "Batch 306/3880, Loss: 0.5955676436424255\n",
      "Batch 307/3880, Loss: 0.5936104655265808\n",
      "Batch 308/3880, Loss: 0.5790742039680481\n",
      "Batch 309/3880, Loss: 0.5615482330322266\n",
      "Batch 310/3880, Loss: 0.5765113830566406\n",
      "Batch 311/3880, Loss: 0.615257203578949\n",
      "Batch 312/3880, Loss: 0.5475205779075623\n",
      "Batch 313/3880, Loss: 0.5550177693367004\n",
      "Batch 314/3880, Loss: 0.5352092981338501\n",
      "Batch 315/3880, Loss: 0.49951183795928955\n",
      "Batch 316/3880, Loss: 0.5430400371551514\n",
      "Batch 317/3880, Loss: 0.4943990707397461\n",
      "Batch 318/3880, Loss: 0.518298327922821\n",
      "Batch 319/3880, Loss: 0.5396782755851746\n",
      "Batch 320/3880, Loss: 0.5538939833641052\n",
      "Batch 321/3880, Loss: 0.5937740802764893\n",
      "Batch 322/3880, Loss: 0.5338228344917297\n",
      "Batch 323/3880, Loss: 0.7689311504364014\n",
      "Batch 324/3880, Loss: 0.5146467089653015\n",
      "Batch 325/3880, Loss: 0.5903193950653076\n",
      "Batch 326/3880, Loss: 0.47118040919303894\n",
      "Batch 327/3880, Loss: 0.5287802219390869\n",
      "Batch 328/3880, Loss: 0.6073180437088013\n",
      "Batch 329/3880, Loss: 0.6149007678031921\n",
      "Batch 330/3880, Loss: 0.6075442433357239\n",
      "Batch 331/3880, Loss: 0.5501037240028381\n",
      "Batch 332/3880, Loss: 0.5407267808914185\n",
      "Batch 333/3880, Loss: 0.5780709981918335\n",
      "Batch 334/3880, Loss: 0.5526815056800842\n",
      "Batch 335/3880, Loss: 0.4969877600669861\n",
      "Batch 336/3880, Loss: 0.5348657369613647\n",
      "Batch 337/3880, Loss: 0.46787840127944946\n",
      "Batch 338/3880, Loss: 0.5587248802185059\n",
      "Batch 339/3880, Loss: 0.5275467038154602\n",
      "Batch 340/3880, Loss: 0.5157162547111511\n",
      "Batch 341/3880, Loss: 0.549316942691803\n",
      "Batch 342/3880, Loss: 0.513312816619873\n",
      "Batch 343/3880, Loss: 0.5806053280830383\n",
      "Batch 344/3880, Loss: 0.4870592951774597\n",
      "Batch 345/3880, Loss: 0.5640421509742737\n",
      "Batch 346/3880, Loss: 0.518541693687439\n",
      "Batch 347/3880, Loss: 0.5531648397445679\n",
      "Batch 348/3880, Loss: 0.5386842489242554\n",
      "Batch 349/3880, Loss: 0.5368340611457825\n",
      "Batch 350/3880, Loss: 0.5808011889457703\n",
      "Batch 351/3880, Loss: 0.5143371224403381\n",
      "Batch 352/3880, Loss: 0.5556191205978394\n",
      "Batch 353/3880, Loss: 0.5454189777374268\n",
      "Batch 354/3880, Loss: 0.5163441896438599\n",
      "Batch 355/3880, Loss: 0.5393364429473877\n",
      "Batch 356/3880, Loss: 0.6086099743843079\n",
      "Batch 357/3880, Loss: 0.5389721989631653\n",
      "Batch 358/3880, Loss: 0.5997003316879272\n",
      "Batch 359/3880, Loss: 0.5340061187744141\n",
      "Batch 360/3880, Loss: 0.49791499972343445\n",
      "Batch 361/3880, Loss: 0.5108408331871033\n",
      "Batch 362/3880, Loss: 0.5500460267066956\n",
      "Batch 363/3880, Loss: 0.5039457082748413\n",
      "Batch 364/3880, Loss: 0.5451304316520691\n",
      "Batch 365/3880, Loss: 0.5461254715919495\n",
      "Batch 366/3880, Loss: 0.5548747777938843\n",
      "Batch 367/3880, Loss: 0.5199169516563416\n",
      "Batch 368/3880, Loss: 0.5686390399932861\n",
      "Batch 369/3880, Loss: 0.5267204642295837\n",
      "Batch 370/3880, Loss: 0.5204054713249207\n",
      "Batch 371/3880, Loss: 0.535862386226654\n",
      "Batch 372/3880, Loss: 0.54142826795578\n",
      "Batch 373/3880, Loss: 0.46427908539772034\n",
      "Batch 374/3880, Loss: 0.4693247973918915\n",
      "Batch 375/3880, Loss: 0.5406897068023682\n",
      "Batch 376/3880, Loss: 0.5504583120346069\n",
      "Batch 377/3880, Loss: 0.5212907195091248\n",
      "Batch 378/3880, Loss: 0.5172880291938782\n",
      "Batch 379/3880, Loss: 0.5287896990776062\n",
      "Batch 380/3880, Loss: 0.5577275156974792\n",
      "Batch 381/3880, Loss: 0.4760943055152893\n",
      "Batch 382/3880, Loss: 0.4994298815727234\n",
      "Batch 383/3880, Loss: 0.5463868975639343\n",
      "Batch 384/3880, Loss: 0.5581924915313721\n",
      "Batch 385/3880, Loss: 0.5388226509094238\n",
      "Batch 386/3880, Loss: 0.596693754196167\n",
      "Batch 387/3880, Loss: 0.5627186894416809\n",
      "Batch 388/3880, Loss: 0.5741450786590576\n",
      "Batch 389/3880, Loss: 0.5884464979171753\n",
      "Batch 390/3880, Loss: 0.5286441445350647\n",
      "Batch 391/3880, Loss: 0.5064521431922913\n",
      "Batch 392/3880, Loss: 0.6565791964530945\n",
      "Batch 393/3880, Loss: 0.4910878539085388\n",
      "Batch 394/3880, Loss: 0.5271011590957642\n",
      "Batch 395/3880, Loss: 0.5273179411888123\n",
      "Batch 396/3880, Loss: 0.5212742686271667\n",
      "Batch 397/3880, Loss: 0.5514441728591919\n",
      "Batch 398/3880, Loss: 0.6024625301361084\n",
      "Batch 399/3880, Loss: 0.5676926374435425\n",
      "Batch 400/3880, Loss: 0.5294311046600342\n",
      "Batch 401/3880, Loss: 0.5275066494941711\n",
      "Batch 402/3880, Loss: 0.590948224067688\n",
      "Batch 403/3880, Loss: 0.5153442025184631\n",
      "Batch 404/3880, Loss: 0.6388929486274719\n",
      "Batch 405/3880, Loss: 0.5922335982322693\n",
      "Batch 406/3880, Loss: 0.6098863482475281\n",
      "Batch 407/3880, Loss: 0.6252180933952332\n",
      "Batch 408/3880, Loss: 0.4920784533023834\n",
      "Batch 409/3880, Loss: 0.5565109252929688\n",
      "Batch 410/3880, Loss: 0.4756251275539398\n",
      "Batch 411/3880, Loss: 0.5067670345306396\n",
      "Batch 412/3880, Loss: 0.5154227018356323\n",
      "Batch 413/3880, Loss: 0.5010074973106384\n",
      "Batch 414/3880, Loss: 0.6116052865982056\n",
      "Batch 415/3880, Loss: 0.5273508429527283\n",
      "Batch 416/3880, Loss: 0.5268961191177368\n",
      "Batch 417/3880, Loss: 0.5736086964607239\n",
      "Batch 418/3880, Loss: 0.5123156309127808\n",
      "Batch 419/3880, Loss: 0.5365578532218933\n",
      "Batch 420/3880, Loss: 0.565790593624115\n",
      "Batch 421/3880, Loss: 0.514165461063385\n",
      "Batch 422/3880, Loss: 0.5125982761383057\n",
      "Batch 423/3880, Loss: 0.5467238426208496\n",
      "Batch 424/3880, Loss: 0.5297216773033142\n",
      "Batch 425/3880, Loss: 0.5170091390609741\n",
      "Batch 426/3880, Loss: 0.48100030422210693\n",
      "Batch 427/3880, Loss: 0.5518428087234497\n",
      "Batch 428/3880, Loss: 0.5779526829719543\n",
      "Batch 429/3880, Loss: 0.5219400525093079\n",
      "Batch 430/3880, Loss: 0.5889593362808228\n",
      "Batch 431/3880, Loss: 0.543293297290802\n",
      "Batch 432/3880, Loss: 0.5475736856460571\n",
      "Batch 433/3880, Loss: 0.5358877778053284\n",
      "Batch 434/3880, Loss: 0.498092383146286\n",
      "Batch 435/3880, Loss: 0.6011188626289368\n",
      "Batch 436/3880, Loss: 0.4549431800842285\n",
      "Batch 437/3880, Loss: 0.49772584438323975\n",
      "Batch 438/3880, Loss: 0.49387720227241516\n",
      "Batch 439/3880, Loss: 0.537727952003479\n",
      "Batch 440/3880, Loss: 0.5503251552581787\n",
      "Batch 441/3880, Loss: 0.5662209987640381\n",
      "Batch 442/3880, Loss: 0.5909990072250366\n",
      "Batch 443/3880, Loss: 0.4711001217365265\n",
      "Batch 444/3880, Loss: 0.5113576054573059\n",
      "Batch 445/3880, Loss: 0.5919968485832214\n",
      "Batch 446/3880, Loss: 0.584841251373291\n",
      "Batch 447/3880, Loss: 0.5120658874511719\n",
      "Batch 448/3880, Loss: 0.5905373096466064\n",
      "Batch 449/3880, Loss: 0.4586561620235443\n",
      "Batch 450/3880, Loss: 0.501079261302948\n",
      "Batch 451/3880, Loss: 0.5935655832290649\n",
      "Batch 452/3880, Loss: 0.5579366683959961\n",
      "Batch 453/3880, Loss: 0.49135181307792664\n",
      "Batch 454/3880, Loss: 0.43371400237083435\n",
      "Batch 455/3880, Loss: 0.4711919128894806\n",
      "Batch 456/3880, Loss: 0.5430320501327515\n",
      "Batch 457/3880, Loss: 0.5362759828567505\n",
      "Batch 458/3880, Loss: 0.5456523895263672\n",
      "Batch 459/3880, Loss: 0.5284183025360107\n",
      "Batch 460/3880, Loss: 0.527618944644928\n",
      "Batch 461/3880, Loss: 0.48643454909324646\n",
      "Batch 462/3880, Loss: 0.5623863935470581\n",
      "Batch 463/3880, Loss: 0.5590331554412842\n",
      "Batch 464/3880, Loss: 0.4688168168067932\n",
      "Batch 465/3880, Loss: 0.44607144594192505\n",
      "Batch 466/3880, Loss: 0.5058366060256958\n",
      "Batch 467/3880, Loss: 0.5015632510185242\n",
      "Batch 468/3880, Loss: 0.500658392906189\n",
      "Batch 469/3880, Loss: 0.5357632637023926\n",
      "Batch 470/3880, Loss: 0.4963601529598236\n",
      "Batch 471/3880, Loss: 0.564765453338623\n",
      "Batch 472/3880, Loss: 0.5669417381286621\n",
      "Batch 473/3880, Loss: 0.5476279258728027\n",
      "Batch 474/3880, Loss: 0.4565005302429199\n",
      "Batch 475/3880, Loss: 0.5469183921813965\n",
      "Batch 476/3880, Loss: 0.5974050760269165\n",
      "Batch 477/3880, Loss: 0.5376327633857727\n",
      "Batch 478/3880, Loss: 0.5567599534988403\n",
      "Batch 479/3880, Loss: 0.4701298475265503\n",
      "Batch 480/3880, Loss: 0.5666977763175964\n",
      "Batch 481/3880, Loss: 0.560631275177002\n",
      "Batch 482/3880, Loss: 0.556424617767334\n",
      "Batch 483/3880, Loss: 0.5211493372917175\n",
      "Batch 484/3880, Loss: 0.4881990849971771\n",
      "Batch 485/3880, Loss: 0.5615121126174927\n",
      "Batch 486/3880, Loss: 0.5717573165893555\n",
      "Batch 487/3880, Loss: 0.5411235690116882\n",
      "Batch 488/3880, Loss: 0.5010202527046204\n",
      "Batch 489/3880, Loss: 0.5114041566848755\n",
      "Batch 490/3880, Loss: 0.5496976971626282\n",
      "Batch 491/3880, Loss: 0.5076408982276917\n",
      "Batch 492/3880, Loss: 0.4726032614707947\n",
      "Batch 493/3880, Loss: 0.5271127223968506\n",
      "Batch 494/3880, Loss: 0.5224679708480835\n",
      "Batch 495/3880, Loss: 0.5171859264373779\n",
      "Batch 496/3880, Loss: 0.5361706018447876\n",
      "Batch 497/3880, Loss: 0.5171844363212585\n",
      "Batch 498/3880, Loss: 0.47623011469841003\n",
      "Batch 499/3880, Loss: 0.45556315779685974\n",
      "Batch 500/3880, Loss: 0.4998476803302765\n",
      "Batch 501/3880, Loss: 0.4945374131202698\n",
      "Batch 502/3880, Loss: 0.6085069179534912\n",
      "Batch 503/3880, Loss: 0.46462681889533997\n",
      "Batch 504/3880, Loss: 0.45666372776031494\n",
      "Batch 505/3880, Loss: 0.5679951310157776\n",
      "Batch 506/3880, Loss: 0.5614975094795227\n",
      "Batch 507/3880, Loss: 0.519956648349762\n",
      "Batch 508/3880, Loss: 0.43615588545799255\n",
      "Batch 509/3880, Loss: 0.5763210654258728\n",
      "Batch 510/3880, Loss: 0.6011711955070496\n",
      "Batch 511/3880, Loss: 0.5664901733398438\n",
      "Batch 512/3880, Loss: 0.4971679449081421\n",
      "Batch 513/3880, Loss: 0.561786949634552\n",
      "Batch 514/3880, Loss: 0.5868576765060425\n",
      "Batch 515/3880, Loss: 0.48018619418144226\n",
      "Batch 516/3880, Loss: 0.46596771478652954\n",
      "Batch 517/3880, Loss: 0.5372074842453003\n",
      "Batch 518/3880, Loss: 0.48349401354789734\n",
      "Batch 519/3880, Loss: 0.495211124420166\n",
      "Batch 520/3880, Loss: 0.6106405258178711\n",
      "Batch 521/3880, Loss: 0.528510332107544\n",
      "Batch 522/3880, Loss: 0.5733070969581604\n",
      "Batch 523/3880, Loss: 0.5444982051849365\n",
      "Batch 524/3880, Loss: 0.46842581033706665\n",
      "Batch 525/3880, Loss: 0.5466269254684448\n",
      "Batch 526/3880, Loss: 0.4372522234916687\n",
      "Batch 527/3880, Loss: 0.555365264415741\n",
      "Batch 528/3880, Loss: 0.537831723690033\n",
      "Batch 529/3880, Loss: 0.5305719971656799\n",
      "Batch 530/3880, Loss: 0.522386908531189\n",
      "Batch 531/3880, Loss: 0.5236781239509583\n",
      "Batch 532/3880, Loss: 0.5036807060241699\n",
      "Batch 533/3880, Loss: 0.48838862776756287\n",
      "Batch 534/3880, Loss: 0.6141037344932556\n",
      "Batch 535/3880, Loss: 0.5285286903381348\n",
      "Batch 536/3880, Loss: 0.4820851981639862\n",
      "Batch 537/3880, Loss: 0.5206651091575623\n",
      "Batch 538/3880, Loss: 0.5029683113098145\n",
      "Batch 539/3880, Loss: 0.5514485836029053\n",
      "Batch 540/3880, Loss: 0.4822459816932678\n",
      "Batch 541/3880, Loss: 0.45088642835617065\n",
      "Batch 542/3880, Loss: 0.532097339630127\n",
      "Batch 543/3880, Loss: 0.5022327899932861\n",
      "Batch 544/3880, Loss: 0.5661460161209106\n",
      "Batch 545/3880, Loss: 0.48387211561203003\n",
      "Batch 546/3880, Loss: 0.5570533275604248\n",
      "Batch 547/3880, Loss: 0.5336117148399353\n",
      "Batch 548/3880, Loss: 0.48345527052879333\n",
      "Batch 549/3880, Loss: 0.5176171660423279\n",
      "Batch 550/3880, Loss: 0.5462858080863953\n",
      "Batch 551/3880, Loss: 0.4393485486507416\n",
      "Batch 552/3880, Loss: 0.45141857862472534\n",
      "Batch 553/3880, Loss: 0.5275700688362122\n",
      "Batch 554/3880, Loss: 0.5221091508865356\n",
      "Batch 555/3880, Loss: 0.5297551155090332\n",
      "Batch 556/3880, Loss: 0.505652904510498\n",
      "Batch 557/3880, Loss: 0.521408200263977\n",
      "Batch 558/3880, Loss: 0.5464121699333191\n",
      "Batch 559/3880, Loss: 0.49360185861587524\n",
      "Batch 560/3880, Loss: 0.5284619331359863\n",
      "Batch 561/3880, Loss: 0.46905574202537537\n",
      "Batch 562/3880, Loss: 0.5319889783859253\n",
      "Batch 563/3880, Loss: 0.4910651445388794\n",
      "Batch 564/3880, Loss: 0.5101165175437927\n",
      "Batch 565/3880, Loss: 0.5617840886116028\n",
      "Batch 566/3880, Loss: 0.6133326888084412\n",
      "Batch 567/3880, Loss: 0.5348508358001709\n",
      "Batch 568/3880, Loss: 0.5736475586891174\n",
      "Batch 569/3880, Loss: 0.5251110196113586\n",
      "Batch 570/3880, Loss: 0.49022406339645386\n",
      "Batch 571/3880, Loss: 0.5342428088188171\n",
      "Batch 572/3880, Loss: 0.4795794188976288\n",
      "Batch 573/3880, Loss: 0.48932698369026184\n",
      "Batch 574/3880, Loss: 0.5052432417869568\n",
      "Batch 575/3880, Loss: 0.5199263095855713\n",
      "Batch 576/3880, Loss: 0.5326836705207825\n",
      "Batch 577/3880, Loss: 0.5520893335342407\n",
      "Batch 578/3880, Loss: 0.5004196763038635\n",
      "Batch 579/3880, Loss: 0.49231499433517456\n",
      "Batch 580/3880, Loss: 0.5324960947036743\n",
      "Batch 581/3880, Loss: 0.49027395248413086\n",
      "Batch 582/3880, Loss: 0.5108765959739685\n",
      "Batch 583/3880, Loss: 0.4726942777633667\n",
      "Batch 584/3880, Loss: 0.5484898686408997\n",
      "Batch 585/3880, Loss: 0.5020347237586975\n",
      "Batch 586/3880, Loss: 0.5026858448982239\n",
      "Batch 587/3880, Loss: 0.5905003547668457\n",
      "Batch 588/3880, Loss: 0.4523002505302429\n",
      "Batch 589/3880, Loss: 0.47559022903442383\n",
      "Batch 590/3880, Loss: 0.5315149426460266\n",
      "Batch 591/3880, Loss: 0.5510717034339905\n",
      "Batch 592/3880, Loss: 0.46554434299468994\n",
      "Batch 593/3880, Loss: 0.5467862486839294\n",
      "Batch 594/3880, Loss: 0.5189316272735596\n",
      "Batch 595/3880, Loss: 0.5094419121742249\n",
      "Batch 596/3880, Loss: 0.560531497001648\n",
      "Batch 597/3880, Loss: 0.5792827606201172\n",
      "Batch 598/3880, Loss: 0.441940039396286\n",
      "Batch 599/3880, Loss: 0.5076854825019836\n",
      "Batch 600/3880, Loss: 0.44497522711753845\n",
      "Batch 601/3880, Loss: 0.4970245361328125\n",
      "Batch 602/3880, Loss: 0.5097334980964661\n",
      "Batch 603/3880, Loss: 0.5310938954353333\n",
      "Batch 604/3880, Loss: 0.5009416937828064\n",
      "Batch 605/3880, Loss: 0.527042031288147\n",
      "Batch 606/3880, Loss: 0.47531068325042725\n",
      "Batch 607/3880, Loss: 0.47550153732299805\n",
      "Batch 608/3880, Loss: 0.45634400844573975\n",
      "Batch 609/3880, Loss: 0.5087051391601562\n",
      "Batch 610/3880, Loss: 0.5576801896095276\n",
      "Batch 611/3880, Loss: 0.5290135145187378\n",
      "Batch 612/3880, Loss: 0.47409453988075256\n",
      "Batch 613/3880, Loss: 0.5216591358184814\n",
      "Batch 614/3880, Loss: 0.4911719858646393\n",
      "Batch 615/3880, Loss: 0.4899968206882477\n",
      "Batch 616/3880, Loss: 0.4678501486778259\n",
      "Batch 617/3880, Loss: 0.5162209868431091\n",
      "Batch 618/3880, Loss: 0.5473206639289856\n",
      "Batch 619/3880, Loss: 0.5153273344039917\n",
      "Batch 620/3880, Loss: 0.4716217517852783\n",
      "Batch 621/3880, Loss: 0.5421987175941467\n",
      "Batch 622/3880, Loss: 0.5070024132728577\n",
      "Batch 623/3880, Loss: 0.503333568572998\n",
      "Batch 624/3880, Loss: 0.4870077073574066\n",
      "Batch 625/3880, Loss: 0.5446667671203613\n",
      "Batch 626/3880, Loss: 0.49800431728363037\n",
      "Batch 627/3880, Loss: 0.5707054734230042\n",
      "Batch 628/3880, Loss: 0.547207772731781\n",
      "Batch 629/3880, Loss: 0.511062502861023\n",
      "Batch 630/3880, Loss: 0.5014918446540833\n",
      "Batch 631/3880, Loss: 0.5282102227210999\n",
      "Batch 632/3880, Loss: 0.4452589750289917\n",
      "Batch 633/3880, Loss: 0.5414112210273743\n",
      "Batch 634/3880, Loss: 0.5069096684455872\n",
      "Batch 635/3880, Loss: 0.5373620390892029\n",
      "Batch 636/3880, Loss: 0.4977981448173523\n",
      "Batch 637/3880, Loss: 0.505755603313446\n",
      "Batch 638/3880, Loss: 0.5133751630783081\n",
      "Batch 639/3880, Loss: 0.5117118954658508\n",
      "Batch 640/3880, Loss: 0.45373862981796265\n",
      "Batch 641/3880, Loss: 0.4458906948566437\n",
      "Batch 642/3880, Loss: 0.5217487215995789\n",
      "Batch 643/3880, Loss: 0.49455180764198303\n",
      "Batch 644/3880, Loss: 0.5334922671318054\n",
      "Batch 645/3880, Loss: 0.5494615435600281\n",
      "Batch 646/3880, Loss: 0.5607663989067078\n",
      "Batch 647/3880, Loss: 0.44504019618034363\n",
      "Batch 648/3880, Loss: 0.502128541469574\n",
      "Batch 649/3880, Loss: 0.5014293193817139\n",
      "Batch 650/3880, Loss: 0.4806353449821472\n",
      "Batch 651/3880, Loss: 0.5543807148933411\n",
      "Batch 652/3880, Loss: 0.5330937504768372\n",
      "Batch 653/3880, Loss: 0.5091881155967712\n",
      "Batch 654/3880, Loss: 0.4536021053791046\n",
      "Batch 655/3880, Loss: 0.5274586081504822\n",
      "Batch 656/3880, Loss: 0.43294018507003784\n",
      "Batch 657/3880, Loss: 0.5345270037651062\n",
      "Batch 658/3880, Loss: 0.46036943793296814\n",
      "Batch 659/3880, Loss: 0.4783494472503662\n",
      "Batch 660/3880, Loss: 0.5330454111099243\n",
      "Batch 661/3880, Loss: 0.4418342411518097\n",
      "Batch 662/3880, Loss: 0.4647446572780609\n",
      "Batch 663/3880, Loss: 0.49633774161338806\n",
      "Batch 664/3880, Loss: 0.45825445652008057\n",
      "Batch 665/3880, Loss: 0.5063705444335938\n",
      "Batch 666/3880, Loss: 0.5718675255775452\n",
      "Batch 667/3880, Loss: 0.5940303206443787\n",
      "Batch 668/3880, Loss: 0.5388938188552856\n",
      "Batch 669/3880, Loss: 0.5420119762420654\n",
      "Batch 670/3880, Loss: 0.4378184974193573\n",
      "Batch 671/3880, Loss: 0.5091425776481628\n",
      "Batch 672/3880, Loss: 0.5054581165313721\n",
      "Batch 673/3880, Loss: 0.4782300293445587\n",
      "Batch 674/3880, Loss: 0.5274674892425537\n",
      "Batch 675/3880, Loss: 0.44721341133117676\n",
      "Batch 676/3880, Loss: 0.49879691004753113\n",
      "Batch 677/3880, Loss: 0.5157409906387329\n",
      "Batch 678/3880, Loss: 0.5169714689254761\n",
      "Batch 679/3880, Loss: 0.4886450171470642\n",
      "Batch 680/3880, Loss: 0.5449738502502441\n",
      "Batch 681/3880, Loss: 0.6005740761756897\n",
      "Batch 682/3880, Loss: 0.5405842065811157\n",
      "Batch 683/3880, Loss: 0.4809952974319458\n",
      "Batch 684/3880, Loss: 0.4332596957683563\n",
      "Batch 685/3880, Loss: 0.47601956129074097\n",
      "Batch 686/3880, Loss: 0.5604122877120972\n",
      "Batch 687/3880, Loss: 0.4828031063079834\n",
      "Batch 688/3880, Loss: 0.44628894329071045\n",
      "Batch 689/3880, Loss: 0.5036140084266663\n",
      "Batch 690/3880, Loss: 0.5433497428894043\n",
      "Batch 691/3880, Loss: 0.5397339463233948\n",
      "Batch 692/3880, Loss: 0.557165265083313\n",
      "Batch 693/3880, Loss: 0.5285478234291077\n",
      "Batch 694/3880, Loss: 0.5084220170974731\n",
      "Batch 695/3880, Loss: 0.5708229541778564\n",
      "Batch 696/3880, Loss: 0.388776957988739\n",
      "Batch 697/3880, Loss: 0.6022385954856873\n",
      "Batch 698/3880, Loss: 0.5037000179290771\n",
      "Batch 699/3880, Loss: 0.49019187688827515\n",
      "Batch 700/3880, Loss: 0.5586762428283691\n",
      "Batch 701/3880, Loss: 0.5283987522125244\n",
      "Batch 702/3880, Loss: 0.4650043845176697\n",
      "Batch 703/3880, Loss: 0.5426732897758484\n",
      "Batch 704/3880, Loss: 0.4368705749511719\n",
      "Batch 705/3880, Loss: 0.5152658224105835\n",
      "Batch 706/3880, Loss: 0.5112989544868469\n",
      "Batch 707/3880, Loss: 0.4692772328853607\n",
      "Batch 708/3880, Loss: 0.470586359500885\n",
      "Batch 709/3880, Loss: 0.47577306628227234\n",
      "Batch 710/3880, Loss: 0.5222688913345337\n",
      "Batch 711/3880, Loss: 0.4757281541824341\n",
      "Batch 712/3880, Loss: 0.4364562928676605\n",
      "Batch 713/3880, Loss: 0.5142425298690796\n",
      "Batch 714/3880, Loss: 0.44230371713638306\n",
      "Batch 715/3880, Loss: 0.47479429841041565\n",
      "Batch 716/3880, Loss: 0.4767489731311798\n",
      "Batch 717/3880, Loss: 0.43916696310043335\n",
      "Batch 718/3880, Loss: 0.531947135925293\n",
      "Batch 719/3880, Loss: 0.5213335156440735\n",
      "Batch 720/3880, Loss: 0.5267032980918884\n",
      "Batch 721/3880, Loss: 0.5332586169242859\n",
      "Batch 722/3880, Loss: 0.4833362102508545\n",
      "Batch 723/3880, Loss: 0.5390090942382812\n",
      "Batch 724/3880, Loss: 0.48569029569625854\n",
      "Batch 725/3880, Loss: 0.5859283804893494\n",
      "Batch 726/3880, Loss: 0.5156909823417664\n",
      "Batch 727/3880, Loss: 0.4703712463378906\n",
      "Batch 728/3880, Loss: 0.5399071574211121\n",
      "Batch 729/3880, Loss: 0.48740988969802856\n",
      "Batch 730/3880, Loss: 0.5655680894851685\n",
      "Batch 731/3880, Loss: 0.5329470038414001\n",
      "Batch 732/3880, Loss: 0.5147526264190674\n",
      "Batch 733/3880, Loss: 0.4406653642654419\n",
      "Batch 734/3880, Loss: 0.4937472939491272\n",
      "Batch 735/3880, Loss: 0.46911248564720154\n",
      "Batch 736/3880, Loss: 0.4655936360359192\n",
      "Batch 737/3880, Loss: 0.498516321182251\n",
      "Batch 738/3880, Loss: 0.48861929774284363\n",
      "Batch 739/3880, Loss: 0.5307998657226562\n",
      "Batch 740/3880, Loss: 0.49215424060821533\n",
      "Batch 741/3880, Loss: 0.5143839716911316\n",
      "Batch 742/3880, Loss: 0.5205313563346863\n",
      "Batch 743/3880, Loss: 0.46876612305641174\n",
      "Batch 744/3880, Loss: 0.5165174603462219\n",
      "Batch 745/3880, Loss: 0.4993409812450409\n",
      "Batch 746/3880, Loss: 0.46855345368385315\n",
      "Batch 747/3880, Loss: 0.45766690373420715\n",
      "Batch 748/3880, Loss: 0.5345333218574524\n",
      "Batch 749/3880, Loss: 0.5257131457328796\n",
      "Batch 750/3880, Loss: 0.41812726855278015\n",
      "Batch 751/3880, Loss: 0.4643740653991699\n",
      "Batch 752/3880, Loss: 0.565463662147522\n",
      "Batch 753/3880, Loss: 0.4436856210231781\n",
      "Batch 754/3880, Loss: 0.48600637912750244\n",
      "Batch 755/3880, Loss: 0.5105745792388916\n",
      "Batch 756/3880, Loss: 0.41184428334236145\n",
      "Batch 757/3880, Loss: 0.5448121428489685\n",
      "Batch 758/3880, Loss: 0.4583519697189331\n",
      "Batch 759/3880, Loss: 0.48569566011428833\n",
      "Batch 760/3880, Loss: 0.48193058371543884\n",
      "Batch 761/3880, Loss: 0.48838332295417786\n",
      "Batch 762/3880, Loss: 0.4305763840675354\n",
      "Batch 763/3880, Loss: 0.45210304856300354\n",
      "Batch 764/3880, Loss: 0.5610005855560303\n",
      "Batch 765/3880, Loss: 0.5186943411827087\n",
      "Batch 766/3880, Loss: 0.4732411205768585\n",
      "Batch 767/3880, Loss: 0.5123463869094849\n",
      "Batch 768/3880, Loss: 0.501227080821991\n",
      "Batch 769/3880, Loss: 0.44780483841896057\n",
      "Batch 770/3880, Loss: 0.4399045407772064\n",
      "Batch 771/3880, Loss: 0.5345260500907898\n",
      "Batch 772/3880, Loss: 0.4741223454475403\n",
      "Batch 773/3880, Loss: 0.5061296820640564\n",
      "Batch 774/3880, Loss: 0.5307989716529846\n",
      "Batch 775/3880, Loss: 0.4887770414352417\n",
      "Batch 776/3880, Loss: 0.5488491058349609\n",
      "Batch 777/3880, Loss: 0.45916521549224854\n",
      "Batch 778/3880, Loss: 0.49006137251853943\n",
      "Batch 779/3880, Loss: 0.4754209518432617\n",
      "Batch 780/3880, Loss: 0.5043984055519104\n",
      "Batch 781/3880, Loss: 0.598300576210022\n",
      "Batch 782/3880, Loss: 0.44446444511413574\n",
      "Batch 783/3880, Loss: 0.5456522107124329\n",
      "Batch 784/3880, Loss: 0.4496954381465912\n",
      "Batch 785/3880, Loss: 0.4430484473705292\n",
      "Batch 786/3880, Loss: 0.48949623107910156\n",
      "Batch 787/3880, Loss: 0.5007274746894836\n",
      "Batch 788/3880, Loss: 0.48538222908973694\n",
      "Batch 789/3880, Loss: 0.49658048152923584\n",
      "Batch 790/3880, Loss: 0.46356913447380066\n",
      "Batch 791/3880, Loss: 0.5464468002319336\n",
      "Batch 792/3880, Loss: 0.5465748906135559\n",
      "Batch 793/3880, Loss: 0.4913731515407562\n",
      "Batch 794/3880, Loss: 0.527256965637207\n",
      "Batch 795/3880, Loss: 0.4638502299785614\n",
      "Batch 796/3880, Loss: 0.47237345576286316\n",
      "Batch 797/3880, Loss: 0.48155632615089417\n",
      "Batch 798/3880, Loss: 0.5124326944351196\n",
      "Batch 799/3880, Loss: 0.46661466360092163\n",
      "Batch 800/3880, Loss: 0.5478082895278931\n",
      "Batch 801/3880, Loss: 0.5152994394302368\n",
      "Batch 802/3880, Loss: 0.5365262031555176\n",
      "Batch 803/3880, Loss: 0.5275614261627197\n",
      "Batch 804/3880, Loss: 0.458870530128479\n",
      "Batch 805/3880, Loss: 0.47539591789245605\n",
      "Batch 806/3880, Loss: 0.5106846690177917\n",
      "Batch 807/3880, Loss: 0.45835641026496887\n",
      "Batch 808/3880, Loss: 0.5439594388008118\n",
      "Batch 809/3880, Loss: 0.5376814007759094\n",
      "Batch 810/3880, Loss: 0.5075924396514893\n",
      "Batch 811/3880, Loss: 0.5516611933708191\n",
      "Batch 812/3880, Loss: 0.4486873745918274\n",
      "Batch 813/3880, Loss: 0.472473680973053\n",
      "Batch 814/3880, Loss: 0.4557318389415741\n",
      "Batch 815/3880, Loss: 0.44361022114753723\n",
      "Batch 816/3880, Loss: 0.4110960066318512\n",
      "Batch 817/3880, Loss: 0.49028298258781433\n",
      "Batch 818/3880, Loss: 0.5175843238830566\n",
      "Batch 819/3880, Loss: 0.5070485472679138\n",
      "Batch 820/3880, Loss: 0.4954778254032135\n",
      "Batch 821/3880, Loss: 0.4626348614692688\n",
      "Batch 822/3880, Loss: 0.5149355530738831\n",
      "Batch 823/3880, Loss: 0.49314993619918823\n",
      "Batch 824/3880, Loss: 0.4917946755886078\n",
      "Batch 825/3880, Loss: 0.5324093103408813\n",
      "Batch 826/3880, Loss: 0.5156618356704712\n",
      "Batch 827/3880, Loss: 0.4273245632648468\n",
      "Batch 828/3880, Loss: 0.4780161678791046\n",
      "Batch 829/3880, Loss: 0.5011848211288452\n",
      "Batch 830/3880, Loss: 0.5616986751556396\n",
      "Batch 831/3880, Loss: 0.5273008346557617\n",
      "Batch 832/3880, Loss: 0.5041553378105164\n",
      "Batch 833/3880, Loss: 0.5110552906990051\n",
      "Batch 834/3880, Loss: 0.46712878346443176\n",
      "Batch 835/3880, Loss: 0.5479613542556763\n",
      "Batch 836/3880, Loss: 0.5587335824966431\n",
      "Batch 837/3880, Loss: 0.45062124729156494\n",
      "Batch 838/3880, Loss: 0.5085650682449341\n",
      "Batch 839/3880, Loss: 0.4275039732456207\n",
      "Batch 840/3880, Loss: 0.5402755737304688\n",
      "Batch 841/3880, Loss: 0.4923599362373352\n",
      "Batch 842/3880, Loss: 0.48008790612220764\n",
      "Batch 843/3880, Loss: 0.5340732336044312\n",
      "Batch 844/3880, Loss: 0.45646196603775024\n",
      "Batch 845/3880, Loss: 0.5245416760444641\n",
      "Batch 846/3880, Loss: 0.49230173230171204\n",
      "Batch 847/3880, Loss: 0.497062623500824\n",
      "Batch 848/3880, Loss: 0.5112252235412598\n",
      "Batch 849/3880, Loss: 0.42055338621139526\n",
      "Batch 850/3880, Loss: 0.4349830448627472\n",
      "Batch 851/3880, Loss: 0.5523857474327087\n",
      "Batch 852/3880, Loss: 0.5524432063102722\n",
      "Batch 853/3880, Loss: 0.49443572759628296\n",
      "Batch 854/3880, Loss: 0.5670229196548462\n",
      "Batch 855/3880, Loss: 0.5285709500312805\n",
      "Batch 856/3880, Loss: 0.5089510083198547\n",
      "Batch 857/3880, Loss: 0.5740712881088257\n",
      "Batch 858/3880, Loss: 0.4931696951389313\n",
      "Batch 859/3880, Loss: 0.4997580945491791\n",
      "Batch 860/3880, Loss: 0.47679054737091064\n",
      "Batch 861/3880, Loss: 0.45066556334495544\n",
      "Batch 862/3880, Loss: 0.5192297101020813\n",
      "Batch 863/3880, Loss: 0.5024674534797668\n",
      "Batch 864/3880, Loss: 0.45953068137168884\n",
      "Batch 865/3880, Loss: 0.4582696855068207\n",
      "Batch 866/3880, Loss: 0.464096337556839\n",
      "Batch 867/3880, Loss: 0.47029542922973633\n",
      "Batch 868/3880, Loss: 0.5025582313537598\n",
      "Batch 869/3880, Loss: 0.508159339427948\n",
      "Batch 870/3880, Loss: 0.4399776756763458\n",
      "Batch 871/3880, Loss: 0.4118365943431854\n",
      "Batch 872/3880, Loss: 0.5033971071243286\n",
      "Batch 873/3880, Loss: 0.445516437292099\n",
      "Batch 874/3880, Loss: 0.5616934895515442\n",
      "Batch 875/3880, Loss: 0.48578590154647827\n",
      "Batch 876/3880, Loss: 0.5146190524101257\n",
      "Batch 877/3880, Loss: 0.447073757648468\n",
      "Batch 878/3880, Loss: 0.4931700527667999\n",
      "Batch 879/3880, Loss: 0.5356448888778687\n",
      "Batch 880/3880, Loss: 0.5134139060974121\n",
      "Batch 881/3880, Loss: 0.47455525398254395\n",
      "Batch 882/3880, Loss: 0.4686291515827179\n",
      "Batch 883/3880, Loss: 0.5034189224243164\n",
      "Batch 884/3880, Loss: 0.5460686087608337\n",
      "Batch 885/3880, Loss: 0.5018095374107361\n",
      "Batch 886/3880, Loss: 0.4730009436607361\n",
      "Batch 887/3880, Loss: 0.5284860134124756\n",
      "Batch 888/3880, Loss: 0.5427880883216858\n",
      "Batch 889/3880, Loss: 0.4640358090400696\n",
      "Batch 890/3880, Loss: 0.5315649509429932\n",
      "Batch 891/3880, Loss: 0.5613136887550354\n",
      "Batch 892/3880, Loss: 0.48646098375320435\n",
      "Batch 893/3880, Loss: 0.47313815355300903\n",
      "Batch 894/3880, Loss: 0.5062967538833618\n",
      "Batch 895/3880, Loss: 0.4855329096317291\n",
      "Batch 896/3880, Loss: 0.5221365690231323\n",
      "Batch 897/3880, Loss: 0.5447684526443481\n",
      "Batch 898/3880, Loss: 0.5141907930374146\n",
      "Batch 899/3880, Loss: 0.43638455867767334\n",
      "Batch 900/3880, Loss: 0.5104951858520508\n",
      "Batch 901/3880, Loss: 0.5183326005935669\n",
      "Batch 902/3880, Loss: 0.5042897462844849\n",
      "Batch 903/3880, Loss: 0.4550727307796478\n",
      "Batch 904/3880, Loss: 0.6309306025505066\n",
      "Batch 905/3880, Loss: 0.43070313334465027\n",
      "Batch 906/3880, Loss: 0.4232839047908783\n",
      "Batch 907/3880, Loss: 0.5146020650863647\n",
      "Batch 908/3880, Loss: 0.47334742546081543\n",
      "Batch 909/3880, Loss: 0.47737979888916016\n",
      "Batch 910/3880, Loss: 0.45518526434898376\n",
      "Batch 911/3880, Loss: 0.4711788594722748\n",
      "Batch 912/3880, Loss: 0.47352179884910583\n",
      "Batch 913/3880, Loss: 0.4877350628376007\n",
      "Batch 914/3880, Loss: 0.4710160195827484\n",
      "Batch 915/3880, Loss: 0.46878743171691895\n",
      "Batch 916/3880, Loss: 0.4714510142803192\n",
      "Batch 917/3880, Loss: 0.4317980110645294\n",
      "Batch 918/3880, Loss: 0.45076507329940796\n",
      "Batch 919/3880, Loss: 0.43688780069351196\n",
      "Batch 920/3880, Loss: 0.43523162603378296\n",
      "Batch 921/3880, Loss: 0.45261937379837036\n",
      "Batch 922/3880, Loss: 0.44392111897468567\n",
      "Batch 923/3880, Loss: 0.44319504499435425\n",
      "Batch 924/3880, Loss: 0.4340537488460541\n",
      "Batch 925/3880, Loss: 0.4383310377597809\n",
      "Batch 926/3880, Loss: 0.42017242312431335\n",
      "Batch 927/3880, Loss: 0.5157579183578491\n",
      "Batch 928/3880, Loss: 0.45824766159057617\n",
      "Batch 929/3880, Loss: 0.519667387008667\n",
      "Batch 930/3880, Loss: 0.4961135685443878\n",
      "Batch 931/3880, Loss: 0.4076572060585022\n",
      "Batch 932/3880, Loss: 0.4517793357372284\n",
      "Batch 933/3880, Loss: 0.451108455657959\n",
      "Batch 934/3880, Loss: 0.5217928886413574\n",
      "Batch 935/3880, Loss: 0.44968411326408386\n",
      "Batch 936/3880, Loss: 0.43238672614097595\n",
      "Batch 937/3880, Loss: 0.551750898361206\n",
      "Batch 938/3880, Loss: 0.4790196418762207\n",
      "Batch 939/3880, Loss: 0.5033277869224548\n",
      "Batch 940/3880, Loss: 0.40529096126556396\n",
      "Batch 941/3880, Loss: 0.5279375314712524\n",
      "Batch 942/3880, Loss: 0.43593287467956543\n",
      "Batch 943/3880, Loss: 0.4949563443660736\n",
      "Batch 944/3880, Loss: 0.5359588861465454\n",
      "Batch 945/3880, Loss: 0.4993211627006531\n",
      "Batch 946/3880, Loss: 0.4654535949230194\n",
      "Batch 947/3880, Loss: 0.4335660934448242\n",
      "Batch 948/3880, Loss: 0.5109800100326538\n",
      "Batch 949/3880, Loss: 0.588111400604248\n",
      "Batch 950/3880, Loss: 0.5011082291603088\n",
      "Batch 951/3880, Loss: 0.4905228912830353\n",
      "Batch 952/3880, Loss: 0.5180114507675171\n",
      "Batch 953/3880, Loss: 0.4103075861930847\n",
      "Batch 954/3880, Loss: 0.5038373470306396\n",
      "Batch 955/3880, Loss: 0.40619656443595886\n",
      "Batch 956/3880, Loss: 0.5166146755218506\n",
      "Batch 957/3880, Loss: 0.45411452651023865\n",
      "Batch 958/3880, Loss: 0.5004391670227051\n",
      "Batch 959/3880, Loss: 0.5218823552131653\n",
      "Batch 960/3880, Loss: 0.4767751395702362\n",
      "Batch 961/3880, Loss: 0.5195556282997131\n",
      "Batch 962/3880, Loss: 0.4841199517250061\n",
      "Batch 963/3880, Loss: 0.5230045318603516\n",
      "Batch 964/3880, Loss: 0.46951624751091003\n",
      "Batch 965/3880, Loss: 0.5242902636528015\n",
      "Batch 966/3880, Loss: 0.48928579688072205\n",
      "Batch 967/3880, Loss: 0.5312738418579102\n",
      "Batch 968/3880, Loss: 0.49136030673980713\n",
      "Batch 969/3880, Loss: 0.4382435083389282\n",
      "Batch 970/3880, Loss: 0.4545556902885437\n",
      "Batch 971/3880, Loss: 0.4368138313293457\n",
      "Batch 972/3880, Loss: 0.46880006790161133\n",
      "Batch 973/3880, Loss: 0.4554949700832367\n",
      "Batch 974/3880, Loss: 0.5109655261039734\n",
      "Batch 975/3880, Loss: 0.5177675485610962\n",
      "Batch 976/3880, Loss: 0.4634241461753845\n",
      "Batch 977/3880, Loss: 0.49350786209106445\n",
      "Batch 978/3880, Loss: 0.496609091758728\n",
      "Batch 979/3880, Loss: 0.47394534945487976\n",
      "Batch 980/3880, Loss: 0.49051985144615173\n",
      "Batch 981/3880, Loss: 0.48767104744911194\n",
      "Batch 982/3880, Loss: 0.477561891078949\n",
      "Batch 983/3880, Loss: 0.5443626046180725\n",
      "Batch 984/3880, Loss: 0.5124191641807556\n",
      "Batch 985/3880, Loss: 0.4759771525859833\n",
      "Batch 986/3880, Loss: 0.5271791219711304\n",
      "Batch 987/3880, Loss: 0.4892021715641022\n",
      "Batch 988/3880, Loss: 0.49906671047210693\n",
      "Batch 989/3880, Loss: 0.5180239677429199\n",
      "Batch 990/3880, Loss: 0.3815516233444214\n",
      "Batch 991/3880, Loss: 0.44877859950065613\n",
      "Batch 992/3880, Loss: 0.4659109115600586\n",
      "Batch 993/3880, Loss: 0.45432937145233154\n",
      "Batch 994/3880, Loss: 0.4576680064201355\n",
      "Batch 995/3880, Loss: 0.4852220118045807\n",
      "Batch 996/3880, Loss: 0.4961779713630676\n",
      "Batch 997/3880, Loss: 0.44901442527770996\n",
      "Batch 998/3880, Loss: 0.4489821195602417\n",
      "Batch 999/3880, Loss: 0.47462987899780273\n",
      "Batch 1000/3880, Loss: 0.5103527903556824\n",
      "Batch 1001/3880, Loss: 0.44894516468048096\n",
      "Batch 1002/3880, Loss: 0.4871947765350342\n",
      "Batch 1003/3880, Loss: 0.5229986906051636\n",
      "Batch 1004/3880, Loss: 0.4260503351688385\n",
      "Batch 1005/3880, Loss: 0.4992464482784271\n",
      "Batch 1006/3880, Loss: 0.533207893371582\n",
      "Batch 1007/3880, Loss: 0.46196091175079346\n",
      "Batch 1008/3880, Loss: 0.4630431830883026\n",
      "Batch 1009/3880, Loss: 0.4732118844985962\n",
      "Batch 1010/3880, Loss: 0.48428860306739807\n",
      "Batch 1011/3880, Loss: 0.481841504573822\n",
      "Batch 1012/3880, Loss: 0.47866693139076233\n",
      "Batch 1013/3880, Loss: 0.42520269751548767\n",
      "Batch 1014/3880, Loss: 0.46258971095085144\n",
      "Batch 1015/3880, Loss: 0.49449238181114197\n",
      "Batch 1016/3880, Loss: 0.47492918372154236\n",
      "Batch 1017/3880, Loss: 0.5053945779800415\n",
      "Batch 1018/3880, Loss: 0.5021038055419922\n",
      "Batch 1019/3880, Loss: 0.39447399973869324\n",
      "Batch 1020/3880, Loss: 0.4642011821269989\n",
      "Batch 1021/3880, Loss: 0.5110105872154236\n",
      "Batch 1022/3880, Loss: 0.4401949346065521\n",
      "Batch 1023/3880, Loss: 0.42970314621925354\n",
      "Batch 1024/3880, Loss: 0.4776977002620697\n",
      "Batch 1025/3880, Loss: 0.43151548504829407\n",
      "Batch 1026/3880, Loss: 0.40232911705970764\n",
      "Batch 1027/3880, Loss: 0.47545579075813293\n",
      "Batch 1028/3880, Loss: 0.4968588948249817\n",
      "Batch 1029/3880, Loss: 0.46069541573524475\n",
      "Batch 1030/3880, Loss: 0.4935091733932495\n",
      "Batch 1031/3880, Loss: 0.5316225290298462\n",
      "Batch 1032/3880, Loss: 0.4930667579174042\n",
      "Batch 1033/3880, Loss: 0.436164528131485\n",
      "Batch 1034/3880, Loss: 0.4924042224884033\n",
      "Batch 1035/3880, Loss: 0.45950350165367126\n",
      "Batch 1036/3880, Loss: 0.48140689730644226\n",
      "Batch 1037/3880, Loss: 0.47834840416908264\n",
      "Batch 1038/3880, Loss: 0.4995419383049011\n",
      "Batch 1039/3880, Loss: 0.43139785528182983\n",
      "Batch 1040/3880, Loss: 0.5167142748832703\n",
      "Batch 1041/3880, Loss: 0.4565364122390747\n",
      "Batch 1042/3880, Loss: 0.4602995812892914\n",
      "Batch 1043/3880, Loss: 0.4561580717563629\n",
      "Batch 1044/3880, Loss: 0.4826722741127014\n",
      "Batch 1045/3880, Loss: 0.48129481077194214\n",
      "Batch 1046/3880, Loss: 0.4705495834350586\n",
      "Batch 1047/3880, Loss: 0.5644133687019348\n",
      "Batch 1048/3880, Loss: 0.5144848823547363\n",
      "Batch 1049/3880, Loss: 0.5023071765899658\n",
      "Batch 1050/3880, Loss: 0.4938010275363922\n",
      "Batch 1051/3880, Loss: 0.4755207300186157\n",
      "Batch 1052/3880, Loss: 0.5028613805770874\n",
      "Batch 1053/3880, Loss: 0.46631088852882385\n",
      "Batch 1054/3880, Loss: 0.4966704845428467\n",
      "Batch 1055/3880, Loss: 0.4984641671180725\n",
      "Batch 1056/3880, Loss: 0.4184517562389374\n",
      "Batch 1057/3880, Loss: 0.4569295644760132\n",
      "Batch 1058/3880, Loss: 0.4758279025554657\n",
      "Batch 1059/3880, Loss: 0.47431179881095886\n",
      "Batch 1060/3880, Loss: 0.40370509028434753\n",
      "Batch 1061/3880, Loss: 0.5316676497459412\n",
      "Batch 1062/3880, Loss: 0.5101921558380127\n",
      "Batch 1063/3880, Loss: 0.43096572160720825\n",
      "Batch 1064/3880, Loss: 0.46725860238075256\n",
      "Batch 1065/3880, Loss: 0.45929276943206787\n",
      "Batch 1066/3880, Loss: 0.5165470838546753\n",
      "Batch 1067/3880, Loss: 0.46332958340644836\n",
      "Batch 1068/3880, Loss: 0.36496207118034363\n",
      "Batch 1069/3880, Loss: 0.5319947600364685\n",
      "Batch 1070/3880, Loss: 0.43508031964302063\n",
      "Batch 1071/3880, Loss: 0.4793902635574341\n",
      "Batch 1072/3880, Loss: 0.43778541684150696\n",
      "Batch 1073/3880, Loss: 0.4254533052444458\n",
      "Batch 1074/3880, Loss: 0.53541499376297\n",
      "Batch 1075/3880, Loss: 0.47765618562698364\n",
      "Batch 1076/3880, Loss: 0.504588782787323\n",
      "Batch 1077/3880, Loss: 0.4883969724178314\n",
      "Batch 1078/3880, Loss: 0.43640050292015076\n",
      "Batch 1079/3880, Loss: 0.40681707859039307\n",
      "Batch 1080/3880, Loss: 0.5042558312416077\n",
      "Batch 1081/3880, Loss: 0.4282565116882324\n",
      "Batch 1082/3880, Loss: 0.5566789507865906\n",
      "Batch 1083/3880, Loss: 0.4653090536594391\n",
      "Batch 1084/3880, Loss: 0.45572012662887573\n",
      "Batch 1085/3880, Loss: 0.46989381313323975\n",
      "Batch 1086/3880, Loss: 0.40189000964164734\n",
      "Batch 1087/3880, Loss: 0.44976505637168884\n",
      "Batch 1088/3880, Loss: 0.48867136240005493\n",
      "Batch 1089/3880, Loss: 0.4779289662837982\n",
      "Batch 1090/3880, Loss: 0.46844422817230225\n",
      "Batch 1091/3880, Loss: 0.4858141243457794\n",
      "Batch 1092/3880, Loss: 0.5329186320304871\n",
      "Batch 1093/3880, Loss: 0.44127607345581055\n",
      "Batch 1094/3880, Loss: 0.4665645360946655\n",
      "Batch 1095/3880, Loss: 0.44261619448661804\n",
      "Batch 1096/3880, Loss: 0.4147496819496155\n",
      "Batch 1097/3880, Loss: 0.4532637596130371\n",
      "Batch 1098/3880, Loss: 0.4500664174556732\n",
      "Batch 1099/3880, Loss: 0.5963417887687683\n",
      "Batch 1100/3880, Loss: 0.4042460024356842\n",
      "Batch 1101/3880, Loss: 0.4014030396938324\n",
      "Batch 1102/3880, Loss: 0.5199210047721863\n",
      "Batch 1103/3880, Loss: 0.4955556094646454\n",
      "Batch 1104/3880, Loss: 0.4328775703907013\n",
      "Batch 1105/3880, Loss: 0.4611015319824219\n",
      "Batch 1106/3880, Loss: 0.49336472153663635\n",
      "Batch 1107/3880, Loss: 0.46763548254966736\n",
      "Batch 1108/3880, Loss: 0.48753201961517334\n",
      "Batch 1109/3880, Loss: 0.5146040320396423\n",
      "Batch 1110/3880, Loss: 0.4094250500202179\n",
      "Batch 1111/3880, Loss: 0.4975110590457916\n",
      "Batch 1112/3880, Loss: 0.4874825179576874\n",
      "Batch 1113/3880, Loss: 0.48378485441207886\n",
      "Batch 1114/3880, Loss: 0.5217203497886658\n",
      "Batch 1115/3880, Loss: 0.4165104925632477\n",
      "Batch 1116/3880, Loss: 0.4478509724140167\n",
      "Batch 1117/3880, Loss: 0.41414594650268555\n",
      "Batch 1118/3880, Loss: 0.4600406289100647\n",
      "Batch 1119/3880, Loss: 0.5037581324577332\n",
      "Batch 1120/3880, Loss: 0.4583958089351654\n",
      "Batch 1121/3880, Loss: 0.5065028071403503\n",
      "Batch 1122/3880, Loss: 0.4320463240146637\n",
      "Batch 1123/3880, Loss: 0.5341084003448486\n",
      "Batch 1124/3880, Loss: 0.4620254933834076\n",
      "Batch 1125/3880, Loss: 0.5292330384254456\n",
      "Batch 1126/3880, Loss: 0.48259779810905457\n",
      "Batch 1127/3880, Loss: 0.4752568304538727\n",
      "Batch 1128/3880, Loss: 0.4068351089954376\n",
      "Batch 1129/3880, Loss: 0.40065038204193115\n",
      "Batch 1130/3880, Loss: 0.439822256565094\n",
      "Batch 1131/3880, Loss: 0.4482649266719818\n",
      "Batch 1132/3880, Loss: 0.48064592480659485\n",
      "Batch 1133/3880, Loss: 0.500286340713501\n",
      "Batch 1134/3880, Loss: 0.5073012113571167\n",
      "Batch 1135/3880, Loss: 0.5034639239311218\n",
      "Batch 1136/3880, Loss: 0.47024449706077576\n",
      "Batch 1137/3880, Loss: 0.516110360622406\n",
      "Batch 1138/3880, Loss: 0.5019994378089905\n",
      "Batch 1139/3880, Loss: 0.47280851006507874\n",
      "Batch 1140/3880, Loss: 0.42167550325393677\n",
      "Batch 1141/3880, Loss: 0.4725869297981262\n",
      "Batch 1142/3880, Loss: 0.3755294382572174\n",
      "Batch 1143/3880, Loss: 0.3878367245197296\n",
      "Batch 1144/3880, Loss: 0.41226089000701904\n",
      "Batch 1145/3880, Loss: 0.43511804938316345\n",
      "Batch 1146/3880, Loss: 0.45008254051208496\n",
      "Batch 1147/3880, Loss: 0.4642198979854584\n",
      "Batch 1148/3880, Loss: 0.5058282613754272\n",
      "Batch 1149/3880, Loss: 0.43416690826416016\n",
      "Batch 1150/3880, Loss: 0.46589183807373047\n",
      "Batch 1151/3880, Loss: 0.4056425094604492\n",
      "Batch 1152/3880, Loss: 0.558655858039856\n",
      "Batch 1153/3880, Loss: 0.4562337398529053\n",
      "Batch 1154/3880, Loss: 0.4811299741268158\n",
      "Batch 1155/3880, Loss: 0.4585322439670563\n",
      "Batch 1156/3880, Loss: 0.40980371832847595\n",
      "Batch 1157/3880, Loss: 0.4637845456600189\n",
      "Batch 1158/3880, Loss: 0.5197370648384094\n",
      "Batch 1159/3880, Loss: 0.49201351404190063\n",
      "Batch 1160/3880, Loss: 0.42608168721199036\n",
      "Batch 1161/3880, Loss: 0.4554435908794403\n",
      "Batch 1162/3880, Loss: 0.4668422043323517\n",
      "Batch 1163/3880, Loss: 0.4450364410877228\n",
      "Batch 1164/3880, Loss: 0.5249554514884949\n",
      "Batch 1165/3880, Loss: 0.43121153116226196\n",
      "Batch 1166/3880, Loss: 0.43625545501708984\n",
      "Batch 1167/3880, Loss: 0.4757668972015381\n",
      "Batch 1168/3880, Loss: 0.4131474792957306\n",
      "Batch 1169/3880, Loss: 0.46561282873153687\n",
      "Batch 1170/3880, Loss: 0.4574853181838989\n",
      "Batch 1171/3880, Loss: 0.47276097536087036\n",
      "Batch 1172/3880, Loss: 0.46099868416786194\n",
      "Batch 1173/3880, Loss: 0.4532892107963562\n",
      "Batch 1174/3880, Loss: 0.5186554789543152\n",
      "Batch 1175/3880, Loss: 0.4643738567829132\n",
      "Batch 1176/3880, Loss: 0.4703439474105835\n",
      "Batch 1177/3880, Loss: 0.4782845973968506\n",
      "Batch 1178/3880, Loss: 0.4481603503227234\n",
      "Batch 1179/3880, Loss: 0.4446593225002289\n",
      "Batch 1180/3880, Loss: 0.46633946895599365\n",
      "Batch 1181/3880, Loss: 0.4405151903629303\n",
      "Batch 1182/3880, Loss: 0.4437183141708374\n",
      "Batch 1183/3880, Loss: 0.4723365604877472\n",
      "Batch 1184/3880, Loss: 0.48015549778938293\n",
      "Batch 1185/3880, Loss: 0.4745810627937317\n",
      "Batch 1186/3880, Loss: 0.5164009928703308\n",
      "Batch 1187/3880, Loss: 0.4713457524776459\n",
      "Batch 1188/3880, Loss: 0.45761725306510925\n",
      "Batch 1189/3880, Loss: 0.42418625950813293\n",
      "Batch 1190/3880, Loss: 0.4833855628967285\n",
      "Batch 1191/3880, Loss: 0.4501708149909973\n",
      "Batch 1192/3880, Loss: 0.49167171120643616\n",
      "Batch 1193/3880, Loss: 0.41213005781173706\n",
      "Batch 1194/3880, Loss: 0.5020257234573364\n",
      "Batch 1195/3880, Loss: 0.4363095462322235\n",
      "Batch 1196/3880, Loss: 0.44120365381240845\n",
      "Batch 1197/3880, Loss: 0.44310376048088074\n",
      "Batch 1198/3880, Loss: 0.47953107953071594\n",
      "Batch 1199/3880, Loss: 0.4949969947338104\n",
      "Batch 1200/3880, Loss: 0.5285601019859314\n",
      "Batch 1201/3880, Loss: 0.454615980386734\n",
      "Batch 1202/3880, Loss: 0.4054511487483978\n",
      "Batch 1203/3880, Loss: 0.434262216091156\n",
      "Batch 1204/3880, Loss: 0.4757654368877411\n",
      "Batch 1205/3880, Loss: 0.4059048295021057\n",
      "Batch 1206/3880, Loss: 0.46088871359825134\n",
      "Batch 1207/3880, Loss: 0.4501917362213135\n",
      "Batch 1208/3880, Loss: 0.48960402607917786\n",
      "Batch 1209/3880, Loss: 0.46934667229652405\n",
      "Batch 1210/3880, Loss: 0.39930397272109985\n",
      "Batch 1211/3880, Loss: 0.46526092290878296\n",
      "Batch 1212/3880, Loss: 0.47768157720565796\n",
      "Batch 1213/3880, Loss: 0.4959118366241455\n",
      "Batch 1214/3880, Loss: 0.45544782280921936\n",
      "Batch 1215/3880, Loss: 0.43305012583732605\n",
      "Batch 1216/3880, Loss: 0.4974400997161865\n",
      "Batch 1217/3880, Loss: 0.5253142714500427\n",
      "Batch 1218/3880, Loss: 0.46080970764160156\n",
      "Batch 1219/3880, Loss: 0.4353083372116089\n",
      "Batch 1220/3880, Loss: 0.47088631987571716\n",
      "Batch 1221/3880, Loss: 0.5202465653419495\n",
      "Batch 1222/3880, Loss: 0.5090832710266113\n",
      "Batch 1223/3880, Loss: 0.42542868852615356\n",
      "Batch 1224/3880, Loss: 0.42814895510673523\n",
      "Batch 1225/3880, Loss: 0.5007675290107727\n",
      "Batch 1226/3880, Loss: 0.416955828666687\n",
      "Batch 1227/3880, Loss: 0.4650975465774536\n",
      "Batch 1228/3880, Loss: 0.46499863266944885\n",
      "Batch 1229/3880, Loss: 0.42650142312049866\n",
      "Batch 1230/3880, Loss: 0.4539313018321991\n",
      "Batch 1231/3880, Loss: 0.5120177865028381\n",
      "Batch 1232/3880, Loss: 0.43804728984832764\n",
      "Batch 1233/3880, Loss: 0.4268878698348999\n",
      "Batch 1234/3880, Loss: 0.45427682995796204\n",
      "Batch 1235/3880, Loss: 0.4308016300201416\n",
      "Batch 1236/3880, Loss: 0.43310773372650146\n",
      "Batch 1237/3880, Loss: 0.45519182085990906\n",
      "Batch 1238/3880, Loss: 0.49982914328575134\n",
      "Batch 1239/3880, Loss: 0.48128190636634827\n",
      "Batch 1240/3880, Loss: 0.4949277937412262\n",
      "Batch 1241/3880, Loss: 0.497183233499527\n",
      "Batch 1242/3880, Loss: 0.5234226584434509\n",
      "Batch 1243/3880, Loss: 0.42696043848991394\n",
      "Batch 1244/3880, Loss: 0.42923569679260254\n",
      "Batch 1245/3880, Loss: 0.44644707441329956\n",
      "Batch 1246/3880, Loss: 0.3685891330242157\n",
      "Batch 1247/3880, Loss: 0.47955942153930664\n",
      "Batch 1248/3880, Loss: 0.4966666102409363\n",
      "Batch 1249/3880, Loss: 0.5362541079521179\n",
      "Batch 1250/3880, Loss: 0.4378596544265747\n",
      "Batch 1251/3880, Loss: 0.4160241186618805\n",
      "Batch 1252/3880, Loss: 0.4859217703342438\n",
      "Batch 1253/3880, Loss: 0.49373286962509155\n",
      "Batch 1254/3880, Loss: 0.4837435185909271\n",
      "Batch 1255/3880, Loss: 0.4755856990814209\n",
      "Batch 1256/3880, Loss: 0.46190258860588074\n",
      "Batch 1257/3880, Loss: 0.5228195786476135\n",
      "Batch 1258/3880, Loss: 0.45717853307724\n",
      "Batch 1259/3880, Loss: 0.48308467864990234\n",
      "Batch 1260/3880, Loss: 0.4975203573703766\n",
      "Batch 1261/3880, Loss: 0.4926580488681793\n",
      "Batch 1262/3880, Loss: 0.4428789019584656\n",
      "Batch 1263/3880, Loss: 0.48172619938850403\n",
      "Batch 1264/3880, Loss: 0.45663535594940186\n",
      "Batch 1265/3880, Loss: 0.4571738541126251\n",
      "Batch 1266/3880, Loss: 0.4125739634037018\n",
      "Batch 1267/3880, Loss: 0.4746442437171936\n",
      "Batch 1268/3880, Loss: 0.41615819931030273\n",
      "Batch 1269/3880, Loss: 0.47907954454421997\n",
      "Batch 1270/3880, Loss: 0.4446395933628082\n",
      "Batch 1271/3880, Loss: 0.4656262695789337\n",
      "Batch 1272/3880, Loss: 0.3954659402370453\n",
      "Batch 1273/3880, Loss: 0.39890286326408386\n",
      "Batch 1274/3880, Loss: 0.44670411944389343\n",
      "Batch 1275/3880, Loss: 0.3847355246543884\n",
      "Batch 1276/3880, Loss: 0.49494022130966187\n",
      "Batch 1277/3880, Loss: 0.43230006098747253\n",
      "Batch 1278/3880, Loss: 0.5030718445777893\n",
      "Batch 1279/3880, Loss: 0.4048524796962738\n",
      "Batch 1280/3880, Loss: 0.4000339210033417\n",
      "Batch 1281/3880, Loss: 0.515792965888977\n",
      "Batch 1282/3880, Loss: 0.46530017256736755\n",
      "Batch 1283/3880, Loss: 0.4919579327106476\n",
      "Batch 1284/3880, Loss: 0.38457122445106506\n",
      "Batch 1285/3880, Loss: 0.49027904868125916\n",
      "Batch 1286/3880, Loss: 0.4555215835571289\n",
      "Batch 1287/3880, Loss: 0.42808204889297485\n",
      "Batch 1288/3880, Loss: 0.49250328540802\n",
      "Batch 1289/3880, Loss: 0.44838598370552063\n",
      "Batch 1290/3880, Loss: 0.42531684041023254\n",
      "Batch 1291/3880, Loss: 0.4418398141860962\n",
      "Batch 1292/3880, Loss: 0.5462023615837097\n",
      "Batch 1293/3880, Loss: 0.3939228057861328\n",
      "Batch 1294/3880, Loss: 0.454133003950119\n",
      "Batch 1295/3880, Loss: 0.5114591717720032\n",
      "Batch 1296/3880, Loss: 0.44126075506210327\n",
      "Batch 1297/3880, Loss: 0.4448220729827881\n",
      "Batch 1298/3880, Loss: 0.4379816949367523\n",
      "Batch 1299/3880, Loss: 0.4101058840751648\n",
      "Batch 1300/3880, Loss: 0.4596283435821533\n",
      "Batch 1301/3880, Loss: 0.41959553956985474\n",
      "Batch 1302/3880, Loss: 0.40955638885498047\n",
      "Batch 1303/3880, Loss: 0.42726606130599976\n",
      "Batch 1304/3880, Loss: 0.4439705014228821\n",
      "Batch 1305/3880, Loss: 0.49087265133857727\n",
      "Batch 1306/3880, Loss: 0.42090001702308655\n",
      "Batch 1307/3880, Loss: 0.43151670694351196\n",
      "Batch 1308/3880, Loss: 0.5709567666053772\n",
      "Batch 1309/3880, Loss: 0.4142909049987793\n",
      "Batch 1310/3880, Loss: 0.47023868560791016\n",
      "Batch 1311/3880, Loss: 0.41931483149528503\n",
      "Batch 1312/3880, Loss: 0.4433165192604065\n",
      "Batch 1313/3880, Loss: 0.4858194887638092\n",
      "Batch 1314/3880, Loss: 0.4461723268032074\n",
      "Batch 1315/3880, Loss: 0.44502657651901245\n",
      "Batch 1316/3880, Loss: 0.4731239378452301\n",
      "Batch 1317/3880, Loss: 0.4825930893421173\n",
      "Batch 1318/3880, Loss: 0.40718400478363037\n",
      "Batch 1319/3880, Loss: 0.4608483612537384\n",
      "Batch 1320/3880, Loss: 0.4989151656627655\n",
      "Batch 1321/3880, Loss: 0.4611281454563141\n",
      "Batch 1322/3880, Loss: 0.4559507369995117\n",
      "Batch 1323/3880, Loss: 0.45190420746803284\n",
      "Batch 1324/3880, Loss: 0.4378252923488617\n",
      "Batch 1325/3880, Loss: 0.4408033788204193\n",
      "Batch 1326/3880, Loss: 0.41796037554740906\n",
      "Batch 1327/3880, Loss: 0.4629096984863281\n",
      "Batch 1328/3880, Loss: 0.4138096272945404\n",
      "Batch 1329/3880, Loss: 0.5323184728622437\n",
      "Batch 1330/3880, Loss: 0.3992809057235718\n",
      "Batch 1331/3880, Loss: 0.47540196776390076\n",
      "Batch 1332/3880, Loss: 0.4281827211380005\n",
      "Batch 1333/3880, Loss: 0.4410230815410614\n",
      "Batch 1334/3880, Loss: 0.39427512884140015\n",
      "Batch 1335/3880, Loss: 0.5349997878074646\n"
     ]
    }
   ],
   "source": [
    "## Создаем токенайзер\n",
    "#tokenizer = Tokenizer(text)\n",
    "#dataset = JokesDataset(tokenizer, cut_text, max_len=128)\n",
    "#\n",
    "## DataLoader\n",
    "#dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Создаем модель\n",
    "input_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = char_tokenizer.vocab_size\n",
    "model = CustomRNN(vocab_size=char_tokenizer.vocab_size, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# Оптимизатор и лосс\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([model.rnn.Wx, model.rnn.Wh, model.rnn.bh, model.rnn.Wy, model.rnn.by, *model.embedding.parameters(), *model.fc.parameters()], lr=0.001)\n",
    "\n",
    "# Обучение\n",
    "losses = train_model(model, char_dataloader, criterion, optimizer, num_epochs=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "\n",
    "# Генерация текста\n",
    "generated_text = model.inference(prefix=\"Привет\", tokenizer=char_tokenizer, max_len=50, device=\"cpu\")\n",
    "print(\"Сгенерированный текст:\", generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}